%\documentclass[pdf,notes]{beamer}
\documentclass[pdf,handout]{beamer}
\mode<presentation>{\usetheme[secheader]{Boadilla}}
\usepackage{mystyle}
\includecomment{versiona}
\usepackage{xeCJK}
\usepackage{subfig}

%\setbeameroption{hide notes}
\newcommand{\red}[1]{{\color[rgb]{0.6,0,0}#1}}
\setCJKmainfont[AutoFakeBold=true]{Hiragino Mincho Pro} %my Mac
%\setCJKmainfont{MS PGothic} %AJP windows

\makeatletter
\newenvironment<>{proofs}[1][\proofname]{\par\def\insertproofname{#1\@addpunct{.}}\usebeamertemplate{proof begin}#2}
{\usebeamertemplate{proof end}}
\makeatother

\makeatletter
\def\th@mystyle{%
\normalfont % body font
\setbeamercolor{block title example}{bg=orange,fg=white}
\setbeamercolor{block body example}{bg=orange!20,fg=black}
\def\inserttheoremblockenv{exampleblock}
}
\makeatother

\theoremstyle{mystyle}
\newtheorem{prop}{Proposition}

\renewcommand{\implies}{\Rightarrow}

\title{Hermitian Neural Networks}
\subtitle{joint with 小林俊行}
\author{レオンチエフ・アレックス、東大数理}

\begin{document}
\begin{frame}\titlepage\end{frame}
\begin{frame}{Outline}
\tableofcontents
\end{frame}
\newcommand{\mytime}[1]{(#1{分})}
\section{Intro: Orthogonal Neural Networks}
\begin{frame}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.3]{onn}
		\caption{Orthogonal Neural Network (taken from \cite{yang1996orthogonal})}
		\label{fig:onn}
		%\note{tesime}
	\end{figure}
	\begin{block}{Idea}
		Approximate signal $y(x)$ using orthogonal functions:\begin{equation*}
			y(x)\approx \hat{y}(x):=\sum_{i=1}^N \hat{\omega}_i  \Phi_i(x)
		\end{equation*}
	\end{block}
	This connects neural networks with functional analysis!
\end{frame}
\begin{frame}{Advantages}
	\begin{itemize}
		\item Architecture is simple;\pause
		\item Parameters are few;\pause
		\item Can choose particular OFs to suit the particular problem domain (e.g. Hermite functions for medical imaging);\pause
		\item Error estimation is readily available ($\implies$ can estimate convergence speed);\pause
		\item For some OFs we can use recurrence relations to speed up the computation (e.g. Lagrange, Hermite functions/polynomials)
	\end{itemize}
\begin{figure}
  \centering
  \vspace{-0.6cm}
  \subfloat{\includegraphics[scale=0.2]{onn2}}\qquad
  \subfloat{\raisebox{0.5cm}{\includegraphics[scale=0.4]{onn3}}}
\caption{(taken from \cite{yang1996orthogonal})}
\end{figure}
\end{frame}
\section{Intro: Convolution of Hermitian Neural Networks}
\begin{frame}{Why Hermite Networks}
	\begin{equation*}
		\begin{array}[]{c}
			h_n(t)=\frac{H_n(t)e^{-t^2/2}}{\sqrt{2^nn!\sqrt{\pi}}}\mbox{ (Hermite {\it functions})};\\
		H_n(t)=(-1)^n e^{t^2}\frac{d^n}{dt^n}e^{-t^2}\mbox{ (Hermite {\it polynomials})}.
		\end{array}
	\end{equation*}
	\pause
	\begin{itemize}
		\item Low order Hermite functions resemble some important signals in biomedical engineering \cite{mackenzie2003hermite};\pause
		\item Recursive formula $H_{n+1}(t)=2tH_n(t)-2nH_{n-1}(t)$ (see above);\pause
		\item Can be implemented in hardware \cite{mackenzie2003hermite};\pause
	\end{itemize}
	Besides, good analytic properties:\pause
	\begin{itemize}
		\item $h_0$ is Gaussian filter\vspace{-0.5cm}\begin{equation*}
				\implies e^{-t^2/2}\ast y\approx e^{-t^2/2} \ast \hat{y} =\sum_{i=1}^N \hat{\omega}_i \left( h_i\ast h_0 \right)
			\end{equation*}
			\vspace{-0.5cm}
			\pause
		\item Invariant under Fourier transform: $\mathcal{F}(h_n)=\left( -i \right)^n\sqrt{2\pi}h_n$ $\left( \implies
			\mathcal{F}\left( \hat{y} \right)=\sum_{j=1}^N \hat{\omega}_j (-i)^j \sqrt{2\pi}h_j \right)$;
	\end{itemize}
\end{frame}
\section{Intro: Hermite-Rodriguez Networks}
\section{My contribution}
\section{Further work}
\section{Q\&{A}\mytime{20}}
\begin{frame}
	\begin{center}
		\Huge Q\&{A}
	\end{center}
\end{frame}
\begin{frame}[allowframebreaks]
\frametitle{References}
\nocite{mackenzie2003hermite}
\nocite{yang1996orthogonal}
\bibliographystyle{apalike}
\bibliography{fmsp.bib}
\end{frame}
\end{document}
