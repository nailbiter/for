{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3caf8509-1719-4af1-82a7-fb5f4843f400",
   "metadata": {},
   "source": [
    "Langgraph workflow:\n",
    "1. initialize model and tools\n",
    "2. initialize graphs with state\n",
    "3. define graph nodes\n",
    "4. define entry points and edges\n",
    "5. compile the graph\n",
    "6. exec the graph\n",
    "\n",
    "node = fundamental unit of exec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd002dba-b9b3-438c-8bdc-268cddaff136",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41932774-af96-4ba8-8f2d-9d3fad4a2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a889f903-2d5f-48c5-bd8a-12083f5b1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph Tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40bcb953-3735-402d-ba68-f030719d54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9930c22b-5354-4b7a-87d1-b4bcccd91946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90978df6-48bb-4349-939a-4107a79a35e4",
   "metadata": {},
   "source": [
    "## 104. Simple LangGraph\n",
    "https://drive.google.com/file/d/1bJ7p5TSFCv2lWAv-ZtFPOrRoOpj3Lw2h/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d30bb5a-6664-4bc8-8583-52fe70050e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d39cab49-bcca-4341-9e79-3ed42a4bd301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10a371790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f111ca9-a241-452c-9811-d79792fe704a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10a371790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6e84d8-b885-4329-b048-74d7e5188d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10a371790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.set_finish_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ffe2ec-ad2d-48ae-a168-bd3513257f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457c71ed-614c-4bda-b64f-6573dd46791f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "603d5fcf-c8f7-408f-a34f-30732b02a659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  why is the sky blue?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with molecules and small particles in the air. Sunlight consists of different colors, each with its wavelength. Blue light waves are shorter and scatter more than the other colors when they strike the gas molecules in the atmosphere. This scattering causes the blue light to spread in all directions and become more visible, making the sky appear blue to our eyes.\\n\\nDuring sunrise or sunset, the sky can appear red or orange because the sun is lower in the sky, and its light passes through a thicker layer of the atmosphere. In this case, the shorter blue wavelengths are scattered out of the line of sight, and the longer red wavelengths dominate.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 148, 'prompt_tokens': 13, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_df0f7b956c', 'id': 'chatcmpl-CANcSYYSJkKAnEs90fk5zkwTC20tK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--85b0d3d4-35eb-4a4b-acaf-9cf7eedceaa6-0', usage_metadata={'input_tokens': 13, 'output_tokens': 148, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "Assistant: The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with molecules and small particles in the air. Sunlight consists of different colors, each with its wavelength. Blue light waves are shorter and scatter more than the other colors when they strike the gas molecules in the atmosphere. This scattering causes the blue light to spread in all directions and become more visible, making the sky appear blue to our eyes.\n",
      "\n",
      "During sunrise or sunset, the sky can appear red or orange because the sun is lower in the sky, and its light passes through a thicker layer of the atmosphere. In this case, the shorter blue wavelengths are scattered out of the line of sight, and the longer red wavelengths dominate.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  why is the grass green?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"The grass appears green primarily due to the presence of chlorophyll, a pigment found in the chloroplasts of plant cells. Chlorophyll is essential for photosynthesis, the process by which plants convert light energy from the sun into chemical energy. Chlorophyll absorbs light most efficiently in the blue and red wavelengths, but it reflects and transmits the green wavelengths, which is why grass looks green to our eyes. Additionally, chlorophyll plays a crucial role in capturing light energy and facilitating the synthesis of carbohydrates, which fuel the plant's growth and metabolic functions.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 13, 'total_tokens': 123, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CANctWbc3PR5mZEYtMNKCi9hhfbCC', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5e64ae3a-55ad-416c-91b4-0b23737985ec-0', usage_metadata={'input_tokens': 13, 'output_tokens': 110, 'total_tokens': 123, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "Assistant: The grass appears green primarily due to the presence of chlorophyll, a pigment found in the chloroplasts of plant cells. Chlorophyll is essential for photosynthesis, the process by which plants convert light energy from the sun into chemical energy. Chlorophyll absorbs light most efficiently in the blue and red wavelengths, but it reflects and transmits the green wavelengths, which is why grass looks green to our eyes. Additionally, chlorophyll plays a crucial role in capturing light energy and facilitating the synthesis of carbohydrates, which fuel the plant's growth and metabolic functions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Summarize what did I asked you during this session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"I'm sorry, but as an AI language model, I don't have the ability to retain or recall information from previous interactions. Each session is independent, and I don't store any personal data or conversation history. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 19, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CANdLaCCXiCkWWp9fX8CoNPERBjzV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4024b916-f550-41f5-a1d8-a1240debe4ea-0', usage_metadata={'input_tokens': 19, 'output_tokens': 47, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "Assistant: I'm sorry, but as an AI language model, I don't have the ability to retain or recall information from previous interactions. Each session is independent, and I don't store any personal data or conversation history. How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in graph.stream({\"messages\": (\"user\", user_input)}):\n",
    "        print(event[\"chatbot\"][\"messages\"])\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377943d2-2566-45d7-8974-9ae12b8b3ab4",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got hi\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     run_with_retry(\n\u001b[1;32m    163\u001b[0m         t,\n\u001b[1;32m    164\u001b[0m         retry_policy,\n\u001b[1;32m    165\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    166\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[1;32m    167\u001b[0m                 _call,\n\u001b[1;32m    168\u001b[0m                 weakref\u001b[38;5;241m.\u001b[39mref(t),\n\u001b[1;32m    169\u001b[0m                 retry_policy\u001b[38;5;241m=\u001b[39mretry_policy,\n\u001b[1;32m    170\u001b[0m                 futures\u001b[38;5;241m=\u001b[39mweakref\u001b[38;5;241m.\u001b[39mref(futures),\n\u001b[1;32m    171\u001b[0m                 schedule_task\u001b[38;5;241m=\u001b[39mschedule_task,\n\u001b[1;32m    172\u001b[0m                 submit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[1;32m    173\u001b[0m             ),\n\u001b[1;32m    174\u001b[0m         },\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_write.py:87\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     80\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     86\u001b[0m     ]\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(\n\u001b[1;32m     88\u001b[0m         config,\n\u001b[1;32m     89\u001b[0m         writes,\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_write.py:129\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# if we want to persist writes found before hitting a ParentCommand\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# can move this to a finally block\u001b[39;00m\n\u001b[1;32m    128\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_SEND]\n\u001b[0;32m--> 129\u001b[0m write(_assemble_writes(writes))\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_write.py:184\u001b[0m, in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    182\u001b[0m     tuples\u001b[38;5;241m.\u001b[39mappend((TASKS, w))\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteTupleEntry):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ww \u001b[38;5;241m:=\u001b[39m w\u001b[38;5;241m.\u001b[39mmapper(w\u001b[38;5;241m.\u001b[39mvalue):\n\u001b[1;32m    185\u001b[0m         tuples\u001b[38;5;241m.\u001b[39mextend(ww)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteEntry):\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/graph/state.py:983\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m    980\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    981\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[1;32m    982\u001b[0m     )\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got hi\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
      "\u001b[0mDuring task with name '__start__' and id '535ce9a8-71d7-5e10-1121-e92417f89f3e'"
     ]
    }
   ],
   "source": [
    "graph.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7796d5-d89c-47a7-baab-686e8dfb5b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hey, how are you?', additional_kwargs={}, response_metadata={}, id='8f049b8d-63e4-4c55-949f-08e140e010fa'),\n",
       "  AIMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 13, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAORtKUV6Brs933oHu7Z2yB9O05yk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b763fa64-ff4f-479a-93ad-5607b653ac0b-0', usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"Hey, how are you?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0c738-e306-4f6f-ad09-97e4c220dd88",
   "metadata": {},
   "source": [
    "## 105. Tool Usage and Persistence\n",
    "* https://drive.google.com/file/d/113yjenUddxEMnHX6Ud3aqSF5jf7ee4x2/view?usp=drive_link\n",
    "* https://www.tavily.com/\n",
    "\n",
    "### tool usage\n",
    "```sh\n",
    "pip install tavily-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f353437-75a8-4f16-8398-4702446d2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert {\"TAVILY_API_KEY\"} <= set(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841f3a43-f1b2-4af5-826f-7fd5089b73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/m1j4q6jj79zbjw0qtxcldpnc0000gn/T/ipykernel_5566/3979812345.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'What is LangGraph? - IBM',\n",
       "  'url': 'https://www.ibm.com/think/topics/langgraph',\n",
       "  'content': 'Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as â€œactorsâ€ that interact with each other in a specific way. For example, to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.',\n",
       "  'score': 0.9263638},\n",
       " {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\",\n",
       "  'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
       "  'content': 'Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses.\\n Nodes: Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step. You define nodes to perform specific tasks, such as processing input, making decisions, or interacting with external APIs. [...] ## Get CPlogâ€™s stories in your inbox\\n\\nJoin Medium for free to get updates from this writer.\\n\\n## Step 3: Define Nodes\\n\\nWe define nodes for classifying the input, handling greetings, and handling search queries.',\n",
       "  'score': 0.86954874}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "tool.invoke(\"What's a 'node' in LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69234f70-170f-4603-830e-176fd3ae09c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x112944650>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Modification: tell the LLM which tools it can call\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "772b80bb-da91-4b4f-85c2-76d06f76d436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x112944650>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## note: can be replaced with pre-built `ToolNode`\n",
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "tool_node = BasicToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d907bf6c-131d-4362-865d-f528f875d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    ") -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n",
    "\n",
    "\n",
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"__end__\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3445ba-68c8-43f3-b419-75d03a570e09",
   "metadata": {},
   "source": [
    "You've hit on a crucial point, and your confusion is completely understandable. For the simple task of calling a single tool, LangGraph absolutely looks like overkill.\n",
    "\n",
    "You're not missing anything in your observation; you're just seeing the difference between a simple, pre-built tool and a powerful, low-level framework.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "* **`AgentExecutor` is like buying a pre-built, factory-made car.** Itâ€™s easy to use, works great for standard driving, and gets you from point A to B reliably. ðŸš—\n",
    "* **`LangGraph` is like being given a high-performance engine, chassis, wheels, and a toolbox.** You have to assemble the car yourself, which is more work. But you can build a Formula 1 racer, a monster truck, or a multi-person vehicleâ€”whatever you can imagine. ðŸŽï¸ðŸ› ï¸\n",
    "\n",
    "The complexity of LangGraph is the price of **ultimate control and flexibility**.\n",
    "\n",
    "---\n",
    "#### ## Key Benefits LangGraph Provides (That `AgentExecutor` Doesn't)\n",
    "\n",
    "The \"obvious benefits\" you were looking for only appear when your agent's logic needs to be more complex than the simple `Think -> Act -> Observe -> Think` loop that `AgentExecutor` forces you into.\n",
    "\n",
    "##### **1. Full Control Over the \"Loop\"**\n",
    "\n",
    "* **`AgentExecutor` has a fixed, hidden loop.** The agent calls an LLM, the LLM decides to use a tool, the tool is run, and the result is fed back to the LLM. You can't easily change this sequence.\n",
    "* **`LangGraph` makes you define the loop explicitly using nodes and edges.** This is its superpower. It means you can create any logic you want, such as:\n",
    "    * **Cycles:** Go back to a previous step if the result isn't good enough.\n",
    "    * **Human-in-the-Loop:** Add a node that pauses the process and waits for a human to approve the LLM's plan before executing a costly tool.\n",
    "    * **Multi-Agent Workflows:** Have one \"Planner\" agent create a task list, and then route tasks to multiple \"Worker\" agents that can run in parallel.\n",
    "    * **Conditional Logic:** If a tool fails, route to a \"Fallback\" node. If the user's query is ambiguous, route to a \"Clarification\" node that asks the user a question.\n",
    "\n",
    "##### **2. Explicit State Management**\n",
    "\n",
    "* **`AgentExecutor` manages state internally** in the `agent_scratchpad`. This works, but it's like a black box.\n",
    "* **`LangGraph` forces you to define an explicit `State` object** (the `TypedDict` in your code). This seems like extra work, but it's a massive advantage for complex tasks. You have complete control over the agent's \"memory.\" At any node, you can read from, add to, or modify the state with precision.\n",
    "\n",
    "##### **3. Enhanced Debugging and Visibility**\n",
    "\n",
    "* Because `AgentExecutor` is a high-level abstraction, it can sometimes be difficult to pinpoint exactly where things went wrong inside its internal loop.\n",
    "* With `LangGraph`, every step (**node**) and every transition (**edge**) is a piece of code you wrote. This makes it far easier to trace the flow of data and debug complex interactions. When you visualize a LangGraph (especially with LangSmith), you see *your* custom logic, not a generic loop.\n",
    "\n",
    "---\n",
    "###### ## When to Use Which\n",
    "\n",
    "Your Udemy course is likely starting with `AgentExecutor` because it's the fastest way to understand the basics. Then it introduces `LangGraph` for more advanced, real-world applications.\n",
    "\n",
    "* **Use `AgentExecutor` for:**\n",
    "    * **Prototyping:** Quickly building standard agents that follow the simple `(LLM -> Tool)` pattern.\n",
    "    * **Simple Tool Use:** When all you need is for an LLM to reliably call one or more tools and return a final answer.\n",
    "\n",
    "* **Use `LangGraph` for:**\n",
    "    * **Complex Control Flow:** Anything that isn't a straight line (e.g., requires cycles, branching, or retries).\n",
    "    * **Human-in-the-Loop:** When a person needs to be part of the decision-making process.\n",
    "    * **Multi-Agent Systems:** When you need to orchestrate collaboration between multiple different chains or agents.\n",
    "    * **Production Applications:** When you need the reliability, state control, and debuggability required for a robust, long-running agent.\n",
    "\n",
    "So, for your example, `AgentExecutor` is simpler and better. But if you wanted to ask, \"How many letters are in the word 'data', and if the answer is less than 5, search for a new word and try again,\" you would immediately need the cyclic power of **LangGraph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74be37-6887-419a-857f-f2d8b4834fac",
   "metadata": {},
   "source": [
    "### adding memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b1621-23e4-48ae-814f-9bf2a5b2d820",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install langgraph-checkpoint-sqlite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecd2479e-8ace-45fc-a8d8-7a5428a2ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4922223-6c38-4445-8461-d56d1bdc4370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x112c437d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88028630-5078-4c01-981d-3cf7d3e2090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b99465b8-008f-4ce7-8703-cce6f0c3afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "952d7ca2-b6af-4c4a-90a1-cfcbd19a986e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_GeneratorContextManager' object has no attribute 'get_next_version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The config is the **second positional argument** to stream() or invoke()!\u001b[39;00m\n\u001b[1;32m      4\u001b[0m events \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)]}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(event)\n\u001b[1;32m      9\u001b[0m     event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:2582\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m runtime \u001b[38;5;241m=\u001b[39m parent_runtime\u001b[38;5;241m.\u001b[39mmerge(runtime)\n\u001b[1;32m   2580\u001b[0m config[CONF][CONFIG_KEY_RUNTIME] \u001b[38;5;241m=\u001b[39m runtime\n\u001b[0;32m-> 2582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SyncPregelLoop(\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2584\u001b[0m     stream\u001b[38;5;241m=\u001b[39mStreamProtocol(stream\u001b[38;5;241m.\u001b[39mput, stream_modes),\n\u001b[1;32m   2585\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m   2586\u001b[0m     store\u001b[38;5;241m=\u001b[39mstore,\n\u001b[1;32m   2587\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   2588\u001b[0m     checkpointer\u001b[38;5;241m=\u001b[39mcheckpointer,\n\u001b[1;32m   2589\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes,\n\u001b[1;32m   2590\u001b[0m     specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels,\n\u001b[1;32m   2591\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   2592\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   2593\u001b[0m     stream_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_channels_asis,\n\u001b[1;32m   2594\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   2595\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   2596\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   2597\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability_,\n\u001b[1;32m   2598\u001b[0m     trigger_to_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_to_nodes,\n\u001b[1;32m   2599\u001b[0m     migrate_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_migrate_checkpoint,\n\u001b[1;32m   2600\u001b[0m     retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2601\u001b[0m     cache_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_policy,\n\u001b[1;32m   2602\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[1;32m   2603\u001b[0m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[1;32m   2604\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PregelRunner(\n\u001b[1;32m   2605\u001b[0m         submit\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   2606\u001b[0m             CONFIG_KEY_RUNNER_SUBMIT, weakref\u001b[38;5;241m.\u001b[39mWeakMethod(loop\u001b[38;5;241m.\u001b[39msubmit)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2609\u001b[0m         node_finished\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[1;32m   2610\u001b[0m     )\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_loop.py:934\u001b[0m, in \u001b[0;36mSyncPregelLoop.__init__\u001b[0;34m(self, input, stream, config, store, cache, checkpointer, nodes, specs, trigger_to_nodes, durability, manager, interrupt_after, interrupt_before, input_keys, output_keys, stream_keys, migrate_checkpoint, retry_policy, cache_policy)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack \u001b[38;5;241m=\u001b[39m ExitStack()\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpointer:\n\u001b[0;32m--> 934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_get_next_version \u001b[38;5;241m=\u001b[39m checkpointer\u001b[38;5;241m.\u001b[39mget_next_version\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes \u001b[38;5;241m=\u001b[39m checkpointer\u001b[38;5;241m.\u001b[39mput_writes\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes_accepts_task_path \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    937\u001b[0m         signature(checkpointer\u001b[38;5;241m.\u001b[39mput_writes)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    939\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_GeneratorContextManager' object has no attribute 'get_next_version'"
     ]
    }
   ],
   "source": [
    "user_input = \"Hi there! My name is Alex Leontiev!\"\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    print(event)\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4eb7651e-0cbe-4b3a-80f4-d30f0ce7b60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_GeneratorContextManager' object has no attribute 'get_next_version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# {'messages':[('human','How are you?')]}\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)]}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:2582\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m runtime \u001b[38;5;241m=\u001b[39m parent_runtime\u001b[38;5;241m.\u001b[39mmerge(runtime)\n\u001b[1;32m   2580\u001b[0m config[CONF][CONFIG_KEY_RUNTIME] \u001b[38;5;241m=\u001b[39m runtime\n\u001b[0;32m-> 2582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SyncPregelLoop(\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2584\u001b[0m     stream\u001b[38;5;241m=\u001b[39mStreamProtocol(stream\u001b[38;5;241m.\u001b[39mput, stream_modes),\n\u001b[1;32m   2585\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m   2586\u001b[0m     store\u001b[38;5;241m=\u001b[39mstore,\n\u001b[1;32m   2587\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   2588\u001b[0m     checkpointer\u001b[38;5;241m=\u001b[39mcheckpointer,\n\u001b[1;32m   2589\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes,\n\u001b[1;32m   2590\u001b[0m     specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels,\n\u001b[1;32m   2591\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   2592\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   2593\u001b[0m     stream_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_channels_asis,\n\u001b[1;32m   2594\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   2595\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   2596\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   2597\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability_,\n\u001b[1;32m   2598\u001b[0m     trigger_to_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_to_nodes,\n\u001b[1;32m   2599\u001b[0m     migrate_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_migrate_checkpoint,\n\u001b[1;32m   2600\u001b[0m     retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2601\u001b[0m     cache_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_policy,\n\u001b[1;32m   2602\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[1;32m   2603\u001b[0m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[1;32m   2604\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PregelRunner(\n\u001b[1;32m   2605\u001b[0m         submit\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   2606\u001b[0m             CONFIG_KEY_RUNNER_SUBMIT, weakref\u001b[38;5;241m.\u001b[39mWeakMethod(loop\u001b[38;5;241m.\u001b[39msubmit)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2609\u001b[0m         node_finished\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[1;32m   2610\u001b[0m     )\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_loop.py:934\u001b[0m, in \u001b[0;36mSyncPregelLoop.__init__\u001b[0;34m(self, input, stream, config, store, cache, checkpointer, nodes, specs, trigger_to_nodes, durability, manager, interrupt_after, interrupt_before, input_keys, output_keys, stream_keys, migrate_checkpoint, retry_policy, cache_policy)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack \u001b[38;5;241m=\u001b[39m ExitStack()\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpointer:\n\u001b[0;32m--> 934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_get_next_version \u001b[38;5;241m=\u001b[39m checkpointer\u001b[38;5;241m.\u001b[39mget_next_version\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes \u001b[38;5;241m=\u001b[39m checkpointer\u001b[38;5;241m.\u001b[39mput_writes\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes_accepts_task_path \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    937\u001b[0m         signature(checkpointer\u001b[38;5;241m.\u001b[39mput_writes)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    939\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_GeneratorContextManager' object has no attribute 'get_next_version'"
     ]
    }
   ],
   "source": [
    "graph.invoke(\n",
    "    # {'messages':[('human','How are you?')]}\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae3890d0-342d-4cf7-9432-96896ef20481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First interaction ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Alex Leontiev! How can I assist you today?\n",
      "\n",
      "--- Second interaction (testing memory) ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You mentioned that your name is Alex Leontiev.\n"
     ]
    }
   ],
   "source": [
    "# from langgraph_sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Use a 'with' statement to properly manage the memory connection\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    # 1. Compile the graph with the checkpointer INSIDE the 'with' block\n",
    "    graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "    # 2. Define a configuration for a specific conversation thread\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    # 3. Interact with the graph INSIDE the 'with' block\n",
    "    print(\"--- First interaction ---\")\n",
    "    user_input = \"Hi there! My name is Alex Leontiev!\"\n",
    "    response = graph.invoke({\"messages\": [(\"user\", user_input)]}, config)\n",
    "    # Pretty-print the final response\n",
    "    response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    print(\"\\n--- Second interaction (testing memory) ---\")\n",
    "    user_input_2 = \"What did I say my name was?\"\n",
    "    response_2 = graph.invoke({\"messages\": [(\"user\", user_input_2)]}, config)\n",
    "    response_2[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83bf472-dc7d-496d-8003-fdb5831172ae",
   "metadata": {},
   "source": [
    "## 106. Human in the loop\n",
    "* https://colab.research.google.com/drive/113yjenUddxEMnHX6Ud3aqSF5jf7ee4x2#scrollTo=dba1b168-f8e0-496d-9bd6-37198fb4776e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e9a2c5-6a7b-4cc8-b78b-0d291437b633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x116afb530>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db3a043a-fd93-48e7-9511-16bf4b468125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- interaction #1 ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_ELU44JJPnQB2Cxz0eh8LnCs8)\n",
      " Call ID: call_ELU44JJPnQB2Cxz0eh8LnCs8\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "('tools',)\n",
      "[{'name': 'tavily_search_results_json', 'args': {'query': 'LangGraph programming language'}, 'id': 'call_ELU44JJPnQB2Cxz0eh8LnCs8', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "# from langgraph_sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Use a 'with' statement to properly manage the memory connection\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    # 1. Compile the graph with the checkpointer INSIDE the 'with' block\n",
    "    graph = graph_builder.compile(\n",
    "        checkpointer=memory,\n",
    "        # This is new!\n",
    "        interrupt_before=[\"tools\"],\n",
    "    )\n",
    "\n",
    "    # 2. Define a configuration for a specific conversation thread\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    # 3. Interact with the graph INSIDE the 'with' block\n",
    "    messages = [\n",
    "        \"\"\"I'm learning LangGraph. Could you do some research on it for me?\"\"\"\n",
    "        # \"Hi there! My name is Alex Leontiev!\",\n",
    "        # \"What did I say my name was?\",\n",
    "    ]\n",
    "    for i, user_input in enumerate(messages):\n",
    "        print(f\"--- interaction #{i+1} ---\")\n",
    "        # user_input =\n",
    "        response = graph.invoke({\"messages\": [(\"user\", user_input)]}, config)\n",
    "        # Pretty-print the final response\n",
    "        response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    # print(\"\\n--- Second interaction (testing memory) ---\")\n",
    "    # user_input_2 =\n",
    "    # response_2 = graph.invoke({\"messages\": [(\"user\", user_input_2)]}, config)\n",
    "    # response_2[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    snapshot = graph.get_state(config)\n",
    "    print(snapshot.next)\n",
    "\n",
    "    existing_message = snapshot.values[\"messages\"][-1]\n",
    "    print(existing_message.tool_calls)\n",
    "\n",
    "    ## allow continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "802356de-f01c-4b05-9c6b-3ffecdd3d6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAAEjCAIAAADllbCOAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3XdcU+f+B/Ane0Iie8mGgoCKori9itrqFSdW3Fpt1aoVR3ut6G0duOrWWmut2jpKrdZRN7aOCm5FZe8tI6yE7PX7I/6oVwMykjwZ3/erf9AknHyCfDjnOeM5BLVajQAAmBBxBwDAokEDAcAJGggATtBAAHCCBgKAEzQQAJzIuAOYm7oqmaBWIRIoRXyFXKZCiIA70btR6UQ6k8i0IrM7kG0cqbjjWBYCHA/UicpiSe5zYX6K0NqWrJCpmVYkpjWZQiMQTKGBajXi18hFAgWNQawqkXkFs7xDWC7eDNy5LAI0sL1qK2VJf1RTGYQO9lSvYJatMw13onapq5Llpwiry2UNtYo+I20d3Om4E5k5aGC73LvIy3km7BNp6x3Cxp1Fx4qzREkXqp096QPG2ePOYs6ggW13cntx139x/LtZ4w6iRwVpwlunqiZ94U6lw047vYAGtoVKpd7/Re74z9wcLWAjjV8j/2VL0ey1XmQqlFD3oIFtsXdJztzN3hRL+o38ITZv6pceDDYJdxBzY0G/Q7oSv7Vo4rKOFlU/hNCUFe4nNhfhTmGGYB3YOol/8Bzd6b5dzG2/S0uU5YkzHvAHRzviDmJWLOsPeTvxSqVFGSLLrB9CyMWbIeQrC9KEuIOYFWhgKyT+wesbaYc7BU59RtomXajGncKsQANbqixXZG1DcQ9g4g6Ck60LzTOAmfu8AXcQ8wENbKmcZ0IbJzhnEjm407OeCHCnMB/QwJbKTxF6BbMM/KZDhw4tLS1t7Xfl5uaOHDlSP4mQVzArPwWGgjoDDWwRXpnU1oVqbUMx5Ju+fPmytra2Dd+YlpamhzivkMgE/+5WRelQQt2ABrZIPU9OJOrrKge1Wn3ixInJkyf37dt36tSpe/fuVSqVjx49ioyMRAiNHj162bJlmjXb5s2bo6Ki+vTpM3Xq1FOnTmm+PScnJyws7M6dOx988MGkSZP279+/Zs2a8vLysLCw48eP6yMwlUasrZLrY8kWCK4PbBEhX8Gy1tfPKj4+/tChQzExMX379r158+a3337LYrFmzZq1c+fOmJiYc+fOubq6IoS2bdtWVlYWGxtLIBAKCgo2b97s7Ozct29fCoWCEDp48OC0adO6du0aFBQkk8muXbt24cIFPQVmWZOFfIWeFm5poIEtIqxXsjj6OiHryZMnnTp10ozcxo4d26NHD5FI9PbLNm7cKBQKXVxcEEJhYWHnz59PSkrq27cvgUBACPXq1WvKlCl6SvgGFodUUyEzzHuZPWhgyxCQ/s5L7tKly549e9auXRsaGjpgwAA3NzetL1Or1fHx8YmJiYWFhZpHNOtGjcDAQD3FexuJQiDC+aE6Ag1sEQaLJKjR18hn8uTJLBbr1q1ba9asIZPJQ4cO/eyzz+zt/+eqPJVKtXjxYplMtnDhwrCwMCsrq9mzZ7/+AhrNcFcGN9QqaAyooG5AA1uEZU0qL5DoaeFEInHs2LFjx47Ny8t78ODBgQMHGhoaduzY8fprMjIyUlNT9+3b17NnT80jAoHAwcFBT5GaJ+QrWdbQQN2ABraIlQ2ZrLej8RcuXAgMDPTx8fH29vb29hYIBGfOnHnjNXV1dQihxsrl5eXl5eX5+PjoK9O7cOwMemDGjMHRiBZx8mAUpIjEQqU+Fn7lypXPP//89u3b9fX1d+7c+euvv7p06YIQ8vT0RAglJCSkpKR4e3uTyeSjR4/y+fyCgoJvvvmmV69eL1++1LpAd3d3Ho938+bNxhGjbj3/u84j0NAnJ5graGBLeQaxClL1chh61apV3t7eS5cujYiIWLdu3cCBA2NjYxFCbm5ukZGR+/fv37Nnj5OT0/r161+8eDF48OAlS5YsWLAgKioqJSUlKirq7QX269eva9euy5cvv3r1qs7TFmWIXH0ZJLIJzAFnEuD6wJbKT2kozhYPGGvp0xbdv1ptxSV3CufgDmImYB3YUl7B7NIcMa9UijsITiKBIuUOH+qnQ7AObIWiDNHTm7Wj57lqfba4uHjatGlanyIQmvw5jxkzJiYmRqcx/xETE5OcnKz1KQ6HU19fr/WppUuXjho1SutTf/5S4ezN6BRuztPDGRg0sHX+jK8IDLd28dIyn7RKpRIKtQ8UxWIxg6F9CmoKhUKn62vCNZFIpFRq33skl8s1p7O9jUajUala9vzWV8sTz/FGfOSs65gWDRrYat+vyJ31tZcFzp9psR9cr+Cn2WqTvrDEWcN+3Vo8ap4L1E/nYB3YFuIG5cntxVNWuFvIJLa/biv+YKYjxxamCNA9i/gF0jkGmzR6vssPq/Iri/V1qpqRqKuSff+f3AHj7aB+egLrwHa5frxCJlX1ibTl2pvbL6hIoEj6o1omUQ2Z4kilwV9qfYEGtlfu84akP6p9Q9lO7nTDTySjD0UZovIC8YtEfp9I28CecOBBv6CBupH1mJ/1tKEgVRTSj0MkIhaHzLImU+gEk7iHrkKuEtYphHyl5pxPV1+Gf6hVIBz0MwhooI4VpAnrquTCeoVQcxdrtS4bWFFRIZfLm7qEt83oLCKNQWJZkzh2FI9AFpzzaUjQQFNy7NgxHo+nv3NogOHBCBsAnKCBAOAEDQQAJ2ggADhBAwHACRoIAE7QQABwggYCgBM0EACcoIEA4AQNBAAnaCAAOEEDAcAJGggATtBAAHCCBgKAEzQQAJyggQDgBA0EACdoIAA4QQMBwAkaCABO0EAAcIIGmhIqldrUnUCBiYIGmhKZTCYWi3GnALoEDQQAJ2ggADhBAwHACRoIAE7QQABwggYCgBM0EACcoIEA4AQNBAAnaCAAOEEDAcAJGggATtBAAHCCBgKAEzQQAJwIarUadwbwDqNGjSIQCAqFQigUIoQ4HI5CoUAIXbx4EXc00F5k3AHAu/n5+d28eZNAIGj+t6GhQaVShYeH484FdAC2Qk3AzJkz7ezsXn/ExsYmOjoaXyKgM9BAExASEhIcHPz6I15eXgMHDsSXCOgMNNA0zJw508bGRvM1h8OZPn067kRAN6CBpiEkJKRLly6ar318fPr37487EdANaKDJmDFjho2NDYfDmTp1Ku4sQGdgX2i7SETK6jKZVKIywHsxkVdYp383NDS42XTPSxEa4B0pVIKtM5VpBb8kegTHA9tIrVJfPVpelCF29WUqFeb5M6SzSEUZQmcvesQkBzqThDuOeYIGtoVMqjq9uyR0kK2rHwt3Fr2rLpMknqsYt9CNwYYS6h6MA9vi9K6SPqMcLaF+CCFbF/rQaa4nNhfhDmKeoIGtlvGI7+LDtHGi4Q5iOAw2uVNvbvLNWtxBzBA0sNUqi6V0tsXtnGBzyS8LpLhTmCFoYKvJxCorGwruFIZmbUdVSGGXge5BA1tNJlGpDXH0wbiolEgoUOBOYYaggQDgBA0EACdoIAA4QQMBwAkaCABO0EAAcIIGAoATNBAAnKCBAOAEDQQAJ2ggADhBA7EpKSkaFBH28NG99ixk9NiIn48e1F0oYGjQQNMzdvzQspel7VnCmrUrLl0+p7tEoO2ggSamvPxlXV17r5TNzEzTURzQXhZ3pSkWfAH/++93Xbp8jsPhhnUP/3jOIkdHp8Znt22Pu3DxjK2t3YD+gz9b9IXmwbt3//7rxtXnL57y+fWBAcHTps0J7Rr2NPnR0mXzEEJTpo7u23fg+rXbNC8+c/bklSvnS8uKu4X2XLpkJZfbQfP4z0cPXr12gcerdHBw6tql+5KYL4lE4qCIMITQN1vXXb5yfs+uH3H8PMA/YB2odwqFYsWXn/Gqq7Zv279o4eeVVRUrVn6mufkRQujwkf2dO3fbvm3/hxOmnjl78q8b1xBCEokkbuMqqVS64j9rNsTtdHf3jF21pKamOrRr2Ma4nQih48fONdbv8uVztbXV8+bFxH65Pjn50d5vtzYu+ey5k/Pnxpz67ersjz69eSvht1PHEUJXLiUihD5fvhrqZwxgHah39+7fSU9P+enwKXd3T4RQx44eJ387VlNTrXk2tGvY0CHDNV/8fib+xYungwcNo9PpBw/EMxgMDoeLEAoMCD53/tSLlOSBAyLeXj6DyZw1c57mzkojR447dfqETCaTyqS/xP80f96Sfv3+hRD618AheXnZx47/OG4s3O/FuEAD9S43N5vJZGrqhxDy9wtYtXK9Zl8oQigkuGvjKznWXKn01VwsIpHw4I97k589rq7maR5pavgX1r1X443NOnUKkcfLedVVdXW1crk8MPCf+734+wc2NDSUlhY7O7vq7bOCVoOtUL0TChtoNHpTz5LIWv4IVlSUL14yRy6Xr47dcO3K3YSrzR2xYDL/mTSRwWAihOrr62pqeAgh+mvvq3lKLBa146MA3YN1oN4xmSyxWKRSqYjElv69u3krQSaTrfjPGgaD0czaT0MiETd+LRQ2IIQ4HK7mQfFrT4lEQoSQjY1dE4sBeMA6UO8C3uskkUgys9I1/1tUVBCz9JPc3OxmvoXPr7eystbUDyF06/afzbw4Jyez8evMzDQqlWpv5+Dj408ikVJTnzU+lZ6eYsW2srd3aPcHAroEDdS7sLBerq4dDxzY/fedGw8f3du5a1NVZYWHh1cz3+Lt7VddzTv/x2mFQnH/QdKTJw84HG5lZTlCqKO7J0Lo5s2EtPQUzYvzC3JP/nZMqVRmZWdcvXZhQP/BFArF2sp66JARx44fSkq6zRfwr127eObsr1FRU4hEIo1Gs7d3ePToXnpGqqF+BqBJsBWqd2QyeeuWfRs3//e/X32OEOrdu//GDbvI2oZ/jSIGv19YmPfz0R927NzYI6zXf774Ov7Xn0/8ckQg4C9dsvKD9yMPH9kfHNRlx/bvFQr5pOgZqanPv9u/k8Vi9QjrvXDBcs1CFny6jEgkrotbqVAoXFzcJk+aNSl6huapKZM/Onxkf2VVxd7dhwzyMwBNgju3tNqlQy89gqzdAyziphGNeGXS+xcro5d3xB3E3MBWKAA4QQMBwAkaCABO0EAAcIIGAoATNBAAnKCBAOAEDQQAJ2ggADhBAwHACRoIAE7QQABwggYCgBM0sNVYXAoi4A5heGrUwZGCO4QZgga2GsuKWFUswZ3C0KpKxXQGCXcKMwQNbB2BQPAkM6GhRoY7iKHVVkg9g5i4U5ghaGDrREdH+3ayd/am3zlbgTuL4Ty8ymOySB6BlnVRsmHANfItcvjw4YCAgN69ezc+8uzv+vxUoUcA286VTqaa5x8ypVLFK5VWForZHFKfSFuEkFgsbpw/CugEzBPzbkePHhUKha/XDyHUpT/HzpmS/kBQmN5QX2mgjVKFUqlWqynNzjGjQzbOVBqD6NuF7d2ZjRCKiooSCAQMBoNOp9NoNC6Xa2Njw+VyFy9ebJg8ZgnWgU1KSEi4cePGhg0b5HI5hWIUuwGPHTvG4/FiYmKwvPvx48cPHDggEAg0/0sgvPrlUavVT58+xRLJDJjn5lM7NTQ0KJXKP//8c+nSpQghI6kfQqh3797Dhg3D9e5TpkwJDAxECBGJRCKRSCAQiEQi1K+doIH/o7y8fO7cuTwej0Qibdq0yc7OuGaY9vHx6dSpE8YAS5cudXX9n9tOGNuPyORAA1+pqqpCCN26devjjz/29PTEHUe7u3fvXrt2DWMAf3//ESNGNG4UqFQqGxub58+fY4xk6qCBSK1Wr1mz5scff0QITZw4MSwsDHeiJuXm5qalYb797fz58z08PDQjQFdX17i4uB07dnz11VeNd30CrUL6+uuvcWfARqVS1dXVyWQyqVQ6e/Zs3HHejcVieXp62tvb443h7u5+7949lUp1/fp1GxubMWPGCIXCOXPmMJnMkJAQvNlMjuWuAxMTE8PDw4lEIofDGTVqFO44LYJ9HKjRs2fP8PDwxMTExkdGjhyZlJRUWloaFRUFO2ZaxRKPRjx8+LBHjx43btwYNGgQ7iytc/fuXYFAgHF36Dvl5+fHxcU5OjrGxsYymXAW27tZ1jpQJBKNGDFCs9PF5OpnJOPA5nl5eR08eLB///7vv//+sWPHcMcxAZayDrxz505wcLBSqVQoFI6OjrjjtFFubq5UKjWGDdGW2LFjx+3bt1euXNmjRw/cWYyXRTRw3759mZmZ27Zta/6eYUDnioqKNmzYwOVyY2NjrayscMcxRubcwIKCgmfPno0ePbqgoMBoD/G1ivGPA7VKSEiIi4ubNWvWjBkzcGcxOmY7DiwrK1u2bFlQUBBCyDzqZxLjQK2GDh168+bN+vr6yMjIe/fu4Y5jXMxtHahQKHbt2rVo0SKRSMTlcnHH0THTGge+raysLC4ujslkxsbGmt+/TtuY2zpw6dKlzs7OVCrVLP+BjeR4YJu5uLh8++23w4cPHz9+vOYkJGAm68ATJ06IxWKTOK+lPUx0HKjVvn37Lly4EBsb27dvX9xZcDKHdeDTp09fvnxpCaN8Ex0HavXpp58ePnz4119/XbJkCY/Hwx0HGxNeByYnJ3/zzTfHjx83nito9c3Ux4Fa3b59Oy4ubty4cXPnzsWdBQOTXAdqTmr566+/1q9fb1RX0OqbqY8DtRowYMDVq1cJBML7779/69Yt3HEMzcTWgQ0NDbGxsZGRkUOGDMGdBQNzGge+jcfjbdiwQaFQrFy50snJCXccAzGZdaBmqJCamjphwgTLrJ+ZjQPfZmdnt3379okTJ86ePXvfvn244xiIaawDDxw4cOvWrePHj+MOgplZjgO1+vHHH0+cOLFy5cqIiAjcWfTL2BuYkZEREBBw+fLl4cOH484CDKqurm7Dhg2acccbk9OYE+PdCs3NzQ0PD9ecSw3108A+T4whcbncLVu2zJgxY/78+bt378YdR1+Mt4FisTgxMdHX1xd3ECPi4ODw8OFD3CkMKjw8/Pz581Kp1FxHhkbawCdPntDpdLiY6A0+Pj6mMqGGbtnb2ysUCtwp9MJIG3j9+vXHjx/jTmGMNFMhrVu3DncQgyoqKvLw8MCdQi+MtIHdunXz8/PDncJ4TZw4cc+ePbhTGE5xcXHHjh1xp9ALI93Ms9gjfi3k7+/PZrNxpzAcM26gka4Dnzx5kpOTgzuFUXNxcUEITZ8+HXcQvZNIJAKBAPssqXpipA2EcWALbdmy5dChQ7hT6JcZrwCNdyu0W7duNjY2uFOYACcnp+joaJlMRqVScWfRF2ggBjAObDnNxLg9e/a8f/8+gUDAHUf3zLuBRroVCuPA1rp79+7Zs2dVKhXuILpXVFTk7u6OO4W+GGkDYRzYWiQSaezYsSUlJbW1tbiz6FhJSQmsAw0Njge2jbu7+4QJE8zsRmJFRUVm3EBjvzYCtMHjx4+DgoLodDruIDoglUoHDRqUlJSEO4i+GOk6EMaB7dG9e/f09PTs7GzcQXTAvHfDGG8DYRzYTqGhoatXrzaDzdGSkhI3NzfcKfTISI9GwPHA9ouPj3/58iWFQrGzs8Odpe3Me0eo8TYQjgfqhLOz861bt6hUau/evXFnaaPi4uLAwEDcKfTISLdCYRyoKwMHDjx+/PjrxwlHjBgRFxeHNVQrwDgQDxgH6tDevXtVKlVWVhZCaPz48ZWVlY8fPzaVC16hgXjA8UDdIpPJxcXFw4YNKywsRAjV1NQkJibiDvVucrm8urravOcONdIGDhkypFu3brhTmJWIiIjG2zMIBILLly/jTvRuZr8CNN4GwjhQ50JDQ4nEV//cBAIhPT3d+M9fgwZiA+NA3QoNDX3jsony8nLj3xCFBmID40DdWrBgQUhIiIODA4lE0uwXlcvlV65cwZ3rHcz+YCCcF2rapGKVTNKKy5HKysrS0tKSkpJKSkqEQiFCaMeOHQ4ODvrM2C6xsbETJkzo2rUr7iBtYdWhRQfbjbSBT548sba2hul6m/IooSb1Lp9CI8pb08BGarVaoVQqFQojP3tbrlCQyWRTvOjY1oVWmivy7cruN9qOxiA180ojPSfm+vXrHh4e0ECtrvxUzrahDJvhyuZayo0TTZFMqqoplx7+umBarAfLusmiwTjQxFw5Ut7BidZlgC3Uz8hRaUQnD8aUlT4/rS1QKprc0jTSrVCgVUGaMPeFqOcH5jlvn7kqyRZWFogGRmn/VzPSdSAcD9SqslhKoRnpPxloCteemp8qbOpZI/3nhOOBWklFSjtnGu4UoHXYXArHjtrUXmsj3RMD1wdqJeQrObAFaoIqi8RNTSRppA2E6wOBhTDSrVAYBwILYaQNhHEgsBBGuhUK40BgIYy0gTAOBBbCSLdCYRwILISRNhDGgcBCGOlWKIwDgYUw0gbCOBBYCCPdCoVxILAQRtpAGAcCC2GkDYTrAwFC6PTv8UOGheNOoV9G2kCYL9QM5OfnRk8eiTuFsTPSBsI40AxkZqXhjmACjGtf6ODBg+vr6xsv2ycQCGq12snJ6dKlS7ijgdY5fGT/z0cPIoQGRYR9On/JhKgpIpFo+84NycmPBAK+p4f38OGjx4yeoHlxM081KioqOHxkf/Kzx2q1Oiioc/SH00NCTHIOtTcY1zqwT58+arWa+P8IBAKJRIqMjMSdC7TarJnzoidOd3R0uvHnowlRUxBCK1Z+VlZWsm7ttpPxlwYMiNi1e3N6Rqrmxc08pSGTyWKWfkIikTZv2rPtm+/IJHLsqiUSiQTTh9Ml42rgpEmTXFxcXn/Ezc1t0qRJ+BIB3bh3P/HFi+TPl60ODAjicLhTJs8KCen6088Hmn+qUXFxYW1tzfhxk/z9Anx8/L7676Y1a74xlds/Nc+4GhgUFBQcHNz4vwQC4YMPPuByuVhDAR3Iz8+h0+leXj6Nj/j7BWZmpjX/VCM3N3cut8OmLV8fO34oJeUZkUgM7RrGZrMN+yH0wrgaiBCaPn16412X3dzcPvzwQ9yJgA5UV/PodMbrjzCZTLFY1PxTjWg02q4dP/QK73fq9IlFi2dPmTYmIcFMdg0YXQM7derUuXNnzdfDhw/v0KED7kRAB1gslkQifv0RoUhoZ2vf/FOvc3f3nD8vJv7Ehbh12729fDds+m9WdoZBsuuX0TUQITRz5kxbW1snJydYAZqN9/w7SSSS7JzMxkfS01M8vXyaf6pRUVHB5SvnEUJ0Or1PnwFff7WZTCZnZaUb9kPoRXuPRpTliup5CqFAIeIrVUqkULTlNgZvse333nwWi/XoshShivYvjsYgEhCBaU1iWpNsXWj2LjDhnyG4ublXV/Pu3Lnp4eHVs2cfFxe37dvjFi9e4WDveObsr+npKbt3HkQINfNUIz6/fss3awsK8iIjx6tVqhs3ExQKRXBQF3wfTmfaOGd2Ybow60lDXoqwgxNDrSaQKCQihUQkkYxzBm4CgaBSKpVypVKmkEsUconSpzMrIMzK0cOo71vytis/lbv4sL1CTGMPRHU1L27DqqfJj2ZM/2TmjE/y83P3f7/z4aN7VCrV29tv8qSZ/fr+S/PKpp46/Xv8d/t3XL92HyH0x4Xfj/z0fU1NNUIorHv45MmzQruG4f6ILXViQ+5Ha70pNC0TFra6gS/zxbfPVFOYVAKZauXAJFOauy+McZKJFQ08oUIsZTBR/zG2XHsq7kQtZVoNBI2aaWDrtkKv/1JVliex9bJhdTCxtcfrqAyyTUcOQohfKTy9pyywp1Wfkba4QwEL1dI9MQq56sjaQomS5t7NxaTr9zprB5ZP746V5cQz35bizgIsVIsaqFSoD3yZ59zJkW3L0n8kQ+O6WlM41vFbi3EHAZbo3Q1UqdTffZHbKcKLxjLbG9axbZnWrjY/rS/EHQRYnHc38PjGIr8+rgYJgxOTS7fpyL3440vcQYBleUcDb57mcTtyaSyT2VvYHlYObDmiJd+qwx0EWJDmGlhdJs1PEVrZW9C+b64L585ZnnEe1QRmqbkG3j5bbedlcZN2Ovl3+PtsNe4UwFI02cDyArFCSbSyZxo2T0slv7i+fHV4g7BW50u28+SW5kmlYqXOlwzA25psYM4zIYFktjs/34FALEgVteB1ALRXkw3MfS60cjDSFaC+MW1Y2ckNuFMAi6D9rLTaShnDiqK/XaAFRc+v3ThYXJLGZnUIfK/fsEFz6HQWQijx3m8Jtw7N/+i7n+O/rKjMc3b0HdBnUo9ur2a8u3Blz6Nnl2hUZmjn9x3s3PWUDSFk7cB8mcrX3/INQ6VSXf/rvLU1zDCgF/Z2Dj7endq/HO0NbKhTSMQ6uc5IC1518fdHFrm5BCz85KBarTp3aft3h+Z/NvcQiUQmkSliseDsxa0fjlnp7hZ8/dahk2fX+3qHdeA6JT04nfTgVPS4r3y9w1Izbifc+FFP8TTXUjTUyoV8BcvauOaSaxW1Wk0gqAMD38MdxAwRCAQKRTe/G9qXIuIrSXq76OHJsytkEmXmpM0sFhchNGF07IbtY1LSb3UJjkAIKZXyoYPmeHQMQQiFdf331T8PlL7M6sB1unP3ZOegiM7BgxFCPbqNLCpJraou0lNChBCVThLWm3YDiUTigP5DqFS4GFIf1Gq1blZRTTRQoCBR9fXLV1D0vKNbJ039EEI2HZxtbdzyC5M1DUQIubsGab5gMqwRQmKJQK1W82qKGzdHEUJuLgF6iqdBYZBEfNOeiotAINCoVrhTmC2CliuN2qLJmhGQvo5KiyUNxaVpy1f/z/0A+IJ/DsER3vpwEqlQpVLSaP/sGaJSGUifVErd/YwBaJr2BjKtyUq5vqZDtbKy9fLo+v7gT15/kMXiNPMtdBqLSCTJX4sklen3aIFSpjTpTVBgKppooBVJKdfXIWkXR7/Hzy55e4YSia+OhZRX5tnbNrdvk0AgdOA6FxS9GNj31SPpmYl6iqchkyiZ1qZ3+T8wOdqPB1rbkClUfW2DDegzSaVSnb+8QyaTVFYVXri6d9veyS8r3nGfli7BQ16k3Uh+cR0h9NffPxeWpOgpnuaCLDaXDOtAYADaG8ixoyokSolApo+3ZDKtly88QaVbvWvYAAAKWElEQVQwdu6fsWX3h3kFTyaMiX3nnpUhA2eFdx999tK25avD0zMTRw2P0eyQ0kdCfoWwg4Olng8EDKvJmZruXqwuKVDbe1vihLllqZU9Ith+oUa3IxFmajJRzczU1ORZab5dWGqzuDNGGxAISq8gM5yPAxihJoc69m50BlNdXyHkOGr/Xayrr9y6V/tdjRg0tliq/bxKJ3vvhZ/80Na0WqyKi2jqKaVSQSJp+YDubkGfzNjd1HdV5dV6BjDIVGOcTRyYn+Z2NgwYZ/fbztKmGmjFtln66VGtT8lkEipV+3xqRKKOd280lQEhJJNLqRQtZ4SQyU2e76pSqqry6ycs8GnqBQDoVnN94NhSAnqwa8oF1k5aRkQkEtmmg4u27zMo3WaoK+EPjHrzniEA6M87trX6jbIT1TQI68zhZqXvVFvCt7JSBvWyxh0EWJB3j3ail7kVJ5crJGa+V6a+vEHKFw6d4oA7CLAsLdrfMG+TT3Ziich814T15Q0qiSh6mRvuIMDitGyPHwHN3+rDL63hVwj0nsjgaotrqQTx2PnOuIMAS9SKfe7Ryzva2irz7pXwK4X6jGQ4taX8jJuFXu+Rh890wp3FKDx8dG/MuCHNvODq1QuCBr3/FVar1ad/j2/DNyYnP24+/+vKy1/OmBU1KCLs4aN7bXgvXWndUa++kbZjFzirJUJeblVVQZ1UKNdbMD0S86WVubXlaeVWLPmsrz26DoR5HF7pEdbr7O/Xm3q2trZm776tLKbez1W4/fdfDx4mteEbM7PSAgODW/jiM2d/9fbyvfHnox5hvdrwXrrS6qNzHRyoo+c6lxdIspMbcp9X0JhklYpAopJIFBKRTEJ6u6qwPQgEgkKuVMkUCplSJpbTGES/rmz/bvYmdOdAw1i0ePbQISNGRY5fsGhWeM++SUm3FEqFvb3jooWfK+TyL1YsJJHIS5fPi1u3o6gof/+BXfX1dSQSqVd4vxnTP6FSqfcfJO37bntAQFB+Xs6Wzd+Oixo6fdqcu3f/njNnYWLiTblc/vny1QihspelU6aOvnzxjkql+nfkgE8+XpSW9iI9I6VHWO/585c8eJC0a/cmDqfDxs1fffmfNa3Kn5mZ5mDvOPvj6MLC/B49es+aOc/fLwAhtOfbrQ8f3mXQGSwW+6NZ84ODu+z5duuFC7+7unbcuWtTzOIVp3+PT0i4qFaraXT6rJnzNPcGXbBoVnBQl+TkR4MGDYueOP3thejkZ97G4+NOnnQnT3r/MXY15bJ6nlzIVwjrFUqFSqkwxgZS6QQiiciyZjKtSXauVDYHzrrWLicn89P5S9VqdX5+jq2N3dZvvmOz2V/Gxly9+sesmfO6dOnO5XSYPy9GKpWuWbdi8qRZI4aPFgj4sauXMhjMqVM+KikurK2pnjhhmre3b05OFolEsrd3/H7/MYTQTz8fGBIxXPMu2dkZHTt60On09PQUhJCXp8+k6Bn19XWzZn8YEtJ1xPDRB3/c++m8JX36DHg927ioYbW1Na8/MnpUVMziFa8/kpWV7tbRY/vW/QihjZu/+u23Y7Er1587fyo9PWVD3E43145Xr15YsfKz079dWzB/6fnzp75csdbP970Tvxy5k3hz/brtdnb2t27/ueLLz07/do3NZhcV5nu4e2nyv72Q308lUKk6+Ave3jNUbJyoNk6wJjEHhYX5UqnUz/e90tJiqVS6fPlqNpuNEFLI5TQaXdPP6A+nI4R+PXnUwcFpVOR4hFCHDjbdu/XMy8tGCOXkZoX36uft7YsQys3NsrO1f3/Yq4lFcnOzPp23pPFrP9/3EELZOZlh3cN79eqHEOJwuG5u7nV1tXwBv6Ki3M/vzWtlfj91rfn89fV1ZS9Lt23dz+FwEUKdAkNevHgqEol+OLjn66+2uLl2RAgNGTJ805avKypeymQyhJCPt59IJDry0/ebN+2xs7NHCA0cELF23ZdFxQW2NnYNwoYpUz5CCGldSBWv0tVFBzvP4RI48EpWVrq3ty+ZTM7ITPP28rW2enVmQkZGalTUFIVCkZ+fqynGs2ePX7xIHhTxz23cNW3Myk6fMf3V1AeZ2el9+g4kk8kIoaKiAqlU6u8fqHkqOyezS+dumioGBXVuXEhNNY/D4WZnZ7BZbHv7Vh+YTc9I9fb2dXR8tVOtpoZnbc3JyckUCoWff7Hg9Vey2Vb3HyR6e/kSicSMzFQKhdJ4S/r6+jqVSsXhcDMyU318/DQd074Qlm6uUIEGgldycrP8fAM0W4k+Pv6aB3m8qgZhQ2BgcE5uFo1Gc3f3RAjJ5LLly1b9e8SY179dIpHk5+f6+72qWWZmWuTIca++zkp3d/fUtFGhUKSmPv9wwlRNFYcM/kDzmsrKitKyktDQHn///ZfWvSnv3ArNzEyzt3ds/N/09JSRI8dJZVJHR6f4Exfe/rC+vu8hhGRS6evTyb14kWxra+fi7Hrp0llfn1cTPTa1EJ2AKwDAK9nZGZpVXE5Opv//bwRmZ2c4ODhaW1kXFxc6ODhpJhbx9vJ9/Pi+QqFQKpU3biYc+el7zStZTJaTk7NmsuCsrHRNnxFCUqmkcfati5fOCgR8X9/3lEplfn7O8xdPNY//fPSHXr36uTi7FhcXOjlpOdf391PXbvz56PX/3hwEZqcX5OdqDpY8fvKgorJ8wIAIL0+f6mpeVnaG5vDDrt2bi4sLX/+wvr7v1dXVZmSmIYRqaqq/+37n2DETCQRCVlZ64w+hqYXoBKwDwSvZ2RkfzZr/xsZkdk6m5jfVy9OnrKxk/IT3T528MmfOwoMH906YOJxEIjk6Oq/8cp1mI7ZxOzMvLwch5OX16hKT/v0H37+fuGjx7Nqa6nFjox0cHK3YVvn5uSQSqVu3nh9Gj1AoFD179vnP518hhPz9A3fs3CgUNqxetaHl4VUq1YvnT+fNi5k9ZyKFQrWzs9+4YRfHmoMQWrdma9yGVQQCobKyfOaMuR07emg+7NyPP0MI2dnZb9q4e9PmryhkCoPJnDlj7pCIDxBCGZmp06bO0Szczs5e60J0oslr5IERMqdr5BMSLp3749Te3YdwBzGEZq6Rh3Wg2aqvrzt3/tQbDyqVShLpzTngWCz2+HHRBoyGNCMxby9fA7+pEYIGmi0Ohzt92hzcKZqUm5vVt++/cKfADxoI8Nj6zT7cEYwC7AsFACdoIAA4QQMBwAkaCABO0EAAcIIGAoATNBAAnKCBAOAEDQQAJ2ggADhBAwHACRoIAE7QQABwggaaEhaHTIKZFk2Qgzujqal0oYGmhMEi8kqluFOA1hHUyAU1MgpNe9eggabE0YMulypxpwCtU1sp9Qppcqp/aKAp6ejPJBLQ0xvVuIOAllLIVTd+Le8/pskbM8NMTabn9pkquUzt09na1oWOOwtoUkOdvLZceuNk+cdx3lR6k6s6aKBJSrlbn5rEl4iUUrEKdxaghaM7vbZC5tOF1czaTwMaaMLUaiSTQAONklpNY745J51W0EAAcII9MQDgBA0EACdoIAA4QQMBwAkaCABO0EAAcPo/5kA//KhQS3sAAAAASUVORK5CYII=",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x1182627e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f8bfdd8-a15e-4103-a0fd-6c909c850063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- interaction #1 ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_Mh2p2ixAanSRk9doJn5U9K7C)\n",
      " Call ID: call_Mh2p2ixAanSRk9doJn5U9K7C\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "('tools',)\n",
      "[{'name': 'tavily_search_results_json', 'args': {'query': 'LangGraph programming language'}, 'id': 'call_Mh2p2ixAanSRk9doJn5U9K7C', 'type': 'tool_call'}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_Mh2p2ixAanSRk9doJn5U9K7C)\n",
      " Call ID: call_Mh2p2ixAanSRk9doJn5U9K7C\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"LangGraph: A Framework for Building Stateful Multi-Agent ...\", \"url\": \"https://medium.com/@ken_lin/langgraph-a-framework-for-building-stateful-multi-agent-llm-applications-a51d5eb68d03\", \"content\": \"> LangGraph is a powerful Python library designed for constructing stateful, multi-actor applications with Large Language Models (LLMs). It extends the capabilities of LangChain Expression Language (LCEL) while specifically addressing limitations in existing frameworks for agent development. As a specialized tool for creating complex LLM applications, LangGraph provides a structured approach to building sophisticated workflows that require cyclical processing patterns. [...] LangGraph is built on top of and intended to be used with LangChain, extending the LangChain Expression Language with capabilities to coordinate multiple chains or actors across computational steps in a cyclic manner. The library is inspired by graph processing frameworks like Pregel and Apache Beam, offering an interface similar to NetworkX for building complex agent systems. Unlike purely Directed Acyclic Graph (DAG) frameworks, LangGraphâ€™s main distinctive feature is its support for cycles, [...] LangGraph represents a significant advancement in the toolkit available for developing sophisticated LLM applications, particularly those involving agent-like behaviors and multi-agent coordination. By extending LangChain with support for cyclical workflows and robust state management, it addresses key limitations of earlier approaches and enables new possibilities for AI systems that can reason, act, and interact in more complex ways.\", \"score\": 0.8974172}, {\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"url\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. [...] Skip to main content\\n\\n# LangGraph Tutorial: What Is LangGraph and How to Use It?\\n\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\n\\nJun 26, 2024  Â· 12 min read\", \"score\": 0.834852}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a powerful Python library that is part of the LangChain ecosystem. It is designed for building stateful and multi-actor applications using Large Language Models (LLMs). Here's a brief overview based on recent information:\n",
      "\n",
      "1. **Purpose and Design**: LangGraph extends the capabilities of the LangChain Expression Language (LCEL) by addressing limitations in existing frameworks for agent development. It helps in constructing complex applications that require coordinated interactions between multiple agents or chains.\n",
      "\n",
      "2. **Features**:\n",
      "    - **Multi-Agent Coordination**: It provides a framework to define, coordinate, and execute multiple LLM agents in a structured way. This is essential for applications where managing the state, coordinating interactions, and handling errors are critical.\n",
      "    - **Cyclical Workflows**: Unlike traditional Directed Acyclic Graph (DAG) frameworks, LangGraph supports cycles, which allows for more flexible processing patterns that are useful in sophisticated agent systems.\n",
      "    - **Robust State Management**: LangGraph enables robust state management and is suitable for applications needing complex reasoning and interaction abilities.\n",
      "\n",
      "3. **Inspirations and Influences**: The library takes inspiration from graph processing frameworks such as Pregel and Apache Beam and offers an interface similar to NetworkX. This aids in building complex agent systems capable of handling stateful interactions and workflows.\n",
      "\n",
      "4. **Potential Applicability**: LangGraph is particularly beneficial for developing advanced LLM applications that involve agent-like behaviors and require sophisticated coordination.\n",
      "\n",
      "For more detailed information, you can explore resources like [this Medium article](https://medium.com/@ken_lin/langgraph-a-framework-for-building-stateful-multi-agent-llm-applications-a51d5eb68d03) or tutorials like [this one on DataCamp](https://www.datacamp.com/tutorial/langgraph-tutorial).\n"
     ]
    }
   ],
   "source": [
    "# from langgraph_sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Use a 'with' statement to properly manage the memory connection\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    # 1. Compile the graph with the checkpointer INSIDE the 'with' block\n",
    "    graph = graph_builder.compile(\n",
    "        checkpointer=memory,\n",
    "        # This is new!\n",
    "        interrupt_before=[\"tools\"],\n",
    "    )\n",
    "\n",
    "    # 2. Define a configuration for a specific conversation thread\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    # 3. Interact with the graph INSIDE the 'with' block\n",
    "    messages = [\n",
    "        \"\"\"I'm learning LangGraph. Could you do some research on it for me?\"\"\"\n",
    "        # \"Hi there! My name is Alex Leontiev!\",\n",
    "        # \"What did I say my name was?\",\n",
    "    ]\n",
    "    for i, user_input in enumerate(messages):\n",
    "        print(f\"--- interaction #{i+1} ---\")\n",
    "        # user_input =\n",
    "        response = graph.invoke({\"messages\": [(\"user\", user_input)]}, config)\n",
    "        # Pretty-print the final response\n",
    "        response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    # print(\"\\n--- Second interaction (testing memory) ---\")\n",
    "    # user_input_2 =\n",
    "    # response_2 = graph.invoke({\"messages\": [(\"user\", user_input_2)]}, config)\n",
    "    # response_2[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    snapshot = graph.get_state(config)\n",
    "    print(snapshot.next)\n",
    "\n",
    "    existing_message = snapshot.values[\"messages\"][-1]\n",
    "    print(existing_message.tool_calls)\n",
    "\n",
    "    ## allow continuation\n",
    "    events = graph.stream(None, config, stream_mode=\"values\")\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3400b-a146-4504-bcf0-5be0a55f2cdf",
   "metadata": {},
   "source": [
    "## 107. Manually Updating the State\n",
    "* https://drive.google.com/file/d/1lH8tJCT9VAnGc2TWT_Zxid9iSBqHMjkK/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46877bde-6f2a-4abb-b44f-463614c38cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# tool = TavilySearchResults(max_results=2)\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "# with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "conn = sqlite3.connect(\"langgraph_memory.sqlite\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8080315-7b24-4189-9188-24eceac3f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # This is new!\n",
    "    interrupt_before=[\"tools\"],\n",
    "    # Note: can also interrupt **after** actions, if desired.\n",
    "    # interrupt_after=[\"tools\"]\n",
    ")\n",
    "\n",
    "user_input = \"I'm learning LangGraph. Could you do some research on it for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream({\"messages\": [(\"user\", user_input)]}, config)\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ad515c-958b-4103-9510-a763cd0432bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_pUjP2dwmDgcj9McmctRDqegK)\n",
      " Call ID: call_pUjP2dwmDgcj9McmctRDqegK\n",
      "  Args:\n",
      "    query: LangGraph programming language\n"
     ]
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "existing_message = snapshot.values[\"messages\"][-1]\n",
    "existing_message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52c969e-fb03-4214-8b6d-696ad39aa72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a library for building stateful, multi-actor applications with LLMs.\n",
      "\n",
      "\n",
      "Last 2 messages;\n",
      "[ToolMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', id='28642d59-a205-4c37-9778-3f19de71dd97', tool_call_id='call_pUjP2dwmDgcj9McmctRDqegK'), AIMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', additional_kwargs={}, response_metadata={}, id='86252d58-7973-4913-a546-20a36e5702a0')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "answer = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs.\"\n",
    ")\n",
    "new_messages = [\n",
    "    # The LLM API expects some ToolMessage to match its tool call. We'll satisfy that here.\n",
    "    ToolMessage(content=answer, tool_call_id=existing_message.tool_calls[0][\"id\"]),\n",
    "    # And then directly \"put words in the LLM's mouth\" by populating its response.\n",
    "    AIMessage(content=answer),\n",
    "]\n",
    "\n",
    "new_messages[-1].pretty_print()\n",
    "graph.update_state(\n",
    "    # Which state to update\n",
    "    config,\n",
    "    # The updated values to provide. The messages in our `State` are \"append-only\", meaning this will be appended\n",
    "    # to the existing state. We will review how to update existing messages in the next section!\n",
    "    {\"messages\": new_messages},\n",
    ")\n",
    "\n",
    "print(\"\\n\\nLast 2 messages;\")\n",
    "print(graph.get_state(config).values[\"messages\"][-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c1934e-bba0-4098-ba5d-513c829d04be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f086d5a-90a3-66b8-8003-5ecfaf472407'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.update_state(\n",
    "    config,\n",
    "    {\"messages\": [AIMessage(content=\"I'm an AI expert!\")]},\n",
    "    # Which node for this function to act as. It will automatically continue\n",
    "    # processing as if this node just ran.\n",
    "    as_node=\"chatbot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0573eac1-a049-4012-918e-1235a3a89620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ToolMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', id='28642d59-a205-4c37-9778-3f19de71dd97', tool_call_id='call_pUjP2dwmDgcj9McmctRDqegK'), AIMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', additional_kwargs={}, response_metadata={}, id='86252d58-7973-4913-a546-20a36e5702a0'), AIMessage(content=\"I'm an AI expert!\", additional_kwargs={}, response_metadata={}, id='026b953e-a670-41c2-abe0-f0cba4bc5532')]\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "print(snapshot.values[\"messages\"][-3:])\n",
    "print(snapshot.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a1d49-21d6-4a5c-a467-c8d6301ec8e6",
   "metadata": {},
   "source": [
    "#### rewriting message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df10ef02-da92-4fab-a340-796d5c237db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm learning LangGraph. Could you do some research on it for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_ZVnFIepyXeNwm6t6BcXtCQCt)\n",
      " Call ID: call_ZVnFIepyXeNwm6t6BcXtCQCt\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "    search_depth: advanced\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I'm learning LangGraph. Could you do some research on it for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}  # we'll use thread_id = 2 here\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d194b994-031e-4dd0-843a-3ee119457a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "Message ID run--ecd354f0-b65b-4367-b1d3-645491d2d11c-0\n",
      "{'name': 'tavily_search', 'args': {'query': 'LangGraph programming language', 'search_depth': 'advanced'}, 'id': 'call_ZVnFIepyXeNwm6t6BcXtCQCt', 'type': 'tool_call'}\n",
      "Updated\n",
      "{'name': 'tavily_search', 'args': {'query': 'LangGraph human-in-the-loop workflow', 'search_depth': 'advanced'}, 'id': 'call_ZVnFIepyXeNwm6t6BcXtCQCt', 'type': 'tool_call'}\n",
      "Message ID run--ecd354f0-b65b-4367-b1d3-645491d2d11c-0\n",
      "\n",
      "\n",
      "Tool calls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search',\n",
       "  'args': {'query': 'LangGraph human-in-the-loop workflow',\n",
       "   'search_depth': 'advanced'},\n",
       "  'id': 'call_ZVnFIepyXeNwm6t6BcXtCQCt',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "snapshot = graph.get_state(config)\n",
    "existing_message = snapshot.values[\"messages\"][-1]\n",
    "print(\"Original\")\n",
    "print(\"Message ID\", existing_message.id)\n",
    "print(existing_message.tool_calls[0])\n",
    "new_tool_call = existing_message.tool_calls[0].copy()\n",
    "new_tool_call[\"args\"][\"query\"] = \"LangGraph human-in-the-loop workflow\"\n",
    "new_message = AIMessage(\n",
    "    content=existing_message.content,\n",
    "    tool_calls=[new_tool_call],\n",
    "    # Important! The ID is how LangGraph knows to REPLACE the message in the state rather than APPEND this messages\n",
    "    id=existing_message.id,\n",
    ")\n",
    "\n",
    "print(\"Updated\")\n",
    "print(new_message.tool_calls[0])\n",
    "print(\"Message ID\", new_message.id)\n",
    "graph.update_state(config, {\"messages\": [new_message]})\n",
    "\n",
    "print(\"\\n\\nTool calls\")\n",
    "graph.get_state(config).values[\"messages\"][-1].tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e625749-515f-40ba-8d05-810a43386f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_ZVnFIepyXeNwm6t6BcXtCQCt)\n",
      " Call ID: call_ZVnFIepyXeNwm6t6BcXtCQCt\n",
      "  Args:\n",
      "    query: LangGraph human-in-the-loop workflow\n",
      "    search_depth: advanced\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"LangGraph human-in-the-loop workflow\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\", \"title\": \"LangGraph's human-in-the-loop - Overview\", \"content\": \"Skip to content\\n\\n Key capabilities\\n Patterns\\n\\nhil\\nhuman-in-the-loop\\noverview\\n\\n# Human-in-the-loopÂ¶\\n\\nTo review, edit, and approve tool calls in an agent or workflow, use LangGraph's human-in-the-loop features to enable human intervention at any point in a workflow. This is especially useful in large language model (LLM)-driven applications where model output may require validation, correction, or additional context.\\n\\nTip [...] + Dynamic interrupts: Use `interrupt` to pause a graph from inside a specific node, based on the current state of the graph.\\n  + Static interrupts: Use `interrupt_before` and `interrupt_after` to pause the graph at pre-defined points, either before or after a node executes.\\n Flexible integration points: Human-in-the-loop logic can be introduced at any point in the workflow. This allows targeted human involvement, such as approving API calls, correcting outputs, or guiding conversations. [...] Edit graph state: Pause the graph to review and edit the graph state. This is useful for correcting mistakes or updating the state with additional information. This pattern often involves updating the state with the human's input.\\n Review tool calls: Pause the graph to review and edit tool calls requested by the LLM before tool execution.\\n Validate human input: Pause the graph to validate human input before proceeding with the next step.\", \"score\": 0.90830135, \"raw_content\": null}, {\"url\": \"https://sangeethasaravanan.medium.com/human-in-the-loop-with-langgraph-a-beginners-guide-8a32b7f45d6e\", \"title\": \"Human-in-the-Loop with LangGraph: A Beginner's Guide\", \"content\": \"## ðŸš€ What Is LangGraph?\\n\\nLangGraph is a framework built on top of LangChain, designed for building stateful, multi-actor workflows involving LLMs, tools, and yes â€” humans. It provides a graph-based programming paradigm that simplifies the orchestration of complex, conditional flows.\\n\\nUnlike traditional linear pipelines, LangGraph allows you to:\\n\\n Define conditional branching\\n Include looping and retries\\n Use custom edge logic (including human judgment)\\n Dynamically update your graph state [...] In the rapidly evolving world of AI, LLMs (Large Language Models) are proving invaluable. Yet, no matter how powerful these systems become, there are moments when human input is essential. Whether itâ€™s correcting hallucinations, enforcing ethical rules, or validating critical decisions â€” a Human in the Loop (HITL) is vital for safety, quality, and trust. This is where LangGraph comes in â€” offering a new way to build LLM workflows with the flexibility to integrate human decisions at key points. [...] This makes it perfect for scenarios where you donâ€™t want your LLM to make the final decision â€” you want a human to step in.\\n\\n## ðŸ”„ What is Human-in-the-Loop (HITL)?\\n\\nHuman-in-the-loop is a design pattern where an AI system includes human judgment as part of the process â€” either continuously or at specific checkpoints. Common use cases:\\n\\n Data labeling or review\\n Escalations when AI confidence is low\\n Ethical reviews or moderation\\n Quality assurance in automation\", \"score\": 0.8943568, \"raw_content\": null}], \"response_time\": 1.79, \"request_id\": \"db5cf1be-892d-40ba-aaa8-86b39dbbb7ae\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework designed for building stateful, multi-actor workflows involving Large Language Models (LLMs), tools, and humans. It's built on top of LangChain and provides a graph-based programming paradigm to simplify orchestrating complex, conditional flows. Unlike traditional linear pipelines, LangGraph allows for conditional branching, looping, retries, and the use of custom edge logic, including human judgment. \n",
      "\n",
      "### Human-in-the-Loop (HITL) in LangGraph\n",
      "LangGraph is especially recognized for its human-in-the-loop features. These features enable human intervention at any point in a workflow, which is essential for scenarios where human oversight is necessary. This might include correcting algorithmic errors, enforcing ethical considerations, validating critical decisions, ensuring quality, and more. HITL is vital for maintaining safety, quality, and trust in AI-driven systems. \n",
      "\n",
      "### Key Features of HITL in LangGraph:\n",
      "- **Dynamic Interrupts**: Pause a graph from inside a specific node.\n",
      "- **Static Interrupts**: Pause the graph at pre-defined points before or after a node executes.\n",
      "- **Flexible Integration Points**: Allow human involvement at targeted workflow points, such as approving API calls or correcting outputs.\n",
      "- **Edit Graph State**: Pause to review and edit the graph, correcting mistakes or updating with additional information.\n",
      "- **Review Tool Calls**: Review and edit tool calls before execution.\n",
      "- **Validate Human Input**: Validate human input before proceeding to the next step.\n",
      "\n",
      "These features make LangGraph particularly suited for situations where human input is critical to the workflow, ensuring better safety, quality, and accountability in AI applications.\n",
      "\n",
      "For more information, you can visit the [official documentation](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/) or read a [beginner's guide on Medium](https://sangeethasaravanan.medium.com/human-in-the-loop-with-langgraph-a-beginners-guide-8a32b7f45d6e).\n"
     ]
    }
   ],
   "source": [
    "events = graph.stream(None, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6734a-f70c-47b1-bf46-350ca4f81c66",
   "metadata": {},
   "source": [
    "## 108. Customizing the State in LangGraph\n",
    "* https://colab.research.google.com/drive/1a-05_nneTiXuhNwcvrSWqx1mUniJC77q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18d39c1-36ea-4894-98d0-8e4414f48ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    # This flag is new\n",
    "    ask_human: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387740e3-eac3-4f92-baa9-762d506003f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "\n",
    "class RequestAssistance(BaseModel):\n",
    "    \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
    "\n",
    "    To use this function, relay the user's 'request' so the expert can provide the right guidance.\n",
    "    \"\"\"\n",
    "\n",
    "    request: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e638a6f-b043-4027-9dc5-88253e0da77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/m1j4q6jj79zbjw0qtxcldpnc0000gn/T/ipykernel_2936/1722260856.py:1: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=2)\n"
     ]
    }
   ],
   "source": [
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# We can bind the llm to a tool definition, a pydantic model, or a json schema\n",
    "llm_with_tools = llm.bind_tools(tools + [RequestAssistance])\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    ask_human = False\n",
    "    if (\n",
    "        response.tool_calls\n",
    "        and response.tool_calls[0][\"name\"] == RequestAssistance.__name__\n",
    "    ):\n",
    "        ask_human = True\n",
    "    return {\"messages\": [response], \"ask_human\": ask_human}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97887464-cc63-4add-a051-bd93c36dec7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10cb70620>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=[tool]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6726537e-c608-45de-81c0-a71ccad3cf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10cb70620>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "\n",
    "def create_response(response: str, ai_message: AIMessage):\n",
    "    return ToolMessage(\n",
    "        content=response,\n",
    "        tool_call_id=ai_message.tool_calls[0][\"id\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    new_messages = []\n",
    "    if not isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        # Typically, the user will have updated the state during the interrupt.\n",
    "        # If they choose not to, we will include a placeholder ToolMessage to\n",
    "        # let the LLM continue.\n",
    "        new_messages.append(\n",
    "            create_response(\"No response from human.\", state[\"messages\"][-1])\n",
    "        )\n",
    "    return {\n",
    "        # Append the new messages\n",
    "        \"messages\": new_messages,\n",
    "        # Unset the flag\n",
    "        \"ask_human\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"human\", human_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ea75d0-137d-4baa-8c52-d273da997cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10cb70620>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_next_node(state: State):\n",
    "    if state[\"ask_human\"]:\n",
    "        return \"human\"\n",
    "    # Otherwise, we can route as before\n",
    "    return tools_condition(state)\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    select_next_node,\n",
    "    {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77784c4-46e6-4a9e-b787-8d2b63a86f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "# with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "conn = sqlite3.connect(\"langgraph_memory.sqlite\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "414217be-9ca3-487d-80f7-c703d1ecfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest is the same\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(\"human\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # We interrupt before 'human' here instead.\n",
    "    interrupt_before=[\"human\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9be8ec07-965e-412d-9588-2aa873d0b699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f9d35e1-a509-41ce-b5b4-19234017954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I need some expert guidance for building this AI agent. Could you request assistance for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  RequestAssistance (call_bt4EujiO1iYitm1UFGIYSj0O)\n",
      " Call ID: call_bt4EujiO1iYitm1UFGIYSj0O\n",
      "  Args:\n",
      "    request: I need expert guidance for building an AI agent using LangGraph.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I need some expert guidance for building this AI agent. Could you request assistance for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a48abf19-ff45-462f-88e9-02a5304711e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human',)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cd6391d-4dab-4e07-aa5d-b971e04196cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0874a6-7af5-6232-8007-daaa666dfa9a'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message = snapshot.values[\"messages\"][-1]\n",
    "human_response = (\n",
    "    \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
    "    \" It's much more reliable and extensible than simple autonomous agents.\"\n",
    ")\n",
    "tool_message = create_response(human_response, ai_message)\n",
    "graph.update_state(config, {\"messages\": [tool_message]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa69669b-f01f-4b4f-995f-7e2d96509f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"I'm learning LangGraph. Could you do some research on it for me?\", additional_kwargs={}, response_metadata={}, id='db50906c-9601-4e47-9789-b541f513ef50'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_pUjP2dwmDgcj9McmctRDqegK', 'function': {'arguments': '{\"query\":\"LangGraph programming language\"}', 'name': 'tavily_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 1283, 'total_tokens': 1302, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1280}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CAnuM6kLz4K8WGf4clU1fCsUZw8Mt', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--2a96f743-79f9-4284-8035-9288aa1ea30a-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'LangGraph programming language'}, 'id': 'call_pUjP2dwmDgcj9McmctRDqegK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1283, 'output_tokens': 19, 'total_tokens': 1302, 'input_token_details': {'audio': 0, 'cache_read': 1280}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', id='28642d59-a205-4c37-9778-3f19de71dd97', tool_call_id='call_pUjP2dwmDgcj9McmctRDqegK'),\n",
       " AIMessage(content='LangGraph is a library for building stateful, multi-actor applications with LLMs.', additional_kwargs={}, response_metadata={}, id='86252d58-7973-4913-a546-20a36e5702a0'),\n",
       " AIMessage(content=\"I'm an AI expert!\", additional_kwargs={}, response_metadata={}, id='026b953e-a670-41c2-abe0-f0cba4bc5532'),\n",
       " HumanMessage(content='I need some expert guidance for building this AI agent. Could you request assistance for me?', additional_kwargs={}, response_metadata={}, id='9e6b8897-f1b6-41ca-b9f0-c1f6f526080e'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_bt4EujiO1iYitm1UFGIYSj0O', 'function': {'arguments': '{\"request\":\"I need expert guidance for building an AI agent using LangGraph.\"}', 'name': 'RequestAssistance'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 259, 'total_tokens': 286, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CB0yS5Lwk1o0Yg9t0kc1rflt74YmL', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--22a12189-3fc0-4961-9a4b-cf814ac83b71-0', tool_calls=[{'name': 'RequestAssistance', 'args': {'request': 'I need expert guidance for building an AI agent using LangGraph.'}, 'id': 'call_bt4EujiO1iYitm1UFGIYSj0O', 'type': 'tool_call'}], usage_metadata={'input_tokens': 259, 'output_tokens': 27, 'total_tokens': 286, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content=\"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.\", id='98e9379d-0e67-447f-b9cb-bf9ea3812abb', tool_call_id='call_bt4EujiO1iYitm1UFGIYSj0O')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c50bf065-a022-41c6-ac1e-028363791ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've requested expert assistance for you to get guidance on building an AI agent with LangGraph. The experts have recommended checking out LangGraph for constructing your agent, as it is more reliable and extensible than simple autonomous agents. If you need further help or have more questions, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "events = graph.stream(None, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "249e9b31-2516-4e5a-a657-f9507a977bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Sure, please use your search tools to find something about LangGraph for me.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_iLjkhgyfUaLw5QtL0ZNO2bzw)\n",
      " Call ID: call_iLjkhgyfUaLw5QtL0ZNO2bzw\n",
      "  Args:\n",
      "    query: LangGraph library\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"langgraph Â· PyPI\", \"url\": \"https://pypi.org/project/langgraph/0.0.25/\", \"content\": \"LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain.\\nIt extends the LangChain Expression Language with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner.\\nIt is inspired by Pregel and Apache Beam.\\nThe current interface exposed is one inspired by NetworkX.\", \"score\": 0.9344013}, {\"title\": \"LangGraph Tutorial with Practical Example\", \"url\": \"https://www.gettingstarted.ai/langgraph-tutorial-with-example/\", \"content\": \"â€” 8 min read\\n\\n## What is LangGraph?\\n\\nLangGraph is a library built by the LangChain team that aims to help developers create graph-based single or multi-agent AI applications. As a low-level framework, LangGraph lets you control how agents interact with each other, which tools to use, and how information flows within the application.\\n\\n## No time to read? Watch this tutorial instead!\\n\\n## How does LangGraph work?\", \"score\": 0.9252368}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are a couple of resources about LangGraph that you might find helpful:\n",
      "\n",
      "1. **[LangGraph on PyPI](https://pypi.org/project/langgraph/0.0.25/)**: This page provides an overview of LangGraph, describing it as a library for building stateful, multi-actor applications with LLMs. It is built on top of LangChain and is intended to be used with it. LangGraph extends the LangChain Expression Language to coordinate multiple chains (or actors) across multiple computation steps in a cyclic manner, drawing inspiration from frameworks like Pregel and Apache Beam. The interface resembles that of NetworkX.\n",
      "\n",
      "2. **[LangGraph Tutorial with Practical Example](https://www.gettingstarted.ai/langgraph-tutorial-with-example/)**: This tutorial is an 8-minute read aimed at helping developers understand LangGraph. It explains how LangGraph is a library built by the LangChain team to facilitate the creation of graph-based single or multi-agent AI applications. The framework offers low-level control over agent interactions, tool usage, and information flow within applications. There's also a video tutorial available if you'd prefer watching over reading.\n",
      "\n",
      "These resources should provide a good starting point to understand and begin using LangGraph for your projects.\n"
     ]
    }
   ],
   "source": [
    "user_input = (\n",
    "    \"Sure, please use your search tools to find something about LangGraph for me.\"\n",
    ")\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8feee9-5d0c-4cfd-928a-f36ccee3883d",
   "metadata": {},
   "source": [
    "## `109.` time travel\n",
    "* https://colab.research.google.com/drive/16pumlTsseLni9DBpv5she7HDuXT9TNk6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e018a768-db38-46ef-b30f-bd4b0f4081cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/var/folders/02/m1j4q6jj79zbjw0qtxcldpnc0000gn/T/ipykernel_3678/4275922845.py:30: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=2)\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import AIMessage, BaseMessage, ToolMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    # This flag is new\n",
    "    ask_human: bool\n",
    "\n",
    "\n",
    "class RequestAssistance(BaseModel):\n",
    "    \"\"\"Escalate the conversation to an expert. Use this if you are unable to assist directly or if the user requires support beyond your permissions.\n",
    "\n",
    "    To use this function, relay the user's 'request' so the expert can provide the right guidance.\n",
    "    \"\"\"\n",
    "\n",
    "    request: str\n",
    "\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "llm = ChatOpenAI()\n",
    "# We can bind the llm to a tool definition, a pydantic model, or a json schema\n",
    "llm_with_tools = llm.bind_tools(tools + [RequestAssistance])\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    ask_human = False\n",
    "    if (\n",
    "        response.tool_calls\n",
    "        and response.tool_calls[0][\"name\"] == RequestAssistance.__name__\n",
    "    ):\n",
    "        ask_human = True\n",
    "    return {\"messages\": [response], \"ask_human\": ask_human}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=[tool]))\n",
    "\n",
    "\n",
    "def create_response(response: str, ai_message: AIMessage):\n",
    "    return ToolMessage(\n",
    "        content=response,\n",
    "        tool_call_id=ai_message.tool_calls[0][\"id\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    new_messages = []\n",
    "    if not isinstance(state[\"messages\"][-1], ToolMessage):\n",
    "        # Typically, the user will have updated the state during the interrupt.\n",
    "        # If they choose not to, we will include a placeholder ToolMessage to\n",
    "        # let the LLM continue.\n",
    "        new_messages.append(\n",
    "            create_response(\"No response from human.\", state[\"messages\"][-1])\n",
    "        )\n",
    "    return {\n",
    "        # Append the new messages\n",
    "        \"messages\": new_messages,\n",
    "        # Unset the flag\n",
    "        \"ask_human\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"human\", human_node)\n",
    "\n",
    "\n",
    "def select_next_node(state: State) -> Literal[\"human\", \"tools\", \"__end__\"]:\n",
    "    if state[\"ask_human\"]:\n",
    "        return \"human\"\n",
    "    # Otherwise, we can route as before\n",
    "    return tools_condition(state)\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    select_next_node,\n",
    "    {\"human\": \"human\", \"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(\"human\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "import sqlite3\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "# with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "conn = sqlite3.connect(\"langgraph_memory.sqlite\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn=conn)\n",
    "\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=memory,\n",
    "    interrupt_before=[\"human\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07571ffc-c63c-4c03-a7a6-ae9e3bf3e0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ebdbae-ff7b-4b05-a217-b92eb9dcdd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm learning LangGraph. Could you do some research on it for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_5C0pPUFu7LnwkQy3DMVJsrAp)\n",
      " Call ID: call_5C0pPUFu7LnwkQy3DMVJsrAp\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "  tavily_search_results_json (call_sIEHU0HCSIhVSSXSNf2eioun)\n",
      " Call ID: call_sIEHU0HCSIhVSSXSNf2eioun\n",
      "  Args:\n",
      "    query: LangGraph library\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"langgraph Â· PyPI\", \"url\": \"https://pypi.org/project/langgraph/0.0.25/\", \"content\": \"LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain.\\nIt extends the LangChain Expression Language with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner.\\nIt is inspired by Pregel and Apache Beam.\\nThe current interface exposed is one inspired by NetworkX.\", \"score\": 0.9344013}, {\"title\": \"LangGraph Tutorial with Practical Example\", \"url\": \"https://www.gettingstarted.ai/langgraph-tutorial-with-example/\", \"content\": \"â€” 8 min read\\n\\n## What is LangGraph?\\n\\nLangGraph is a library built by the LangChain team that aims to help developers create graph-based single or multi-agent AI applications. As a low-level framework, LangGraph lets you control how agents interact with each other, which tools to use, and how information flows within the application.\\n\\n## No time to read? Watch this tutorial instead!\\n\\n## How does LangGraph work?\", \"score\": 0.9252368}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have gathered some information about LangGraph for you:\n",
      "\n",
      "### LangGraph Programming Language:\n",
      "1. **[LangGraph: What It Is and How To Use It [Tutorial]](https://lazyprogrammer.me/langgraph/)**:\n",
      "   - LangGraph primarily works with Python and also supports JavaScript and TypeScript, making it suitable for integrating into web applications.\n",
      "   \n",
      "2. **[LangGraph Tutorial: What Is LangGraph and How to Use It?](https://www.datacamp.com/tutorial/langgraph-tutorial)**:\n",
      "   - LangGraph is a library within the LangChain ecosystem designed to manage the state of agents and coordinate their interactions effectively.\n",
      "\n",
      "### LangGraph Library:\n",
      "1. **[LangGraph on PyPI](https://pypi.org/project/langgraph/0.0.25/)**:\n",
      "   - LangGraph is a library for building stateful, multi-actor applications with LLMs, inspired by Pregel and Apache Beam.\n",
      "   \n",
      "2. **[LangGraph Tutorial with Practical Example](https://www.gettingstarted.ai/langgraph-tutorial-with-example/)**:\n",
      "   - LangGraph is aimed at helping developers create graph-based single or multi-agent AI applications and offers control over agent interactions.\n",
      "\n",
      "These resources should provide you with a good understanding of LangGraph and its capabilities. Let me know if you need more information or assistance!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"I'm learning LangGraph. Could you do some research on it for me?\")\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9ad72c-d582-404c-b857-af344d7f2bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Ya that's helpful. Maybe I'll build an autonomous agent with it!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's a fantastic idea! Building an autonomous agent with LangGraph can be an exciting and rewarding project. If you need any help, guidance, or have any questions while working on your autonomous agent project, feel free to ask. I'm here to support you along the way!\n"
     ]
    }
   ],
   "source": [
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f7e28b6-7648-4038-80da-e32df774822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df56f22f-a991-41e5-a88f-bc0dd54e7853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(graph.get_state_history(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36417a5c-fa20-4dc1-b428-cbc32a259b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Messages:  27 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  26 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  25 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  25 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  24 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  22 Next:  ('tools',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  21 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  20 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  20 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  19 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  18 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  18 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  17 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  15 Next:  ('tools',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  14 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  13 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  13 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  12 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  11 Next:  ('tools',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  10 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  9 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  9 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  8 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  8 Next:  ('human',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  7 Next:  ('human',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  6 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  5 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  5 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  4 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  2 Next:  ('tools',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  1 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  0 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "to_replay = None\n",
    "for i, state in enumerate(graph.get_state_history(config)):\n",
    "    # print((i, state))\n",
    "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
    "    print(\"-\" * 80)\n",
    "    if len(state.values[\"messages\"]) == 2:\n",
    "        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
    "        to_replay = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1663e02-efc1-4259-9447-e2044973011e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tools',)\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d56-acc5-66d8-8001-98540fe86a2c'}}\n"
     ]
    }
   ],
   "source": [
    "print(to_replay.next)\n",
    "print(to_replay.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ee12037-78d9-4f10-9057-1a8e4228a6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_pUjP2dwmDgcj9McmctRDqegK)\n",
      " Call ID: call_pUjP2dwmDgcj9McmctRDqegK\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "Error: tavily_search is not a valid tool, try one of [tavily_search_results_json].\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_Dlpa0FavKfj9W3Ij1z738Cpr)\n",
      " Call ID: call_Dlpa0FavKfj9W3Ij1z738Cpr\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"LangGraph: What It Is and How To Use It [Tutorial]\", \"url\": \"https://lazyprogrammer.me/langgraph/\", \"content\": \"What Programming Languages Are Supported by LangGraph?\\n\\nLangGraph works primarily with Python. It also supports JavaScript and TypeScript, making it easy to integrate into web applications and build scalable solutions. This range of language support ensures LangGraph fits smoothly into most tech setups.\\n\\nMaster AI Engineering In Less Than 30 Days\\n\\nWhatâ€™s the LangGraph Studio?\", \"score\": 0.87782305}, {\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"url\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. [...] Skip to main content\\n\\n# LangGraph Tutorial: What Is LangGraph and How to Use It?\\n\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\n\\nJun 26, 2024  Â· 12 min read\", \"score\": 0.834852}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I found some information on LangGraph for you:\n",
      "\n",
      "1. **Title:** LangGraph: What It Is and How To Use It [Tutorial]\n",
      "   **URL:** [LangGraph Tutorial](https://lazyprogrammer.me/langgraph/)\n",
      "   **Content:** LangGraph works primarily with Python and also supports JavaScript and TypeScript, making it easy to integrate into web applications and build scalable solutions.\n",
      "\n",
      "2. **Title:** LangGraph Tutorial: What Is LangGraph and How to Use It?\n",
      "   **URL:** [LangGraph Tutorial](https://www.datacamp.com/tutorial/langgraph-tutorial)\n",
      "   **Content:** LangGraph is a library within the LangChain ecosystem designed to provide a framework for defining, coordinating, and executing multiple Large Language Model (LLM) agents in a structured and efficient manner.\n"
     ]
    }
   ],
   "source": [
    "# The `thread_ts` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
    "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863b713-38b9-493a-a366-bdcc23053d49",
   "metadata": {},
   "source": [
    "## `110.` RAGs in LangGraph\n",
    "* https://colab.research.google.com/drive/15Jwew-WBLHCKA8T-AGga7afocBePlbLz\n",
    "* https://docs.google.com/document/d/1b7DlLtvrVxihcO65dsBaIjUU-a6aLjdQfNHSOM2PhUM/edit?tab=t.0#bookmark=id.6msokmfck1u4\n",
    "* https://arxiv.org/abs/2310.11511\n",
    "\n",
    "```sh\n",
    "pip install chromadb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d2221a-e027-4245-b192-8a2767af3f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8850392e-0b16-4ed8-8938-24e5243ebf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1893: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc7e10d9-214b-451c-ba86-233d9ff17589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LLM-powered autonomous agents, memory is divided into short-term and long-term components. Short-term memory involves in-context learning, while long-term memory allows the agent to retain and recall information over extended periods, often using an external vector store for fast retrieval. This memory system enables agents to learn from past experiences and improve future actions.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7e37b7f-9558-4163-b493-b29c67b93a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1893: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52ca5493-dc4f-41a2-a632-ac0f20b6a58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1893: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='no')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66ae891f-c6dd-418d-854f-61ee900eff8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the key functions and benefits of agent memory in artificial intelligence systems?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba149cf6-4c50-4f7e-8e4a-eacf3b9dc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d0176cb-dc5e-4620-bdc6-e36a517ca765",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16a3dfac-1b31-4b72-a660-bc1bf2cd3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99195a8d-1f36-418e-9b0a-d0ec2167cdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pprint in module pprint:\n",
      "\n",
      "pprint(object, stream=None, indent=1, width=80, depth=None, *, compact=False, sort_dicts=True, underscore_numbers=False)\n",
      "    Pretty-print a Python object to a stream [default is sys.stdout].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b80cbeaa-d2fc-4ae4-a07d-5292c7ae5153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('In LLM-powered autonomous agents, short-term memory involves in-context '\n",
      " 'learning, where the model uses prompt engineering to learn and adapt '\n",
      " 'quickly. Long-term memory allows the agent to retain and recall information '\n",
      " 'over extended periods by leveraging an external vector store for fast '\n",
      " 'retrieval. This combination enables the agent to handle complex tasks and '\n",
      " 'improve over time by learning from past experiences.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "740fb3f6-7fba-4442-80a9-ef61f52633e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('Chain of thought prompting works by instructing a language model to \"think '\n",
      " 'step by step,\" breaking down complex tasks into smaller, manageable steps. '\n",
      " \"This approach enhances the model's performance on complex tasks by utilizing \"\n",
      " \"more test-time computation and providing an interpretation of the model's \"\n",
      " 'reasoning process. It involves prompting the model with high-quality '\n",
      " 'reasoning chains, either manually written or model-generated, to guide its '\n",
      " 'problem-solving approach.')\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Explain how chain of thought prompting works?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-for-genai",
   "language": "python",
   "name": "conda-for-genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
