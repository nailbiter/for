{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3caf8509-1719-4af1-82a7-fb5f4843f400",
   "metadata": {},
   "source": [
    "Langgraph workflow:\n",
    "1. initialize model and tools\n",
    "2. initialize graphs with state\n",
    "3. define graph nodes\n",
    "4. define entry points and edges\n",
    "5. compile the graph\n",
    "6. exec the graph\n",
    "\n",
    "node = fundamental unit of exec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd002dba-b9b3-438c-8bdc-268cddaff136",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41932774-af96-4ba8-8f2d-9d3fad4a2ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a889f903-2d5f-48c5-bd8a-12083f5b1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph Tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bcb953-3735-402d-ba68-f030719d54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9930c22b-5354-4b7a-87d1-b4bcccd91946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90978df6-48bb-4349-939a-4107a79a35e4",
   "metadata": {},
   "source": [
    "## 104. Simple LangGraph\n",
    "https://drive.google.com/file/d/1bJ7p5TSFCv2lWAv-ZtFPOrRoOpj3Lw2h/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d30bb5a-6664-4bc8-8583-52fe70050e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d39cab49-bcca-4341-9e79-3ed42a4bd301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10a371790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f111ca9-a241-452c-9811-d79792fe704a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10a371790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6e84d8-b885-4329-b048-74d7e5188d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10a371790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.set_finish_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ffe2ec-ad2d-48ae-a168-bd3513257f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457c71ed-614c-4bda-b64f-6573dd46791f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "603d5fcf-c8f7-408f-a34f-30732b02a659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  why is the sky blue?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with molecules and small particles in the air. Sunlight consists of different colors, each with its wavelength. Blue light waves are shorter and scatter more than the other colors when they strike the gas molecules in the atmosphere. This scattering causes the blue light to spread in all directions and become more visible, making the sky appear blue to our eyes.\\n\\nDuring sunrise or sunset, the sky can appear red or orange because the sun is lower in the sky, and its light passes through a thicker layer of the atmosphere. In this case, the shorter blue wavelengths are scattered out of the line of sight, and the longer red wavelengths dominate.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 148, 'prompt_tokens': 13, 'total_tokens': 161, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_df0f7b956c', 'id': 'chatcmpl-CANcSYYSJkKAnEs90fk5zkwTC20tK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--85b0d3d4-35eb-4a4b-acaf-9cf7eedceaa6-0', usage_metadata={'input_tokens': 13, 'output_tokens': 148, 'total_tokens': 161, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "Assistant: The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with molecules and small particles in the air. Sunlight consists of different colors, each with its wavelength. Blue light waves are shorter and scatter more than the other colors when they strike the gas molecules in the atmosphere. This scattering causes the blue light to spread in all directions and become more visible, making the sky appear blue to our eyes.\n",
      "\n",
      "During sunrise or sunset, the sky can appear red or orange because the sun is lower in the sky, and its light passes through a thicker layer of the atmosphere. In this case, the shorter blue wavelengths are scattered out of the line of sight, and the longer red wavelengths dominate.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  why is the grass green?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"The grass appears green primarily due to the presence of chlorophyll, a pigment found in the chloroplasts of plant cells. Chlorophyll is essential for photosynthesis, the process by which plants convert light energy from the sun into chemical energy. Chlorophyll absorbs light most efficiently in the blue and red wavelengths, but it reflects and transmits the green wavelengths, which is why grass looks green to our eyes. Additionally, chlorophyll plays a crucial role in capturing light energy and facilitating the synthesis of carbohydrates, which fuel the plant's growth and metabolic functions.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 13, 'total_tokens': 123, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CANctWbc3PR5mZEYtMNKCi9hhfbCC', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5e64ae3a-55ad-416c-91b4-0b23737985ec-0', usage_metadata={'input_tokens': 13, 'output_tokens': 110, 'total_tokens': 123, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "Assistant: The grass appears green primarily due to the presence of chlorophyll, a pigment found in the chloroplasts of plant cells. Chlorophyll is essential for photosynthesis, the process by which plants convert light energy from the sun into chemical energy. Chlorophyll absorbs light most efficiently in the blue and red wavelengths, but it reflects and transmits the green wavelengths, which is why grass looks green to our eyes. Additionally, chlorophyll plays a crucial role in capturing light energy and facilitating the synthesis of carbohydrates, which fuel the plant's growth and metabolic functions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Summarize what did I asked you during this session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"I'm sorry, but as an AI language model, I don't have the ability to retain or recall information from previous interactions. Each session is independent, and I don't store any personal data or conversation history. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 19, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CANdLaCCXiCkWWp9fX8CoNPERBjzV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4024b916-f550-41f5-a1d8-a1240debe4ea-0', usage_metadata={'input_tokens': 19, 'output_tokens': 47, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "Assistant: I'm sorry, but as an AI language model, I don't have the ability to retain or recall information from previous interactions. Each session is independent, and I don't store any personal data or conversation history. How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in graph.stream({\"messages\": (\"user\", user_input)}):\n",
    "        print(event[\"chatbot\"][\"messages\"])\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "377943d2-2566-45d7-8974-9ae12b8b3ab4",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got hi\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     run_with_retry(\n\u001b[1;32m    163\u001b[0m         t,\n\u001b[1;32m    164\u001b[0m         retry_policy,\n\u001b[1;32m    165\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    166\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[1;32m    167\u001b[0m                 _call,\n\u001b[1;32m    168\u001b[0m                 weakref\u001b[38;5;241m.\u001b[39mref(t),\n\u001b[1;32m    169\u001b[0m                 retry_policy\u001b[38;5;241m=\u001b[39mretry_policy,\n\u001b[1;32m    170\u001b[0m                 futures\u001b[38;5;241m=\u001b[39mweakref\u001b[38;5;241m.\u001b[39mref(futures),\n\u001b[1;32m    171\u001b[0m                 schedule_task\u001b[38;5;241m=\u001b[39mschedule_task,\n\u001b[1;32m    172\u001b[0m                 submit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[1;32m    173\u001b[0m             ),\n\u001b[1;32m    174\u001b[0m         },\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_write.py:87\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     80\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     86\u001b[0m     ]\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(\n\u001b[1;32m     88\u001b[0m         config,\n\u001b[1;32m     89\u001b[0m         writes,\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_write.py:129\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# if we want to persist writes found before hitting a ParentCommand\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# can move this to a finally block\u001b[39;00m\n\u001b[1;32m    128\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_SEND]\n\u001b[0;32m--> 129\u001b[0m write(_assemble_writes(writes))\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_write.py:184\u001b[0m, in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    182\u001b[0m     tuples\u001b[38;5;241m.\u001b[39mappend((TASKS, w))\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteTupleEntry):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ww \u001b[38;5;241m:=\u001b[39m w\u001b[38;5;241m.\u001b[39mmapper(w\u001b[38;5;241m.\u001b[39mvalue):\n\u001b[1;32m    185\u001b[0m         tuples\u001b[38;5;241m.\u001b[39mextend(ww)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteEntry):\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/graph/state.py:983\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m    980\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    981\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[1;32m    982\u001b[0m     )\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got hi\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
      "\u001b[0mDuring task with name '__start__' and id '535ce9a8-71d7-5e10-1121-e92417f89f3e'"
     ]
    }
   ],
   "source": [
    "graph.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7796d5-d89c-47a7-baab-686e8dfb5b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hey, how are you?', additional_kwargs={}, response_metadata={}, id='8f049b8d-63e4-4c55-949f-08e140e010fa'),\n",
       "  AIMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 13, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAORtKUV6Brs933oHu7Z2yB9O05yk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b763fa64-ff4f-479a-93ad-5607b653ac0b-0', usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"Hey, how are you?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0c738-e306-4f6f-ad09-97e4c220dd88",
   "metadata": {},
   "source": [
    "## 105. Tool Usage and Persistence\n",
    "* https://drive.google.com/file/d/113yjenUddxEMnHX6Ud3aqSF5jf7ee4x2/view?usp=drive_link\n",
    "* https://www.tavily.com/\n",
    "\n",
    "### tool usage\n",
    "```sh\n",
    "pip install tavily-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f353437-75a8-4f16-8398-4702446d2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert {\"TAVILY_API_KEY\"} <= set(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841f3a43-f1b2-4af5-826f-7fd5089b73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/m1j4q6jj79zbjw0qtxcldpnc0000gn/T/ipykernel_5566/3979812345.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'What is LangGraph? - IBM',\n",
       "  'url': 'https://www.ibm.com/think/topics/langgraph',\n",
       "  'content': 'Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as â€œactorsâ€ that interact with each other in a specific way. For example, to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.',\n",
       "  'score': 0.9263638},\n",
       " {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\",\n",
       "  'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
       "  'content': 'Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses.\\n Nodes: Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step. You define nodes to perform specific tasks, such as processing input, making decisions, or interacting with external APIs. [...] ## Get CPlogâ€™s stories in your inbox\\n\\nJoin Medium for free to get updates from this writer.\\n\\n## Step 3: Define Nodes\\n\\nWe define nodes for classifying the input, handling greetings, and handling search queries.',\n",
       "  'score': 0.86954874}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "tool.invoke(\"What's a 'node' in LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69234f70-170f-4603-830e-176fd3ae09c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x112944650>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Modification: tell the LLM which tools it can call\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "772b80bb-da91-4b4f-85c2-76d06f76d436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x112944650>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## note: can be replaced with pre-built `ToolNode`\n",
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "tool_node = BasicToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d907bf6c-131d-4362-865d-f528f875d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    ") -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n",
    "\n",
    "\n",
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"__end__\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3445ba-68c8-43f3-b419-75d03a570e09",
   "metadata": {},
   "source": [
    "You've hit on a crucial point, and your confusion is completely understandable. For the simple task of calling a single tool, LangGraph absolutely looks like overkill.\n",
    "\n",
    "You're not missing anything in your observation; you're just seeing the difference between a simple, pre-built tool and a powerful, low-level framework.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "* **`AgentExecutor` is like buying a pre-built, factory-made car.** Itâ€™s easy to use, works great for standard driving, and gets you from point A to B reliably. ðŸš—\n",
    "* **`LangGraph` is like being given a high-performance engine, chassis, wheels, and a toolbox.** You have to assemble the car yourself, which is more work. But you can build a Formula 1 racer, a monster truck, or a multi-person vehicleâ€”whatever you can imagine. ðŸŽï¸ðŸ› ï¸\n",
    "\n",
    "The complexity of LangGraph is the price of **ultimate control and flexibility**.\n",
    "\n",
    "---\n",
    "#### ## Key Benefits LangGraph Provides (That `AgentExecutor` Doesn't)\n",
    "\n",
    "The \"obvious benefits\" you were looking for only appear when your agent's logic needs to be more complex than the simple `Think -> Act -> Observe -> Think` loop that `AgentExecutor` forces you into.\n",
    "\n",
    "##### **1. Full Control Over the \"Loop\"**\n",
    "\n",
    "* **`AgentExecutor` has a fixed, hidden loop.** The agent calls an LLM, the LLM decides to use a tool, the tool is run, and the result is fed back to the LLM. You can't easily change this sequence.\n",
    "* **`LangGraph` makes you define the loop explicitly using nodes and edges.** This is its superpower. It means you can create any logic you want, such as:\n",
    "    * **Cycles:** Go back to a previous step if the result isn't good enough.\n",
    "    * **Human-in-the-Loop:** Add a node that pauses the process and waits for a human to approve the LLM's plan before executing a costly tool.\n",
    "    * **Multi-Agent Workflows:** Have one \"Planner\" agent create a task list, and then route tasks to multiple \"Worker\" agents that can run in parallel.\n",
    "    * **Conditional Logic:** If a tool fails, route to a \"Fallback\" node. If the user's query is ambiguous, route to a \"Clarification\" node that asks the user a question.\n",
    "\n",
    "##### **2. Explicit State Management**\n",
    "\n",
    "* **`AgentExecutor` manages state internally** in the `agent_scratchpad`. This works, but it's like a black box.\n",
    "* **`LangGraph` forces you to define an explicit `State` object** (the `TypedDict` in your code). This seems like extra work, but it's a massive advantage for complex tasks. You have complete control over the agent's \"memory.\" At any node, you can read from, add to, or modify the state with precision.\n",
    "\n",
    "##### **3. Enhanced Debugging and Visibility**\n",
    "\n",
    "* Because `AgentExecutor` is a high-level abstraction, it can sometimes be difficult to pinpoint exactly where things went wrong inside its internal loop.\n",
    "* With `LangGraph`, every step (**node**) and every transition (**edge**) is a piece of code you wrote. This makes it far easier to trace the flow of data and debug complex interactions. When you visualize a LangGraph (especially with LangSmith), you see *your* custom logic, not a generic loop.\n",
    "\n",
    "---\n",
    "###### ## When to Use Which\n",
    "\n",
    "Your Udemy course is likely starting with `AgentExecutor` because it's the fastest way to understand the basics. Then it introduces `LangGraph` for more advanced, real-world applications.\n",
    "\n",
    "* **Use `AgentExecutor` for:**\n",
    "    * **Prototyping:** Quickly building standard agents that follow the simple `(LLM -> Tool)` pattern.\n",
    "    * **Simple Tool Use:** When all you need is for an LLM to reliably call one or more tools and return a final answer.\n",
    "\n",
    "* **Use `LangGraph` for:**\n",
    "    * **Complex Control Flow:** Anything that isn't a straight line (e.g., requires cycles, branching, or retries).\n",
    "    * **Human-in-the-Loop:** When a person needs to be part of the decision-making process.\n",
    "    * **Multi-Agent Systems:** When you need to orchestrate collaboration between multiple different chains or agents.\n",
    "    * **Production Applications:** When you need the reliability, state control, and debuggability required for a robust, long-running agent.\n",
    "\n",
    "So, for your example, `AgentExecutor` is simpler and better. But if you wanted to ask, \"How many letters are in the word 'data', and if the answer is less than 5, search for a new word and try again,\" you would immediately need the cyclic power of **LangGraph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74be37-6887-419a-857f-f2d8b4834fac",
   "metadata": {},
   "source": [
    "### adding memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b1621-23e4-48ae-814f-9bf2a5b2d820",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install langgraph-checkpoint-sqlite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecd2479e-8ace-45fc-a8d8-7a5428a2ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4922223-6c38-4445-8461-d56d1bdc4370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x112c437d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88028630-5078-4c01-981d-3cf7d3e2090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b99465b8-008f-4ce7-8703-cce6f0c3afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "952d7ca2-b6af-4c4a-90a1-cfcbd19a986e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_GeneratorContextManager' object has no attribute 'get_next_version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The config is the **second positional argument** to stream() or invoke()!\u001b[39;00m\n\u001b[1;32m      4\u001b[0m events \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)]}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(event)\n\u001b[1;32m      9\u001b[0m     event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:2582\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m runtime \u001b[38;5;241m=\u001b[39m parent_runtime\u001b[38;5;241m.\u001b[39mmerge(runtime)\n\u001b[1;32m   2580\u001b[0m config[CONF][CONFIG_KEY_RUNTIME] \u001b[38;5;241m=\u001b[39m runtime\n\u001b[0;32m-> 2582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SyncPregelLoop(\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2584\u001b[0m     stream\u001b[38;5;241m=\u001b[39mStreamProtocol(stream\u001b[38;5;241m.\u001b[39mput, stream_modes),\n\u001b[1;32m   2585\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m   2586\u001b[0m     store\u001b[38;5;241m=\u001b[39mstore,\n\u001b[1;32m   2587\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   2588\u001b[0m     checkpointer\u001b[38;5;241m=\u001b[39mcheckpointer,\n\u001b[1;32m   2589\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes,\n\u001b[1;32m   2590\u001b[0m     specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels,\n\u001b[1;32m   2591\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   2592\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   2593\u001b[0m     stream_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_channels_asis,\n\u001b[1;32m   2594\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   2595\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   2596\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   2597\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability_,\n\u001b[1;32m   2598\u001b[0m     trigger_to_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_to_nodes,\n\u001b[1;32m   2599\u001b[0m     migrate_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_migrate_checkpoint,\n\u001b[1;32m   2600\u001b[0m     retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2601\u001b[0m     cache_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_policy,\n\u001b[1;32m   2602\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[1;32m   2603\u001b[0m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[1;32m   2604\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PregelRunner(\n\u001b[1;32m   2605\u001b[0m         submit\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   2606\u001b[0m             CONFIG_KEY_RUNNER_SUBMIT, weakref\u001b[38;5;241m.\u001b[39mWeakMethod(loop\u001b[38;5;241m.\u001b[39msubmit)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2609\u001b[0m         node_finished\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[1;32m   2610\u001b[0m     )\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_loop.py:934\u001b[0m, in \u001b[0;36mSyncPregelLoop.__init__\u001b[0;34m(self, input, stream, config, store, cache, checkpointer, nodes, specs, trigger_to_nodes, durability, manager, interrupt_after, interrupt_before, input_keys, output_keys, stream_keys, migrate_checkpoint, retry_policy, cache_policy)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack \u001b[38;5;241m=\u001b[39m ExitStack()\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpointer:\n\u001b[0;32m--> 934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_get_next_version \u001b[38;5;241m=\u001b[39m checkpointer\u001b[38;5;241m.\u001b[39mget_next_version\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes \u001b[38;5;241m=\u001b[39m checkpointer\u001b[38;5;241m.\u001b[39mput_writes\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes_accepts_task_path \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    937\u001b[0m         signature(checkpointer\u001b[38;5;241m.\u001b[39mput_writes)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    939\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_GeneratorContextManager' object has no attribute 'get_next_version'"
     ]
    }
   ],
   "source": [
    "user_input = \"Hi there! My name is Alex Leontiev!\"\n",
    "\n",
    "# The config is the **second positional argument** to stream() or invoke()!\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    print(event)\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4eb7651e-0cbe-4b3a-80f4-d30f0ce7b60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_GeneratorContextManager' object has no attribute 'get_next_version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# {'messages':[('human','How are you?')]}\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)]}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/main.py:2582\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2579\u001b[0m runtime \u001b[38;5;241m=\u001b[39m parent_runtime\u001b[38;5;241m.\u001b[39mmerge(runtime)\n\u001b[1;32m   2580\u001b[0m config[CONF][CONFIG_KEY_RUNTIME] \u001b[38;5;241m=\u001b[39m runtime\n\u001b[0;32m-> 2582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SyncPregelLoop(\n\u001b[1;32m   2583\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2584\u001b[0m     stream\u001b[38;5;241m=\u001b[39mStreamProtocol(stream\u001b[38;5;241m.\u001b[39mput, stream_modes),\n\u001b[1;32m   2585\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m   2586\u001b[0m     store\u001b[38;5;241m=\u001b[39mstore,\n\u001b[1;32m   2587\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   2588\u001b[0m     checkpointer\u001b[38;5;241m=\u001b[39mcheckpointer,\n\u001b[1;32m   2589\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes,\n\u001b[1;32m   2590\u001b[0m     specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels,\n\u001b[1;32m   2591\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   2592\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   2593\u001b[0m     stream_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_channels_asis,\n\u001b[1;32m   2594\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   2595\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   2596\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   2597\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability_,\n\u001b[1;32m   2598\u001b[0m     trigger_to_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_to_nodes,\n\u001b[1;32m   2599\u001b[0m     migrate_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_migrate_checkpoint,\n\u001b[1;32m   2600\u001b[0m     retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2601\u001b[0m     cache_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_policy,\n\u001b[1;32m   2602\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[1;32m   2603\u001b[0m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[1;32m   2604\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PregelRunner(\n\u001b[1;32m   2605\u001b[0m         submit\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   2606\u001b[0m             CONFIG_KEY_RUNNER_SUBMIT, weakref\u001b[38;5;241m.\u001b[39mWeakMethod(loop\u001b[38;5;241m.\u001b[39msubmit)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2609\u001b[0m         node_finished\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[1;32m   2610\u001b[0m     )\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langgraph/pregel/_loop.py:934\u001b[0m, in \u001b[0;36mSyncPregelLoop.__init__\u001b[0;34m(self, input, stream, config, store, cache, checkpointer, nodes, specs, trigger_to_nodes, durability, manager, interrupt_after, interrupt_before, input_keys, output_keys, stream_keys, migrate_checkpoint, retry_policy, cache_policy)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack \u001b[38;5;241m=\u001b[39m ExitStack()\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpointer:\n\u001b[0;32m--> 934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_get_next_version \u001b[38;5;241m=\u001b[39m checkpointer\u001b[38;5;241m.\u001b[39mget_next_version\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes \u001b[38;5;241m=\u001b[39m checkpointer\u001b[38;5;241m.\u001b[39mput_writes\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointer_put_writes_accepts_task_path \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    937\u001b[0m         signature(checkpointer\u001b[38;5;241m.\u001b[39mput_writes)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    939\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_GeneratorContextManager' object has no attribute 'get_next_version'"
     ]
    }
   ],
   "source": [
    "graph.invoke(\n",
    "    # {'messages':[('human','How are you?')]}\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae3890d0-342d-4cf7-9432-96896ef20481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First interaction ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Alex Leontiev! How can I assist you today?\n",
      "\n",
      "--- Second interaction (testing memory) ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You mentioned that your name is Alex Leontiev.\n"
     ]
    }
   ],
   "source": [
    "# from langgraph_sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Use a 'with' statement to properly manage the memory connection\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    # 1. Compile the graph with the checkpointer INSIDE the 'with' block\n",
    "    graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "    # 2. Define a configuration for a specific conversation thread\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    # 3. Interact with the graph INSIDE the 'with' block\n",
    "    print(\"--- First interaction ---\")\n",
    "    user_input = \"Hi there! My name is Alex Leontiev!\"\n",
    "    response = graph.invoke({\"messages\": [(\"user\", user_input)]}, config)\n",
    "    # Pretty-print the final response\n",
    "    response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    print(\"\\n--- Second interaction (testing memory) ---\")\n",
    "    user_input_2 = \"What did I say my name was?\"\n",
    "    response_2 = graph.invoke({\"messages\": [(\"user\", user_input_2)]}, config)\n",
    "    response_2[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83bf472-dc7d-496d-8003-fdb5831172ae",
   "metadata": {},
   "source": [
    "## 106. Human in the loop\n",
    "* https://colab.research.google.com/drive/113yjenUddxEMnHX6Ud3aqSF5jf7ee4x2#scrollTo=dba1b168-f8e0-496d-9bd6-37198fb4776e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e9a2c5-6a7b-4cc8-b78b-0d291437b633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x116afb530>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=[tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db3a043a-fd93-48e7-9511-16bf4b468125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- interaction #1 ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_ELU44JJPnQB2Cxz0eh8LnCs8)\n",
      " Call ID: call_ELU44JJPnQB2Cxz0eh8LnCs8\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "('tools',)\n",
      "[{'name': 'tavily_search_results_json', 'args': {'query': 'LangGraph programming language'}, 'id': 'call_ELU44JJPnQB2Cxz0eh8LnCs8', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "# from langgraph_sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Use a 'with' statement to properly manage the memory connection\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    # 1. Compile the graph with the checkpointer INSIDE the 'with' block\n",
    "    graph = graph_builder.compile(\n",
    "        checkpointer=memory,\n",
    "        # This is new!\n",
    "        interrupt_before=[\"tools\"],\n",
    "    )\n",
    "\n",
    "    # 2. Define a configuration for a specific conversation thread\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    # 3. Interact with the graph INSIDE the 'with' block\n",
    "    messages = [\n",
    "        \"\"\"I'm learning LangGraph. Could you do some research on it for me?\"\"\"\n",
    "        # \"Hi there! My name is Alex Leontiev!\",\n",
    "        # \"What did I say my name was?\",\n",
    "    ]\n",
    "    for i, user_input in enumerate(messages):\n",
    "        print(f\"--- interaction #{i+1} ---\")\n",
    "        # user_input =\n",
    "        response = graph.invoke({\"messages\": [(\"user\", user_input)]}, config)\n",
    "        # Pretty-print the final response\n",
    "        response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    # print(\"\\n--- Second interaction (testing memory) ---\")\n",
    "    # user_input_2 =\n",
    "    # response_2 = graph.invoke({\"messages\": [(\"user\", user_input_2)]}, config)\n",
    "    # response_2[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    snapshot = graph.get_state(config)\n",
    "    print(snapshot.next)\n",
    "\n",
    "    existing_message = snapshot.values[\"messages\"][-1]\n",
    "    print(existing_message.tool_calls)\n",
    "\n",
    "    ## allow continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "802356de-f01c-4b05-9c6b-3ffecdd3d6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x1182627e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f8bfdd8-a15e-4103-a0fd-6c909c850063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- interaction #1 ---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_Mh2p2ixAanSRk9doJn5U9K7C)\n",
      " Call ID: call_Mh2p2ixAanSRk9doJn5U9K7C\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "('tools',)\n",
      "[{'name': 'tavily_search_results_json', 'args': {'query': 'LangGraph programming language'}, 'id': 'call_Mh2p2ixAanSRk9doJn5U9K7C', 'type': 'tool_call'}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_Mh2p2ixAanSRk9doJn5U9K7C)\n",
      " Call ID: call_Mh2p2ixAanSRk9doJn5U9K7C\n",
      "  Args:\n",
      "    query: LangGraph programming language\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"LangGraph: A Framework for Building Stateful Multi-Agent ...\", \"url\": \"https://medium.com/@ken_lin/langgraph-a-framework-for-building-stateful-multi-agent-llm-applications-a51d5eb68d03\", \"content\": \"> LangGraph is a powerful Python library designed for constructing stateful, multi-actor applications with Large Language Models (LLMs). It extends the capabilities of LangChain Expression Language (LCEL) while specifically addressing limitations in existing frameworks for agent development. As a specialized tool for creating complex LLM applications, LangGraph provides a structured approach to building sophisticated workflows that require cyclical processing patterns. [...] LangGraph is built on top of and intended to be used with LangChain, extending the LangChain Expression Language with capabilities to coordinate multiple chains or actors across computational steps in a cyclic manner. The library is inspired by graph processing frameworks like Pregel and Apache Beam, offering an interface similar to NetworkX for building complex agent systems. Unlike purely Directed Acyclic Graph (DAG) frameworks, LangGraphâ€™s main distinctive feature is its support for cycles, [...] LangGraph represents a significant advancement in the toolkit available for developing sophisticated LLM applications, particularly those involving agent-like behaviors and multi-agent coordination. By extending LangChain with support for cyclical workflows and robust state management, it addresses key limitations of earlier approaches and enables new possibilities for AI systems that can reason, act, and interact in more complex ways.\", \"score\": 0.8974172}, {\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"url\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. [...] Skip to main content\\n\\n# LangGraph Tutorial: What Is LangGraph and How to Use It?\\n\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\n\\nJun 26, 2024  Â· 12 min read\", \"score\": 0.834852}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a powerful Python library that is part of the LangChain ecosystem. It is designed for building stateful and multi-actor applications using Large Language Models (LLMs). Here's a brief overview based on recent information:\n",
      "\n",
      "1. **Purpose and Design**: LangGraph extends the capabilities of the LangChain Expression Language (LCEL) by addressing limitations in existing frameworks for agent development. It helps in constructing complex applications that require coordinated interactions between multiple agents or chains.\n",
      "\n",
      "2. **Features**:\n",
      "    - **Multi-Agent Coordination**: It provides a framework to define, coordinate, and execute multiple LLM agents in a structured way. This is essential for applications where managing the state, coordinating interactions, and handling errors are critical.\n",
      "    - **Cyclical Workflows**: Unlike traditional Directed Acyclic Graph (DAG) frameworks, LangGraph supports cycles, which allows for more flexible processing patterns that are useful in sophisticated agent systems.\n",
      "    - **Robust State Management**: LangGraph enables robust state management and is suitable for applications needing complex reasoning and interaction abilities.\n",
      "\n",
      "3. **Inspirations and Influences**: The library takes inspiration from graph processing frameworks such as Pregel and Apache Beam and offers an interface similar to NetworkX. This aids in building complex agent systems capable of handling stateful interactions and workflows.\n",
      "\n",
      "4. **Potential Applicability**: LangGraph is particularly beneficial for developing advanced LLM applications that involve agent-like behaviors and require sophisticated coordination.\n",
      "\n",
      "For more detailed information, you can explore resources like [this Medium article](https://medium.com/@ken_lin/langgraph-a-framework-for-building-stateful-multi-agent-llm-applications-a51d5eb68d03) or tutorials like [this one on DataCamp](https://www.datacamp.com/tutorial/langgraph-tutorial).\n"
     ]
    }
   ],
   "source": [
    "# from langgraph_sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Use a 'with' statement to properly manage the memory connection\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    # 1. Compile the graph with the checkpointer INSIDE the 'with' block\n",
    "    graph = graph_builder.compile(\n",
    "        checkpointer=memory,\n",
    "        # This is new!\n",
    "        interrupt_before=[\"tools\"],\n",
    "    )\n",
    "\n",
    "    # 2. Define a configuration for a specific conversation thread\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    # 3. Interact with the graph INSIDE the 'with' block\n",
    "    messages = [\n",
    "        \"\"\"I'm learning LangGraph. Could you do some research on it for me?\"\"\"\n",
    "        # \"Hi there! My name is Alex Leontiev!\",\n",
    "        # \"What did I say my name was?\",\n",
    "    ]\n",
    "    for i, user_input in enumerate(messages):\n",
    "        print(f\"--- interaction #{i+1} ---\")\n",
    "        # user_input =\n",
    "        response = graph.invoke({\"messages\": [(\"user\", user_input)]}, config)\n",
    "        # Pretty-print the final response\n",
    "        response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    # print(\"\\n--- Second interaction (testing memory) ---\")\n",
    "    # user_input_2 =\n",
    "    # response_2 = graph.invoke({\"messages\": [(\"user\", user_input_2)]}, config)\n",
    "    # response_2[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    snapshot = graph.get_state(config)\n",
    "    print(snapshot.next)\n",
    "\n",
    "    existing_message = snapshot.values[\"messages\"][-1]\n",
    "    print(existing_message.tool_calls)\n",
    "\n",
    "    ## allow continuation\n",
    "    events = graph.stream(None, config, stream_mode=\"values\")\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-for-genai",
   "language": "python",
   "name": "conda-for-genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
