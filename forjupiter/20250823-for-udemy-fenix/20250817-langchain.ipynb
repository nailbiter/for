{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f6f399-8c3b-4af3-ae8c-a1ac0a62a7df",
   "metadata": {},
   "source": [
    "* https://docs.google.com/document/d/1b7DlLtvrVxihcO65dsBaIjUU-a6aLjdQfNHSOM2PhUM/edit?tab=t.0#heading=h.v0phdvfalpr2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc866dac-b1b0-4ce4-9990-5dea8256244e",
   "metadata": {},
   "source": [
    "## setup and tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a239417-e284-42ed-92dc-ba8ead8b34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25481e72-a46a-47ef-a0be-a39f9fabf387",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install langchain-google-genai langchain langchain_openai beautifulsoup4 langchain-community lxml serpapi google-search-results faiss-cpu langchainhub langchain_elasticsearch wikipedia langgraph  --upgrade\n",
    "```\n",
    "\n",
    "or `-U`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a31554-bf98-4134-823c-1f7421bbf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from openai import OpenAI\n",
    "import jupyter_black\n",
    "import tqdm, tqdm.notebook\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "assert {\"OPENAI_API_KEY\", \"GOOGLE_API_KEY\"} <= set(os.environ)\n",
    "\n",
    "GEMINI_MODEL_NAME_CLEVER = \"gemini-2.0-flash\"\n",
    "GEMINI_MODEL_NAME_FAST = \"gemini-2.0-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96269410-51db-42e0-82ee-dd0dc0833e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing with LangChain effectively involves understanding its core components, leveraging its strengths, and following best practices for maintainability, performance, and reliability. Here's a breakdown of key best practices:\n",
      "\n",
      "**1. Understand the Core Components:**\n",
      "\n",
      "*   **LLMs (Large Language Models):**  The foundation.  Know which LLMs are available, their strengths and weaknesses (e.g., cost, context window, speed, capabilities), and how to integrate them.\n",
      "*   **Prompts:**  Crafting effective prompts is crucial.  Learn prompt engineering techniques (few-shot learning, chain-of-thought, persona development) to get the desired outputs.\n",
      "*   **Chains:**  Chains are sequences of operations.  Understand the different chain types (Sequential, Router, Transform, etc.) and choose the right one for your task.\n",
      "*   **Indexes:**  Indexes store and retrieve data from external sources (documents, databases).  Master the different index types (Vectorstores, Document loaders, etc.) and choose the appropriate one for your data and retrieval needs.\n",
      "*   **Agents:** Agents use LLMs to decide which actions to take.  Understand the agent types (Zero-shot, Plan-and-Execute, etc.) and how to define tools for your agent.\n",
      "*   **Memory:**  Manage the context of conversations.  Choose the appropriate memory type (ConversationBufferMemory, ConversationBufferWindowMemory, etc.) to store and retrieve past interactions.\n",
      "*   **Callbacks:**  Monitor and log the execution of LangChain components.  Use callbacks to track costs, debug issues, and provide user feedback.\n",
      "\n",
      "**2. Prompt Engineering Best Practices:**\n",
      "\n",
      "*   **Be Specific and Clear:**  The more precise your instructions, the better the results.  Avoid ambiguity.\n",
      "*   **Provide Context:**  Give the LLM necessary background information or examples (few-shot learning) to guide its response.\n",
      "*   **Specify the Desired Output Format:**  Tell the LLM how you want the answer presented (e.g., JSON, list, paragraph).\n",
      "*   **Iterate and Refine:**  Prompt engineering is iterative.  Experiment with different phrasing, examples, and instructions to optimize your prompts.\n",
      "*   **Use Templates:**  LangChain's prompt templates are excellent for managing and reusing prompts.\n",
      "*   **Test Thoroughly:**  Evaluate your prompts with various inputs and edge cases.\n",
      "\n",
      "**3. Chain Design and Optimization:**\n",
      "\n",
      "*   **Keep Chains Simple:**  Break down complex tasks into smaller, more manageable chains.\n",
      "*   **Choose the Right Chain Type:**  Select the chain type that best suits your needs (Sequential, Router, etc.).\n",
      "*   **Optimize for Speed and Cost:**  Consider the number of API calls and the cost associated with each step in the chain.  Use caching where appropriate.\n",
      "*   **Error Handling:**  Implement robust error handling to gracefully manage unexpected situations (e.g., API errors, invalid inputs).\n",
      "*   **Modular Design:**  Break your chains into reusable components (functions, classes) to promote maintainability and reduce redundancy.\n",
      "\n",
      "**4. Indexing and Data Handling:**\n",
      "\n",
      "*   **Choose the Right Vectorstore:**  Select a vectorstore that aligns with your data size, query performance requirements, and cost considerations. Popular choices include Chroma, Pinecone, Weaviate, and FAISS.\n",
      "*   **Chunking Strategy:**  Experiment with different chunking strategies (e.g., fixed size, sliding window) to optimize retrieval accuracy and relevance.\n",
      "*   **Metadata:**  Leverage metadata to filter and refine search results.  Include relevant information about your documents (e.g., author, date, topic).\n",
      "*   **Data Cleaning and Preprocessing:**  Prepare your data by cleaning and preprocessing it to remove noise and improve accuracy.\n",
      "*   **Document Loaders:**  Use appropriate document loaders to handle different file formats (PDF, text, CSV, etc.).\n",
      "\n",
      "**5. Agent Design and Tool Selection:**\n",
      "\n",
      "*   **Define Clear Tools:**  Carefully design the tools your agent can use.  Each tool should have a specific purpose and well-defined inputs and outputs.\n",
      "*   **Tool Descriptions:**  Provide clear and concise descriptions of your tools.  The agent uses these descriptions to decide which tool to use.\n",
      "*   **Agent Type Selection:**  Choose the agent type that best suits your task (e.g., Zero-shot, Plan-and-Execute).\n",
      "*   **Error Handling for Tools:**  Handle errors that may occur when your agent uses a tool.\n",
      "*   **Tool Scope:** Limit the tools available to the agent to improve focus and prevent unintended actions.\n",
      "\n",
      "**6. Memory Management:**\n",
      "\n",
      "*   **Choose the Right Memory Type:**  Select the appropriate memory type based on your application's needs (e.g., ConversationBufferMemory for basic conversation history, ConversationBufferWindowMemory for a limited history).\n",
      "*   **Context Window Awareness:**  Be mindful of the LLM's context window size.  Manage the memory to avoid exceeding the context window.\n",
      "*   **Memory Pruning:**  Consider techniques for pruning the memory to keep the context relevant and avoid unnecessary tokens.\n",
      "*   **Persistence:**  If you need to persist the conversation history, use memory that supports persistence (e.g., storing the memory in a database).\n",
      "\n",
      "**7. Callbacks and Monitoring:**\n",
      "\n",
      "*   **Use Callbacks:**  Implement callbacks to monitor the execution of your LangChain components.\n",
      "*   **Logging:**  Log relevant information (prompts, responses, errors) for debugging and analysis.\n",
      "*   **Cost Tracking:**  Track the cost of API calls to monitor your expenses.\n",
      "*   **Performance Monitoring:**  Monitor the performance of your chains and indexes to identify bottlenecks.\n",
      "*   **User Feedback:**  Use callbacks to provide real-time feedback to the user (e.g., \"Thinking...\", \"Generating response...\").\n",
      "\n",
      "**8. Code Organization and Maintainability:**\n",
      "\n",
      "*   **Use a Consistent Structure:**  Organize your code logically (e.g., separate files for prompts, chains, indexes, agents).\n",
      "*   **Code Comments:**  Add clear and concise comments to explain your code.\n",
      "*   **Version Control:**  Use a version control system (e.g., Git) to track changes and collaborate with others.\n",
      "*   **Testing:**  Write unit tests and integration tests to ensure your code works correctly.\n",
      "*   **Documentation:**  Document your code, including your prompts, chains, and agents.\n",
      "*   **Refactoring:**  Regularly refactor your code to improve readability and maintainability.\n",
      "\n",
      "**9. Security Considerations:**\n",
      "\n",
      "*   **Input Validation:**  Validate user inputs to prevent malicious attacks (e.g., prompt injection).\n",
      "*   **Data Privacy:**  Protect sensitive data.  Avoid storing personal information in your prompts or responses.\n",
      "*   **Rate Limiting:**  Implement rate limiting to protect your API keys and prevent abuse.\n",
      "*   **Access Control:**  Control access to your application and data.\n",
      "*   **Prompt Injection Prevention:**  Implement measures to mitigate prompt injection attacks (e.g., input sanitization, prompt filtering).\n",
      "\n",
      "**10. Continuous Learning and Adaptation:**\n",
      "\n",
      "*   **Stay Updated:**  LangChain is constantly evolving.  Keep up-to-date with the latest features, updates, and best practices.\n",
      "*   **Experiment:**  Experiment with different approaches and techniques to improve your results.\n",
      "*   **Community Resources:**  Utilize the LangChain documentation, examples, and community resources (e.g., Stack Overflow, Discord) to learn and get help.\n",
      "*   **Monitor LLM Updates:**  LLMs are frequently updated. Stay informed about the changes in model capabilities and costs.\n",
      "\n",
      "By following these best practices, you can build robust, efficient, and reliable applications with LangChain. Remember that the best approach depends on your specific project requirements and the LLMs you choose to use.  Adapt these guidelines to suit your needs and always iterate to optimize your results.\n",
      "CPU times: user 1.65 s, sys: 460 ms, total: 2.11 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "response = llm.invoke(\"What are the best practices for developing with LangChain?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a078a1-9f34-4f21-a57b-3435bd23bf9f",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "# chat = ChatOpenAI(openai_api_key=\"...\")\n",
    "# If you have an envionrment variable set for OPENAI_API_KEY, you can just do:\n",
    "chat = ChatOpenAI()\n",
    "chat.invoke(\"Hello, how are you?\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "654b4fcb-f826-4012-96b9-a35fa19c5c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well, thank you for asking! As a large language model, I don't experience emotions like humans do, but I am functioning and ready to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable.\n",
    "# You can get one from Google AI Studio: https://aistudio.google.com/app/apikey\n",
    "\n",
    "# Initialize the Gemini chat model\n",
    "# You can also specify other models like \"gemini-1.5-pro\"\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "# Invoke the model with a prompt\n",
    "response = chat.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0333dc9-a2a4-4b45-9bf1-02348ff467b3",
   "metadata": {},
   "source": [
    "## 85. Chat Models -- Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4b12e-cda6-4142-ac41-f61911adfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = chat.invoke(\"What is the capital of France?\")\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb264da-cacc-4fa9-87c0-b7a305281b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0f7d4-5d29-45c2-bcd3-385cfcfccc53",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "result = chat.invoke(messages)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5784b22-dce4-4992-9562-cd3af0fa1f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Ensure your GOOGLE_API_KEY environment variable is set\n",
    "# 1. Initialize the Gemini chat model\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "# 2. Define the message(s) for the model\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that generates company names.\"),\n",
    "    HumanMessage(content=text),\n",
    "]\n",
    "\n",
    "# 3. Invoke the model with the messages\n",
    "result = chat.invoke(messages)\n",
    "\n",
    "# 4. Print the AI's response content\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a4456-7f1f-4747-839e-943486e3425c",
   "metadata": {},
   "source": [
    "## 86. Chat Prompt Templates\n",
    "* https://drive.google.com/file/d/1JoyxZlYfngmXnvrRyo7qqvUoB7qz6il0/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6a8d9-78c2-4604-b3e0-cbb9a603efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that generates company names\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat_prompt_template.invoke(\n",
    "    {\n",
    "        \"text\": \"What would be a good company name for a company that makes colorful socks?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# model = Cha(model='gpt-4o-mini')\n",
    "\n",
    "ai_llm_result = chat.invoke(result)\n",
    "print(ai_llm_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561f15e-9717-4d09-9e29-8ba7d53fbc3a",
   "metadata": {},
   "source": [
    "## 87. Streaming\n",
    "* https://drive.google.com/file/d/18sGlOZ8AKwON1CXUMnqf9ONfj7bwjSiO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db2846-80d8-4ef7-a5ad-b225aba91066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm, tqdm.notebook\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    # streaming=True,\n",
    ")\n",
    "for chunk in tqdm.notebook.tqdm(chat.stream(\"What is the capital of the moon?\")):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb60bd3-6ed4-46de-b0bb-af66c3962836",
   "metadata": {},
   "source": [
    "## 88. Output Parsers\n",
    "* https://drive.google.com/file/d/1QWwi3AOCHEoMR83zR21sB7zzKdUxVdfO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7202cb-737a-4396-998f-36a914b55ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddb083-21d6-4463-baf5-9de828dad2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7732fa1-8542-4911-aa58-18613e3b8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6c1e9-e7b7-4a97-b4f5-94318c5358b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "\n",
    "messages = chat_prompt.invoke(\n",
    "    {\n",
    "        \"query\": \"What is a really funny joke about Python programming?\",\n",
    "        \"format_instructions\": parser.get_format_instructions(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8f8a7-7e75-4dab-974c-7845e12c1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()\n",
    "## does not work with Gemini\n",
    "result = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af04d1-8046-43d5-a1b7-6da3f7c30306",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    joke_object = parser.parse(result.content)\n",
    "    print(joke_object.setup)\n",
    "    print(joke_object.punchline)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c358e2-007f-429a-8297-c71275ed72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7afe0-a0b0-4d5b-9233-6556838c9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f91080-cfa2-49a3-b42c-40ce6b224b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    explanation: str = Field(\n",
    "        description=\"A detailed explanation of why this joke is funny.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3a9b0-6199-410e-9c69-9cc95f51c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275de99-b846-4cbd-88da-9f4bcfa2bf2e",
   "metadata": {},
   "source": [
    "## 89. Summarizing large amounts of text\n",
    "* https://colab.research.google.com/drive/11t0e03SThhKRPq9T1M7xg6BcooBFaTkA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775946d-4c56-414d-a533-914ba58de93a",
   "metadata": {},
   "source": [
    "### crisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d808c-3f91-4376-b8a9-4c0eb30c4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    temperature=0,\n",
    "    model=GEMINI_MODEL_NAME_CLEVER,\n",
    ")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "res = chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56313b30-4169-49ec-9d0c-e484fc5122bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"input_documents\"]\n",
    "Markdown(res[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224b2aa-5ecc-456a-9851-b141d0a820aa",
   "metadata": {},
   "source": [
    "### map reduce\n",
    "\n",
    "* problem if pages refer to each other (since summaries are done independently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f7fa2-83c6-4dcb-99c0-b9e6cab914cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import (\n",
    "    ReduceDocumentsChain,\n",
    "    MapReduceDocumentsChain,\n",
    "    StuffDocumentsChain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f491b-eed6-4373-ae2f-9a4447342be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(temperature=0, model=GEMINI_MODEL_NAME_CLEVER)\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# map_chain:\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes.\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26d252-e08a-4b56-879f-33d170308e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccadc0-b9f5-4623-ab75-d49b33a39531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ddcd9-71e2-4653-805e-6624093556e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print()\n",
    "res = map_reduce_chain.invoke(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1fa67-aced-4345-aedb-192336c7270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(res[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db3570-931e-47fc-ae4a-4258cc741b9f",
   "metadata": {},
   "source": [
    "### template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109f63e-e3a5-435b-8484-8c14782636c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)\n",
    "\n",
    "# Page 1 --> Page 2 (Refine) --> Page 3 (Refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b77696-979a-473c-89a2-7c66a06ea248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(result[\"intermediate_steps\"]):\n",
    "    display(Markdown(f\"## step {i+1}\\n{v}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ee122-daab-4695-9547-55f1b7ed317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65598b-c2ad-422c-9ebb-407e0e372a3e",
   "metadata": {},
   "source": [
    "## 91. Document Loaders, Text Splitting, Creating LangChain Documents\n",
    "\n",
    "https://colab.research.google.com/drive/1YdtBCggWStErmFeP5GBSmEaw04kXeKqD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f681afb-0def-485e-b64e-acba6c7c4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import requests\n",
    "\n",
    "# Get this file and save it locally:\n",
    "url = \"https://github.com/hammer-mt/thumb/blob/master/README.md\"\n",
    "\n",
    "# Save it locally:\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the text from the HTML:\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "loader = TextLoader(\"README.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab45fc-b5c7-4fc8-821c-77db1e7079e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac22e81-df9a-4f65-b986-2d604671b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "[Document(page_content=\"test\", metadata={\"test\": \"test\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa692e28-061f-4d92-8250-ca8abd7f0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into sentences:\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "final_docs = text_splitter.split_documents(loader.load())\n",
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c560f-0533-43a2-aeb8-647960c0cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20a453-590b-43a9-b71a-306a02cdee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chain = load_summarize_chain(llm=chat, chain_type=\"map_reduce\")\n",
    "res = chain.invoke(\n",
    "    {\n",
    "        \"input_documents\": final_docs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853c392-b6bd-4a21-ab70-7babac04a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.keys()\n",
    "res[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee58ead-f5ea-4b49-a05c-580744658705",
   "metadata": {},
   "source": [
    "## 92. Tagging Documents\n",
    "https://colab.research.google.com/drive/1Gn1IxMqz0RcOaDKVdY7cnzgik0JoQlqZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50718f47-4d27-4e63-8211-57501d8091e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixes a bug with asyncio and jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cbb2c-9f7f-4efe-9db6-36f8a2cb5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493471d-d23f-4bb2-b8c7-7a30dbcf8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitemap_loader = SitemapLoader(web_path=\"https://understandingdata.com/sitemap.xml\")\n",
    "sitemap_loader.requests_per_second = 5\n",
    "docs = sitemap_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb080f-8883-4469-bddc-e1981a440684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\"type\": \"string\"},\n",
    "        \"aggressiveness\": {\"type\": \"integer\"},\n",
    "        \"primary_topic\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The main topic of the document.\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"primary_topic\", \"sentiment\", \"aggressiveness\"],\n",
    "}\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "## does not work with Gemini\n",
    "# llm = ChatGoogleGenerativeAI(temperature=0, model=GEMINI_MODEL_NAME_CLEVER)\n",
    "chain = create_tagging_chain(schema, llm, output_key=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee82b9-9eb4-4a27-8f73-8fda88b2b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Remove the 0:10 to run on all documents:\n",
    "for index, doc in enumerate(docs[0:10]):\n",
    "    print(f\"Processing doc {index +1}\")\n",
    "    chain_result = chain.invoke({\"input\": doc.page_content})\n",
    "    results.append(chain_result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75147b4b-ab4b-4298-85e9-548c495e045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0aff68-09f5-4472-bbdd-88a6466c29f5",
   "metadata": {},
   "source": [
    "### with Pyadntic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ba245-dd5a-429d-95e6-d8a4f6df239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixes a bug with asyncio and jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_tagging_chain_pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. Pydantic Schema Definition\n",
    "class DocumentTags(BaseModel):\n",
    "    \"\"\"Pydantic model for the tags to be extracted from the document.\"\"\"\n",
    "\n",
    "    sentiment: str = Field(\n",
    "        description=\"The overall sentiment of the document (e.g., positive, negative, neutral).\"\n",
    "    )\n",
    "    aggressiveness: int = Field(\n",
    "        description=\"A rating from 1 to 10 of how aggressive the text is.\"\n",
    "    )\n",
    "    primary_topic: str = Field(description=\"The main topic of the document.\")\n",
    "\n",
    "\n",
    "# 2. Load Documents\n",
    "# Note: This can take a moment to run\n",
    "sitemap_loader = SitemapLoader(web_path=\"https://understandingdata.com/sitemap.xml\")\n",
    "sitemap_loader.requests_per_second = 5\n",
    "docs = sitemap_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b45ff-3eb2-425c-93f1-49d878939e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493d8a6-ef21-45ec-bd84-863c7db90122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize Gemini LLM\n",
    "# Make sure your GOOGLE_API_KEY environment variable is set\n",
    "# llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.0-flash-lite\")\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 4. Create the Pydantic Tagging Chain\n",
    "# This chain is specifically designed to work with Pydantic models\n",
    "chain = create_tagging_chain_pydantic(DocumentTags, llm)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 5. Process Documents\n",
    "# Using a smaller slice [0:3] for a quick demonstration\n",
    "for index, doc in tqdm.notebook.tqdm(list(enumerate(docs[:10]))):\n",
    "    print(f\"--- Processing doc {index + 1} ---\")\n",
    "\n",
    "    # The input to invoke is the document content\n",
    "    chain_result = chain.invoke({\"input\": doc.page_content})\n",
    "\n",
    "    # The result is a Pydantic object, which we convert to a dict\n",
    "    # Access the result via the \"function\" key\n",
    "    # tag_data = chain_result[\"function\"].dict()\n",
    "    tag_data = chain_result[\"text\"]\n",
    "    results.append(tag_data)\n",
    "\n",
    "    print(tag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ceccd-18a1-4119-84ea-c0c990206bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display results in a DataFrame\n",
    "df = pd.DataFrame(map(dict, results))\n",
    "print(\"\\n--- Final Results ---\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ce924-c2a7-4a50-83ef-7ea897c102b7",
   "metadata": {},
   "source": [
    "## 93. Tracing with LangSmith\n",
    "* https://colab.research.google.com/drive/1Sf-_1QP92iuJmFkykCufOYRkOB7tkliU\n",
    "* https://smith.langchain.com\n",
    "* https://serpapi.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94539d54-f1f2-41aa-81f3-8754a2a17225",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert {\"LANGCHAIN_API_KEY\", \"SERPAPI_API_KEY\"} <= set(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126efb30-3271-48e2-be43-f36e7b718fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "unique_id = uuid.uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"LANGCHAIN_API_KEY\"\n",
    "\n",
    "# # Used by the agent in this tutorial\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"SERPAPI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0759b-b384-484e-a06d-bca273cfdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f6d78-90df-45b0-a209-cf0e9642454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_CLEVER, temperature=0)\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a6a55-4ad9-4939-9d25-ca4afe63e070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import tenacity\n",
    "\n",
    "inputs = [\n",
    "    \"How many people live in canada as of 2023?\",\n",
    "    \"who is dua lipa's boyfriend? what is his age raised to the .43 power?\",\n",
    "    \"what is dua lipa's boyfriend age raised to the .43 power?\",\n",
    "    \"how far is it from paris to boston in miles\",\n",
    "    \"what was the total number of points scored in the 2023 super bowl? what is that number raised to the .23 power?\",\n",
    "    \"what was the total number of points scored in the 2023 super bowl raised to the .23 power?\",\n",
    "    \"how many more points were scored in the 2023 super bowl than in the 2022 super bowl?\",\n",
    "    \"what is 153 raised to .1312 power?\",\n",
    "    \"who is kendall jenner's boyfriend? what is his height (in inches) raised to .13 power?\",\n",
    "    \"what is 1213 divided by 4345?\",\n",
    "]\n",
    "results = []\n",
    "\n",
    "\n",
    "async def arun(agent, input_example):\n",
    "    try:\n",
    "        return await agent.arun(input_example)\n",
    "    except Exception as e:\n",
    "        # The agent sometimes makes mistakes! These will be captured by the tracing.\n",
    "        return e\n",
    "\n",
    "\n",
    "@tenacity.retry(stop=tenacity.stop_after_attempt(4), wait=tenacity.wait_fixed(30))\n",
    "def run(agent, input_example: str) -> str:\n",
    "    return agent.invoke(input_example)\n",
    "\n",
    "\n",
    "# for input_example in inputs:\n",
    "#     results.append(arun(agent, input_example))\n",
    "# results = await asyncio.gather(*results)\n",
    "\n",
    "for input_example in tqdm.notebook.tqdm(inputs):\n",
    "    time.sleep(2)\n",
    "    results.append(run(agent, input_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe29bb1-ae28-48aa-8fe4-16165ef147e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n",
    "\n",
    "# Logs are submitted in a background thread to avoid blocking execution.\n",
    "# For the sake of this tutorial, we want to make sure\n",
    "# they've been submitted before moving on. This is also\n",
    "# useful for serverless deployments.\n",
    "wait_for_all_tracers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2dcaa0-deef-4606-9d2e-88a6ee4ca897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## gemini\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(pd.DataFrame(results).to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c0a58-6657-4bbc-9e90-5a66f43223c6",
   "metadata": {},
   "source": [
    "tracing result: https://drive.google.com/drive/folders/1_4hnRpTYZxO_JLBDDE2HCESQW0eHE6Bj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6799dd-e707-4734-8b96-bde3df4f2060",
   "metadata": {},
   "source": [
    "## 94. LangChain Hub\n",
    "* https://colab.research.google.com/drive/1lxCk4cnk60rzmu0Wz6pzK-sUmWGca5b0\n",
    "* https://docs.smith.langchain.com/prompt_engineering/how_to_guides#prompt-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3e290-23fb-4b7d-95fc-d433dd89533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf1a02-f62d-43ac-94b0-b62ea5d35ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"homanp/question-answer-pair\")\n",
    "prompt_two = hub.pull(\"gitmaxd/synthetic-training-data\")\n",
    "prompt_three = hub.pull(\"rlm/text-to-sql\")\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79610019-1f43-4ef0-a239-4b8caecaa272",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc55b0bb-d856-48bc-8fa9-9318d7c33e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5ab0d-6dc8-4928-be6e-63dc71c02d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5882f65-a8ae-4184-90dc-290ab131c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360c1fd-95c4-4299-8fff-4d4d75ccb9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load docs\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Store splits\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# RAG prompt\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=vectorstore.as_retriever(), chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19932a95-31d1-42bc-af29-303e25049ac2",
   "metadata": {},
   "source": [
    "## 95. LCEL (=LangChain Expression Language) - The Runnable Protocol\n",
    "* https://colab.research.google.com/drive/1iHmhKEhntUy71C_gO4kNqylZQYshAgD1\n",
    "* https://python.langchain.com/docs/concepts/lcel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2b8a9-7fe3-4293-86f0-adc962e6dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "print(\n",
    "    type(RunnableLambda(lambda x: x + 1))\n",
    ")  # <class 'langchain.schema.runnable.RunnableLambda'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1603e-8e72-4a4a-8f85-0818eda80d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableLambda(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1703f88-bdfd-458c-9950-96053d3cb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(1), chain.invoke(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5a1ac-7814-4c58-be22-e535fd101f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A RunnableSequence constructed using the `|` operator\n",
    "sequence = RunnableLambda(lambda x: x + 1) | (lambda x: x * 2)\n",
    "\n",
    "print(type(sequence))  # <class 'langchain.schema.runnable.RunnableSequence'>\n",
    "print(\"\\n\\n---\")\n",
    "print(sequence.invoke(1))  # 4\n",
    "sequence.batch([1, 2, 3])  # [4, 6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc536f93-1f86-49eb-95df-f7b61e47da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sequence that contains a RunnableParallel constructed using a dict literal\n",
    "sequence = RunnableLambda(lambda x: x + 1) | {\n",
    "    \"mul_2\": RunnableLambda(lambda x: x * 2),\n",
    "    \"mul_5\": RunnableLambda(lambda x: x * 5),\n",
    "}\n",
    "sequence.invoke(1)  # {'mul_2': 4, 'mul_5': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca29822-115d-4abd-b47f-ee317bec9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = (\n",
    "    RunnableLambda(lambda x: x + 1)\n",
    "    | {\n",
    "        \"mul_2\": RunnableLambda(lambda x: x * 2),\n",
    "        \"mul_5\": RunnableLambda(lambda x: x * 5),\n",
    "    }\n",
    "    | RunnableLambda(lambda x: x[\"mul_2\"] + x[\"mul_5\"])\n",
    ")\n",
    "sequence.invoke(1)  # {'mul_2': 4, 'mul_5': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e06f2-971f-4be3-b7d4-bdb4672cc3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    {\"mul_2\": RunnableLambda(lambda x: x * 2), \"mul_5\": RunnableLambda(lambda x: x * 5)}\n",
    ")\n",
    "\n",
    "# This is a dictionary, however it will be composed with other runnables when used in a sequence:\n",
    "parallel_two = {\n",
    "    \"mul_2\": RunnableLambda(lambda x: x[\"input_one\"] * 2),\n",
    "    \"mul_5\": RunnableLambda(lambda x: x[\"input_two\"] * 5),\n",
    "}\n",
    "\n",
    "print(type(parallel))  # <class 'langchain.schema.runnable.RunnableParallel'>\n",
    "print(type(parallel_two))  # <class 'dict'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d138d-0b57-4e54-88f5-bc7c4682c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = parallel | RunnableLambda(lambda x: x[\"mul_2\"] + x[\"mul_5\"])\n",
    "chain.invoke(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b03119-3cf8-4dad-bd18-7d5d8bef8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_chain = parallel_two | RunnableLambda(lambda x: x[\"mul_2\"] + x[\"mul_5\"])\n",
    "second_chain.invoke({\"input_one\": 5, \"input_two\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a444c0-59a3-49cb-a38f-b9924bd6d233",
   "metadata": {},
   "source": [
    "## 96. ChatModels, itemgetter and RAG\n",
    "* https://colab.research.google.com/drive/1PwupkZARnPadd4i1700WY6Y0u8eiiHxz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c95bec-5554-482b-988a-a064b73c1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    "    RunnableLambda,\n",
    ")\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe03aa-3b67-408d-8da1-4860fcb660f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnableParallel(origin=RunnablePassthrough(), modified=lambda x: x + 1)\n",
    "\n",
    "print(runnable.invoke(1))  # {'origin': 1, 'modified': 2}\n",
    "\n",
    "\n",
    "def fake_llm(prompt: str) -> str:  # Fake LLM for the example\n",
    "    return prompt + \" world\"\n",
    "\n",
    "\n",
    "chain = RunnableLambda(fake_llm) | {\n",
    "    \"original\": RunnablePassthrough(),  # Original LLM output\n",
    "    \"parsed\": lambda text: text[::-1],  # Parsing logic\n",
    "}\n",
    "\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d15bf-6cf2-4e7d-b0a9-392b83685592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# chat = ChatOpenAI()\n",
    "chat = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_FAST)\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | chat\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7339de-cec7-427f-88bd-7a9b8a5b3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first\", chain.first)\n",
    "print(\"last\", chain.last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44276d97-879b-4af8-9d73-6fb09a5ffcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream:\n",
    "print(\"\\n\\nStream:\\n\")\n",
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(s.content, end=\"\", flush=True)\n",
    "\n",
    "# Invoke:\n",
    "print(\"\\n\\nInvoke:\\n\")\n",
    "print(chain.invoke({\"topic\": \"bears\"}).content)\n",
    "\n",
    "# Batch:\n",
    "print(\"\\n\\nBatch:\\n\")\n",
    "res = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"redhats\"}, {\"topic\": \"monks\"}])\n",
    "print(res)\n",
    "print(\"\\n\".join((map(operator.attrgetter(\"content\"), res))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee95f25-f9ad-4767-8ac2-4effd15fa117",
   "metadata": {},
   "source": [
    "### RAG in LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a3aaa-92fa-45e0-adf7-64636e5dd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770e92d-29bf-40c5-b92c-4f7154d3a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\n",
    "        \"James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData\",\n",
    "        \"James Phoenix has an age of 31 years old.\",\n",
    "    ],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# model = ChatOpenAI()\n",
    "model = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_CLEVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b09cf62-4bd9-4149-95ee-d028f041baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# It's the same as this, but the tuple allows for line breaks:\n",
    "# {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbd029-77aa-4535-8904-081ea14985e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What company does James phoenix work at?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ca61d-8b29-4317-a37e-1d47a12def3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What is James Phoenix's age?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f930a-687a-4191-9093-803467eff922",
   "metadata": {},
   "source": [
    "### itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93d2fb-8f07-4d8a-a4b4-7dd343caa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\"data\": [\"This is a test\", \"Another entry...\"]}\n",
    "\n",
    "print(itemgetter(test))\n",
    "print(itemgetter(\"data\")(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb7dd3-c7fc-4904-8bdf-25db861996ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"What is the profession of James Phoenix? His profession is {profession}.\"\"\"\n",
    ")\n",
    "\n",
    "first_chain = RunnableParallel(name=lambda x: \"James Phoenix\", age=lambda x: 31)\n",
    "\n",
    "second_chain = {\n",
    "    # itemgetter is used to get the value from the dictionary from the previous step: (note this is only the previous step, not the whole chain)\n",
    "    \"name\": itemgetter(\"name\"),\n",
    "    \"age\": itemgetter(\"age\"),\n",
    "    # You can not use string values, either use itemgetter or a lambda, or RunnablePassthrough\n",
    "    \"profession\": lambda x: \"Data Engineer\",\n",
    "}\n",
    "\n",
    "chain = (\n",
    "    first_chain\n",
    "    | second_chain\n",
    "    | prompt\n",
    "    | ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_FAST)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69e5f3-e9be-4221-9a72-aa135c0d96c5",
   "metadata": {},
   "source": [
    "## 97. LCEL - Chat Message History and Memory\n",
    "* https://colab.research.google.com/drive/1dmDHw39-5NNiQtz3s_b8470lXTJYNx8I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c7f40-3c81-4ffe-916f-4d4e1c797a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066032b-bb1f-4068-92fc-b069249efd18",
   "metadata": {},
   "source": [
    "### conversational history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45915c3c-485d-4195-a423-514addf2dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\n",
    "        \"James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData\",\n",
    "        \"James is 31 years old.\",\n",
    "    ],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242968dc-f556-4c6f-985a-36d36d7c0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561aeb6-01fc-42f0-aece-69e83c1973d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "ANSWER_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171df715-eaed-4210-989e-c407d9d5e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs: list,\n",
    "    document_prompt: PromptTemplate = DEFAULT_DOCUMENT_PROMPT,\n",
    "    document_separator: str = \"\\n\\n\",\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf861e-ced7-4f21-9924-51342152855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "def _format_chat_history(\n",
    "    chat_history: List[Union[HumanMessage, SystemMessage, AIMessage]]\n",
    ") -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        if isinstance(dialogue_turn, HumanMessage):\n",
    "            buffer += \"\\nHuman: \" + dialogue_turn.content\n",
    "        elif isinstance(dialogue_turn, AIMessage):\n",
    "            buffer += \"\\nAssistant: \" + dialogue_turn.content\n",
    "        elif isinstance(dialogue_turn, SystemMessage):\n",
    "            buffer += \"\\nSystem: \" + dialogue_turn.content\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692659bb-8c75-4c5a-a8b1-1374f6acc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_CLEVER, temperature=0)\n",
    "_inputs = RunnableMap(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    # | ChatOpenAI(temperature=0)\n",
    "    | chat\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520b95f-548f-4f57-accd-44b19a66b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"where did James work?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd0582-c94f-437c-bde3-c4c2667bcc76",
   "metadata": {},
   "source": [
    "### memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8a734-cc8d-45fa-bb33-2b882564f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90140c74-aba5-4f40-95d0-69cd9688d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    # | ChatOpenAI(temperature=0)\n",
    "    | chat\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "\n",
    "# This is REALLY IMPORTANT as the chain above becomes StrOutputParser() so it will only have one key, which gets passed to the retriever!\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": (\n",
    "        final_inputs\n",
    "        | ANSWER_PROMPT\n",
    "        # | ChatOpenAI()\n",
    "        | chat\n",
    "    ),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = (\n",
    "    loaded_memory\n",
    "    | standalone_question\n",
    "    | retrieved_documents\n",
    "    | answer\n",
    "    # | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5417400-38fa-4e90-b1b3-44280b97cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"where did James Phoenix work?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8048c-fdf5-4f72-a912-95b430b6444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98699c61-cd94-4da7-a0df-5f9f350a570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aea857-a9f3-43e5-b5de-cacd06abaa6f",
   "metadata": {},
   "source": [
    "## 98. LCEL - Multiple Chains\n",
    "* https://colab.research.google.com/drive/1W6T-iQ835IEV4fPPiYykY29yCYL8ZIwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a378c0-4e4e-4081-aff9-af1900209ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"What city was {person} born in?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"What country is the city {city} in? Respond in {language}\"\n",
    ")\n",
    "\n",
    "# model = ChatOpenAI()\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd07f2-e207-4cf1-b015-a75e5246c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    prompt1 | ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_FAST) | StrOutputParser()\n",
    ").invoke({\"person\": \"Barack Obama\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55eae3-f666-4b65-a253-e9323c5fe61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(prompt1 | model | StrOutputParser()).invoke({\"person\": \"Barack Obama\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4b03b-d727-4695-ace2-67262ec8144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2.invoke({\"person\": \"Barack Obama\", \"language\": \"Spanish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d8956-c435-4f56-aed5-cbe1594f7ab4",
   "metadata": {},
   "source": [
    "## 99. LCEL - Conditional Logic, Branching and Merging\n",
    "* https://colab.research.google.com/drive/1f4rSRMDzzAGlSh2zR0oJGIqW2s8NfJeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf76cb2-fc25-45db-a454-db89f77e6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66079a-5323-4520-b5a2-7a58a4db2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = RunnableBranch(\n",
    "    (lambda x: x == \"hello\", lambda x: x),\n",
    "    (lambda x: isinstance(x, str), lambda x: x.upper()),\n",
    "    (lambda x: \"This is the default case, in case no above lambda functions match.\"),\n",
    ")\n",
    "\n",
    "print(branch.invoke(\"hello\"))  # \"hello\"\n",
    "print(branch.invoke(\"hell\"))\n",
    "print(branch.invoke(None))  # \"This is the default case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73296bf4-01f0-4875-af46-ec61daea07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = (\n",
    "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the pros or positive aspects of {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the cons or negative aspects of {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
    "            (\"system\", \"Generate a final response given the critique\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c3dee-c61d-43f2-a9ab-c3167854a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"scrum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94218513-d0c8-4020-b493-970544de4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import langchain\n",
    "from contextlib import contextmanager\n",
    "from operator import itemgetter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from typing import Any, Dict\n",
    "\n",
    "# 1. Configure logging to save output to a file\n",
    "# logging.basicConfig(\n",
    "#     filename=\"langchain_debug.log\",\n",
    "#     filemode=\"w\",\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "# )\n",
    "logger = logging.getLogger(\"langchain\")\n",
    "logger.setLevel(logging.INFO)  # Set the desired logging level (e.g., INFO, DEBUG)\n",
    "\n",
    "# 2. Create a handler to write to a file\n",
    "#    'w' mode overwrites the file each time, use 'a' to append\n",
    "handler = logging.FileHandler(\"langchain.log\", mode=\"w\")\n",
    "\n",
    "# 3. Create a formatter to define the log message's structure\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# 4. Add the handler to the logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# 5. Prevent logs from propagating to the root logger to avoid duplicates\n",
    "logger.propagate = False\n",
    "\n",
    "\n",
    "class MyLoggingCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"A callback handler that logs events to a given logger.\"\"\"\n",
    "\n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Log the start of a chain run.\"\"\"\n",
    "        self.logger.info(f\"Chain started with inputs: {inputs}\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs: Any) -> None:\n",
    "        \"\"\"Log the end of an LLM call.\"\"\"\n",
    "        # The actual response object structure may vary by model provider\n",
    "        self.logger.info(\n",
    "            f\"LLM generated response: {response.generations[0][0].text[:80]}...\"\n",
    "        )\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Instantiate your handler with your logger\n",
    "my_handler = MyLoggingCallbackHandler(logger=logger)\n",
    "\n",
    "print(\"--- Running chain with a custom callback handler ---\")\n",
    "response = chain.invoke(\n",
    "    {\"topic\": \"cats\"}, config={\"callbacks\": [my_handler]}  # Pass the handler here\n",
    ")\n",
    "print(\"\\n--- Final Response ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a99360-a32a-4479-b006-91a1e6bef34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba8ea53-65b9-4d2a-af0e-1475ef29ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = False\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.stdout import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "joke_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | {\"joke\": RunnablePassthrough(), \"topic\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "explain_joke = (\n",
    "    ChatPromptTemplate.from_template(\"Explain the joke: {joke}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "benefits_of_joke = (\n",
    "    ChatPromptTemplate.from_template(\"List the benefits of this joke: {joke}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are responsible for generating a small analysis of a joke. The topic will be: {topic}\",\n",
    "            ),\n",
    "            (\"ai\", \"{joke}. The benefits of this joke are: {benefits}\"),\n",
    "            (\"human\", \"The explanation of the joke is: {explanation}\"),\n",
    "            (\"human\", \"Generate a small analysis of the joke. Analysis: \"),\n",
    "        ]\n",
    "    )\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | joke_chain\n",
    "    | {\n",
    "        \"explanation\": explain_joke,\n",
    "        \"benefits\": benefits_of_joke,\n",
    "        \"joke\": itemgetter(\"joke\"),\n",
    "        \"topic\": itemgetter(\"topic\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")\n",
    "\n",
    "final_chain.invoke(\n",
    "    {\"topic\": \"bears\"},\n",
    "    # config={\"callbacks\": [StdOutCallbackHandler()]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9891a0-0c53-422b-9e53-8d8ad3f8a949",
   "metadata": {},
   "source": [
    "## 100. LCEL -- LangChain Vector Databases + indexing API\n",
    "* https://colab.research.google.com/drive/1a3WMSxKRkyyGzlWf13zGrtXW6QdqQNhC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43896c-a6b6-40cd-961d-28cce8b5edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76241628-0cc5-4b74-9beb-e80bd40f3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"\n",
    "Digital marketing encompasses a broad range of marketing activities that utilize digital channels to connect with customers. At its core, digital marketing aims to reach a targeted audience through various online and electronic means, including social media, email, search engines, and websites. Unlike traditional marketing methods, digital marketing offers unparalleled opportunities for businesses to engage with their audience in real-time, enabling personalized communication and immediate feedback. This real-time interaction not only enhances customer experience but also allows businesses to gather valuable data on consumer behaviors, preferences, and trends, facilitating more effective marketing strategies and campaigns.\n",
    "The rise of digital marketing can be attributed to the increasing reliance on the internet and digital devices by consumers. As more people spend time online, businesses have shifted their marketing efforts to where their audiences are. Digital marketing leverages this online presence, employing strategies such as search engine optimization (SEO), content marketing, pay-per-click (PPC) advertising, and social media marketing to improve visibility and attract potential customers. These strategies are designed to increase traffic to a company's online platforms, build brand awareness, and ultimately drive conversions and sales. The ability to measure the effectiveness of these strategies through analytics and metrics further underscores the advantage of digital marketing, enabling businesses to refine their approach and maximize return on investment (ROI).\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test.txt\", \"w\") as f:\n",
    "    f.write(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b29052-9918-469e-8627-12b6b857f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader(\"test.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf4a77-382f-4c29-9e6f-a536df92f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search_with_relevance_scores(\"digital marketing\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe87817-827b-4fde-bbe0-b2553740531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding on extra documents directly within LangChain:\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"James phoenix worked in digital marketing for 3 years.\",\n",
    "        metadata={\"source\": \"James Phoenix\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Digital marketing is a growing industry.\",\n",
    "        metadata={\"source\": \"Wikipedia\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84713048-e20e-4646-8eef-920c9d29862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989499a5-ee80-4f56-87a3-bef51d6af80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search(\"James\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0462da-7840-4f49-b299-b228e399204b",
   "metadata": {},
   "source": [
    "## 101. LCEL - Configurable Fields\n",
    "* https://colab.research.google.com/drive/1OQ-GNTMVsPhQwQs7NbxZ5RSETNst_6Pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af739dba-a2cf-4201-9d3d-175ed3f2a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM Temperature\",\n",
    "        description=\"The temperature of the LLM\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df005b-4644-4bd7-8164-d885d4c56740",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"pick a random number\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341aa63-462a-4f34-b741-6ece1e373aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\n",
    "    \"pick a random number\"\n",
    ").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6a992-2a2a-4813-90fe-3bcaff71f933",
   "metadata": {},
   "source": [
    "### configuring prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de488c58-e1d4-4ae5-aa7a-32b907ce8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(temperature=0)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a joke about {topic}\"\n",
    ").configurable_alternatives(\n",
    "    # This gives this field an id\n",
    "    # When configuring the end runnable, we can then use this id to configure this field\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    # This sets a default_key.\n",
    "    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used\n",
    "    default_key=\"joke\",\n",
    "    # This adds a new option, with name `poem`\n",
    "    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),\n",
    "    # You can add more configuration options here\n",
    ")\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ca9b1-e777-46bd-b7ee-8dc8257210df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default it will write a joke\n",
    "chain.invoke({\"topic\": \"bears\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bbdcf-d481-4ea4-afe1-ed8044da3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Markdown(\n",
    "    chain.with_config(configurable={\"prompt\": \"poem\"})\n",
    "    .invoke({\"topic\": \"bears\"})\n",
    "    .content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41da6d-d4ff-44ba-9a5c-9fa5caeb0843",
   "metadata": {},
   "source": [
    "### saving configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729359e-2dfd-4437-a134-ebf157a84724",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_joke = chain.with_config(configurable={\"llm\": \"openai\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30f349-01b1-4983-b903-91f8d3145fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_joke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e312683-e3e8-4a06-a48d-03a71b7b64a4",
   "metadata": {},
   "source": [
    "## 102. LCEL -- LangChain Agents & Tools\n",
    "* https://colab.research.google.com/drive/1PkT9FIEtrvbIDcS_QQE8jtYTNCaPCBzQ\n",
    "* https://python.langchain.com/docs/how_to/#agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed75f2-f615-4db5-a108-4ddf436aaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standard Tools\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=1000)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "print(tool.name)\n",
    "print(tool.description)\n",
    "print(tool.args)\n",
    "\n",
    "# We can see if the tool should return directly to the user\n",
    "print(\n",
    "    \"Will this automatically return the output to the user? This value is a boolean:\",\n",
    "    tool.return_direct,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d167766-3d0b-4008-af23-e31b15ddd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool.invoke(\"What is Digital Marketing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4786ce7-afdd-418b-ad85-67c8a2c19648",
   "metadata": {},
   "source": [
    "### tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e22d6-a88b-431a-bd7b-d077f53361a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2d231-796c-437d-87fd-7a6e118f99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Look up things online.\"\"\"\n",
    "    return \"LangChain\"\n",
    "\n",
    "\n",
    "print(search.name)\n",
    "print(search.description)\n",
    "print(search.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784f0d7-48f2-4c9f-9bbb-6cdca329a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "\n",
    "\n",
    "@tool(\"search-tool\", args_schema=SearchInput, return_direct=True)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Look up things online.\"\"\"\n",
    "    return \"LangChain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb9aa1-8e6e-4a1e-9f33-209d9f03a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_function(query: str):\n",
    "    return \"LangChain\"\n",
    "\n",
    "\n",
    "search = StructuredTool.from_function(\n",
    "    func=search_function,\n",
    "    name=\"Search\",\n",
    "    description=\"useful for when you need to answer questions about current events\",\n",
    "    # coroutine= ... <- you can specify an async method if desired as well\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb7d9e-f43a-40ac-a856-538ef395c7d8",
   "metadata": {},
   "source": [
    "### agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff785d9f-504b-4b55-ae6c-d08c56497239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "# 1. Create the tool:\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "\n",
    "# 2. Assign the tools to a Python list:\n",
    "tools = [get_word_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c8efa-64d7-4dad-984b-1428c9922ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the ChatPromptTemplate:\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but don't know current events\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b0c50-d224-49b5-9075-336fd5d3b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 4. Create the LLM and bind the tools directly to the LLM:\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "llm_with_tools = llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f2b67-96c1-4c71-88ff-5cb749440073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "# 5. Creating the LCEL agent chain:\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9781160-a8a9-43f6-bbc9-bf9d5b863ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb44d68-a4ba-4fb0-9c49-f93af4c1e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(agent_executor.stream({\"input\": \"How many letters in the word data\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e30d99-84c7-424e-b1e7-e5289d23564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"How many letters in the word data\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777cbfb-9b39-456a-aa95-d4b90c2981af",
   "metadata": {},
   "source": [
    "### adding memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33a08e-e01f-4ff3-8a08-3440051b1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837258c-7b8a-4ada-bde1-fd11fb62f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd984c00-bf33-44ab-ae35-875a980b4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e62e91-460d-421a-975d-83237b9a97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = \"how many letters in the word data?\"\n",
    "result = agent_executor.invoke({\"input\": input1, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=input1),\n",
    "        AIMessage(content=result[\"output\"]),\n",
    "    ]\n",
    ")\n",
    "agent_executor.invoke({\"input\": \"is that a real word?\", \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e98a7c-54d0-42c7-a4ce-a56543cedc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80531d12-e74f-4e4d-afe5-5b682c079e15",
   "metadata": {},
   "source": [
    "#### customizing memory with id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cfce95-19cd-460e-ac26-8bf5b6e40b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customising the memory by\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store:dict = {}\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"history\": lambda x: x.get(\"history\", []),\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556ad48-e4fd-4dc7-bdc8-faee7b39c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"input\": \"My name is James\", \"history\": []},\n",
    "    config={\"configurable\": {\"session_id\": \"some_session_id\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fe99e-2555-4a46-af31-52e26bc77c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"some_session_id\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1358b7f-dd43-446c-a2de-43e34e514f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"some_different_session_id\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa0e50-ac52-45ea-9ae5-3116fafb54ed",
   "metadata": {},
   "source": [
    "## 103. 2025-08-30 LangGraph Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90b85c-0e1c-48cf-8772-858eaa80230e",
   "metadata": {},
   "source": [
    "## 104. Simple LangGraph Flows\n",
    "* https://colab.research.google.com/drive/1bJ7p5TSFCv2lWAv-ZtFPOrRoOpj3Lw2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ae6dc6-a2a9-409f-918c-f7d0d6d0ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4958441a-71fe-42b2-ae2a-0394057eecee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x114ee2cc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-for-genai",
   "language": "python",
   "name": "conda-for-genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
