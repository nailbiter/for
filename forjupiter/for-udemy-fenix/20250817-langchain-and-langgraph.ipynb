{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f6f399-8c3b-4af3-ae8c-a1ac0a62a7df",
   "metadata": {},
   "source": [
    "* https://docs.google.com/document/d/1b7DlLtvrVxihcO65dsBaIjUU-a6aLjdQfNHSOM2PhUM/edit?tab=t.0#heading=h.v0phdvfalpr2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc866dac-b1b0-4ce4-9990-5dea8256244e",
   "metadata": {},
   "source": [
    "## setup and tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a239417-e284-42ed-92dc-ba8ead8b34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f99fa0-3230-4882-8265-952753150d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from openai import OpenAI\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96269410-51db-42e0-82ee-dd0dc0833e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing with LangChain effectively requires understanding its core components and applying best practices for efficiency, maintainability, and responsible AI usage. Here's a breakdown of key areas and recommended practices:\n",
      "\n",
      "**1. Understanding LangChain Components & Architecture:**\n",
      "\n",
      "*   **LLMs (Large Language Models):**  Familiarize yourself with different LLMs (OpenAI, Cohere, Hugging Face models, etc.) and their strengths and weaknesses. Choose the right model for your task (e.g., GPT-3.5 for cost-effectiveness, GPT-4 for advanced reasoning).\n",
      "*   **Prompts & Prompt Templates:**  Master prompt engineering. Understand how to craft effective prompts that guide the LLM's behavior. Use prompt templates for reusability and parameterization.\n",
      "*   **Chains:**  Learn how to chain LLMs and other components together. Chains enable complex workflows, from simple summarization to sophisticated question-answering systems.\n",
      "*   **Indexes:**  Utilize indexes (e.g., document loaders, retrievers, vectorstores) to efficiently manage and search through large datasets.  Understand the trade-offs of different indexing techniques.\n",
      "*   **Agents:**  Explore agents, which use LLMs to decide which actions to take (e.g., calling APIs, searching the web).  Understand their capabilities and limitations.\n",
      "*   **Memory:**  Implement memory mechanisms to maintain context across interactions.  Choose the appropriate memory type (e.g., conversation buffer, summary memory) for your application.\n",
      "*   **Callbacks:**  Use callbacks to monitor and debug your LangChain applications. They provide insights into the inner workings of chains, agents, and LLMs.\n",
      "*   **Document Loaders & Splitters:**  Learn how to load different document types and split them into manageable chunks for processing.\n",
      "*   **Vectorstores:** Understand how to store and query embeddings generated from your documents. Choose the appropriate vectorstore (e.g., ChromaDB, FAISS, Pinecone) based on your needs.\n",
      "\n",
      "**2. Prompt Engineering Best Practices:**\n",
      "\n",
      "*   **Clarity & Specificity:**  Be clear and specific in your instructions. Avoid ambiguity.\n",
      "*   **Role-Playing:**  Assign roles to the LLM to influence its behavior (e.g., \"You are a helpful assistant...\" or \"You are a legal expert...\").\n",
      "*   **Examples (Few-Shot Learning):**  Provide examples of input-output pairs to guide the LLM's response. This is especially useful when dealing with specific formats or tasks.\n",
      "*   **Step-by-Step Instructions:** Break down complex tasks into smaller, manageable steps.\n",
      "*   **Constraints & Formatting:**  Set constraints on the output (e.g., \"Summarize in 3 sentences,\" \"Return the answer in JSON format\").  Specify desired output formats.\n",
      "*   **Iterative Refinement:**  Prompt engineering is often an iterative process. Experiment with different prompts and refine them based on the LLM's responses.\n",
      "*   **Test Prompts Regularly:**  Use a set of test cases to evaluate your prompts and ensure consistency.\n",
      "\n",
      "**3. Chain & Agent Design Best Practices:**\n",
      "\n",
      "*   **Modularity:**  Design chains and agents with modularity in mind. Break down complex tasks into smaller, reusable components.\n",
      "*   **Error Handling:**  Implement robust error handling to gracefully handle unexpected situations (e.g., API failures, invalid inputs).\n",
      "*   **Input Validation:**  Validate user inputs to prevent issues and ensure data integrity.\n",
      "*   **Contextualization:**  Provide sufficient context to LLMs and agents to improve their accuracy and relevance.\n",
      "*   **Agent Selection:**  Carefully choose the right agent type for your task. Consider the agent's reasoning capabilities, tools, and limitations.\n",
      "*   **Tool Selection (Agents):**  Select and configure tools carefully for agents.  Ensure the tools are relevant and reliable.  Consider the potential for misuse and security implications.\n",
      "*   **Chain of Thought (CoT):**  Encourage the LLM to explain its reasoning process (e.g., \"Let's think step by step...\"). This can improve accuracy and transparency.\n",
      "\n",
      "**4. Indexing & Retrieval Best Practices:**\n",
      "\n",
      "*   **Chunking Strategy:** Choose an appropriate chunking strategy based on the document type and the desired retrieval granularity.  Consider semantic meaning when chunking.\n",
      "*   **Embedding Models:** Select an embedding model that is suitable for your data and the task (e.g., OpenAI's embeddings, Sentence Transformers).\n",
      "*   **Vectorstore Selection:** Choose a vectorstore that meets your scalability, performance, and cost requirements.\n",
      "*   **Metadata:**  Add relevant metadata to your documents to enable filtering and more precise retrieval.\n",
      "*   **Hybrid Search:**  Consider using a hybrid search approach that combines keyword search with semantic search for improved accuracy.\n",
      "*   **Query Optimization:**  Optimize your queries to the vectorstore to retrieve the most relevant results.\n",
      "*   **Evaluation:** Evaluate the performance of your retrieval system using metrics like precision, recall, and F1-score.\n",
      "\n",
      "**5. Memory Management Best Practices:**\n",
      "\n",
      "*   **Choose the Right Memory Type:**  Select the memory type that aligns with your application's requirements.  Consider the context window limits of your LLM.\n",
      "*   **Context Window Awareness:**  Be mindful of the LLM's context window size.  Summarize or truncate the conversation history if needed.\n",
      "*   **Memory Overflows:** Implement mechanisms to prevent memory overflows.\n",
      "*   **Serialization & Persistence:**  Consider serializing and persisting memory to disk or a database for long-running applications.\n",
      "\n",
      "**6. Code Quality & Maintainability:**\n",
      "\n",
      "*   **Use a Version Control System (Git):**  Track your code changes and collaborate effectively.\n",
      "*   **Write Clear & Concise Code:**  Follow coding style guidelines and use meaningful variable names.\n",
      "*   **Document Your Code:**  Add comments to explain the purpose of your code and the logic behind it.\n",
      "*   **Write Unit Tests:**  Create unit tests to verify the correctness of your functions and components.\n",
      "*   **Use a Linter & Formatter:**  Use a linter (e.g., `flake8`, `pylint`) and a formatter (e.g., `black`, `autopep8`) to maintain consistent code style.\n",
      "*   **Modularize Your Code:** Break down your code into reusable functions and classes.\n",
      "*   **Refactor Regularly:**  Refactor your code to improve its readability, maintainability, and performance.\n",
      "*   **Follow SOLID principles:** Apply object-oriented design principles for well-structured code.\n",
      "\n",
      "**7. Responsible AI Practices:**\n",
      "\n",
      "*   **Bias Detection & Mitigation:**  Be aware of potential biases in LLMs and your data.  Implement strategies to mitigate bias (e.g., data augmentation, prompt engineering).\n",
      "*   **Transparency & Explainability:**  Strive for transparency in your LLM-based systems.  Provide explanations for the LLM's decisions.\n",
      "*   **Human Oversight:**  Implement human oversight where appropriate, especially in high-stakes applications.\n",
      "*   **Data Privacy & Security:**  Protect user data and comply with privacy regulations.  Implement security measures to prevent unauthorized access.\n",
      "*   **Avoid Hallucinations:**  Be aware of the risk of LLMs generating false or misleading information (hallucinations).  Implement techniques to reduce hallucinations (e.g., grounding in reliable sources, confidence scores).\n",
      "*   **Content Moderation:**  Implement content moderation mechanisms to prevent the generation of harmful or offensive content.\n",
      "*   **Consider the Impact:**  Think about the potential societal impact of your application and address any ethical concerns.\n",
      "*   **Documentation:** Document your system's capabilities, limitations, and intended use cases.\n",
      "\n",
      "**8. Debugging & Monitoring:**\n",
      "\n",
      "*   **Use Callbacks:**  Leverage LangChain's callback system to monitor the execution of chains and agents.\n",
      "*   **Logging:**  Implement comprehensive logging to track events, errors, and performance metrics.\n",
      "*   **Error Reporting:**  Implement error reporting mechanisms to capture and analyze errors.\n",
      "*   **Performance Monitoring:**  Monitor the performance of your LLM-based applications (e.g., latency, throughput, cost).\n",
      "*   **Experimentation:**  Experiment with different configurations and evaluate their impact on performance and accuracy.\n",
      "\n",
      "**9. Deployment & Scaling:**\n",
      "\n",
      "*   **Infrastructure:** Choose the appropriate infrastructure for deploying your application (e.g., cloud platforms, containerization).\n",
      "*   **Scalability:** Design your application to scale to handle increasing workloads.  Consider techniques like load balancing and caching.\n",
      "*   **Cost Optimization:**  Optimize your application for cost-effectiveness.  Monitor your usage of LLMs and other resources.\n",
      "*   **API Management:**  Use an API gateway to manage access to your application and provide security.\n",
      "*   **CI/CD:**  Implement continuous integration and continuous deployment (CI/CD) to automate the deployment process.\n",
      "\n",
      "**10. Staying Up-to-Date:**\n",
      "\n",
      "*   **Follow LangChain Updates:**  Keep up-to-date with the latest LangChain releases, features, and bug fixes.\n",
      "*   **Read Documentation:**  Consult the official LangChain documentation regularly.\n",
      "*   **Explore Examples & Tutorials:**  Learn from LangChain's examples and tutorials.\n",
      "*   **Join the Community:**  Participate in the LangChain community (e.g., forums, Discord) to learn from others and share your experiences.\n",
      "*   **Experiment and Learn:**  Continually experiment with new techniques and technologies to improve your skills.\n",
      "\n",
      "By following these best practices, you can develop robust, maintainable, and responsible LangChain applications that effectively leverage the power of LLMs. Remember that the field of LLMs is rapidly evolving, so continuous learning and adaptation are crucial.\n",
      "CPU times: user 28.8 ms, sys: 17.9 ms, total: 46.7 ms\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "response = llm.invoke(\"What are the best practices for developing with LangChain?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a078a1-9f34-4f21-a57b-3435bd23bf9f",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "# chat = ChatOpenAI(openai_api_key=\"...\")\n",
    "# If you have an envionrment variable set for OPENAI_API_KEY, you can just do:\n",
    "chat = ChatOpenAI()\n",
    "chat.invoke(\"Hello, how are you?\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654b4fcb-f826-4012-96b9-a35fa19c5c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well, thank you for asking! As a large language model, I don't experience emotions like humans do, but I am functioning properly and ready to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable.\n",
    "# You can get one from Google AI Studio: https://aistudio.google.com/app/apikey\n",
    "\n",
    "# Initialize the Gemini chat model\n",
    "# You can also specify other models like \"gemini-1.5-pro\"\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "# Invoke the model with a prompt\n",
    "response = chat.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0333dc9-a2a4-4b45-9bf1-02348ff467b3",
   "metadata": {},
   "source": [
    "## 85. Chat Models -- Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95c4b12e-cda6-4142-ac41-f61911adfd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of France is **Paris**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = chat.invoke(\"What is the capital of France?\")\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abb264da-cacc-4fa9-87c0-b7a305281b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
       " 'finish_reason': 'STOP',\n",
       " 'model_name': 'gemini-2.0-flash-lite',\n",
       " 'safety_ratings': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0f7d4-5d29-45c2-bcd3-385cfcfccc53",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "result = chat.invoke(messages)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5784b22-dce4-4992-9562-cd3af0fa1f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some company name ideas for a colorful sock company, playing on different angles:\n",
      "\n",
      "**Emphasizing Color & Fun:**\n",
      "\n",
      "*   Chromatic Threads\n",
      "*   Rainbow Footwear\n",
      "*   Socktacular Colors\n",
      "*   Kaleidoscope Socks\n",
      "*   Vivid Soles\n",
      "*   The Color Sock Shop\n",
      "*   Happy Feet Hues\n",
      "*   Sock Pop\n",
      "*   Technicolor Toes\n",
      "\n",
      "**Highlighting Quality & Comfort:**\n",
      "\n",
      "*   Cozy Canvas Socks\n",
      "*   Comfort & Hue\n",
      "*   The Soft Sole\n",
      "*   Thread & Thrive\n",
      "*   Artisan Socks\n",
      "*   Stride in Style\n",
      "*   Foot Haven\n",
      "\n",
      "**Playful & Catchy:**\n",
      "\n",
      "*   Sock It To 'Em\n",
      "*   Sole Mates\n",
      "*   Toe-riffic Socks\n",
      "*   Sock Monster\n",
      "*   The Sock Drawer\n",
      "*   Get Your Sock On\n",
      "*   Soleful Creations\n",
      "\n",
      "**Modern & Minimalist:**\n",
      "\n",
      "*   Hue & Heel\n",
      "*   The Sock Lab\n",
      "*   Bold Threads\n",
      "*   Stride Studio\n",
      "\n",
      "**Tips for Choosing:**\n",
      "\n",
      "*   **Check availability:** Make sure the name isn't already in use and that the domain name is available.\n",
      "*   **Consider your target audience:** Who are you trying to sell to? (e.g., kids, adults, fashion-conscious individuals)\n",
      "*   **Keep it memorable:** Easy to say and spell.\n",
      "*   **Reflect your brand:** Does the name convey the feeling you want to evoke?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Ensure your GOOGLE_API_KEY environment variable is set\n",
    "# 1. Initialize the Gemini chat model\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "# 2. Define the message(s) for the model\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that generates company names.\"),\n",
    "    HumanMessage(content=text),\n",
    "]\n",
    "\n",
    "# 3. Invoke the model with the messages\n",
    "result = chat.invoke(messages)\n",
    "\n",
    "# 4. Print the AI's response content\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a4456-7f1f-4747-839e-943486e3425c",
   "metadata": {},
   "source": [
    "## 86. Chat Prompt Templates\n",
    "* https://drive.google.com/file/d/1JoyxZlYfngmXnvrRyo7qqvUoB7qz6il0/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1f6a8d9-78c2-4604-b3e0-cbb9a603efad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to ChatPromptTemplate is missing variables {'text'}.  Expected: ['text'] Received: []\\nNote: if you intended {text} to be part of the string and not a variable, please escape it with double curly braces like: '{{text}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m      3\u001b[0m chat_prompt_template \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m      4\u001b[0m     [\n\u001b[1;32m      5\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that generates company names\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m     ]\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m result \u001b[38;5;241m=\u001b[39m chat_prompt_template\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     11\u001b[0m     {\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# \"text\": \"What would be a good company name for a company that makes colorful socks?\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# model = Cha(model='gpt-4o-mini')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ai_llm_result \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39minvoke(result)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langchain_core/prompts/base.py:216\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[1;32m    215\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_prompt_with_error_handling,\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    219\u001b[0m     config,\n\u001b[1;32m    220\u001b[0m     run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m     serialized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m    222\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langchain_core/runnables/base.py:1938\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m   1936\u001b[0m         output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1937\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 1938\u001b[0m             context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m   1939\u001b[0m                 call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m                 func,\n\u001b[1;32m   1941\u001b[0m                 input_,\n\u001b[1;32m   1942\u001b[0m                 config,\n\u001b[1;32m   1943\u001b[0m                 run_manager,\n\u001b[1;32m   1944\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1945\u001b[0m             ),\n\u001b[1;32m   1946\u001b[0m         )\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1948\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langchain_core/runnables/config.py:429\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    428\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langchain_core/prompts/base.py:189\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m--> 189\u001b[0m     inner_input_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_input_)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/for-genai/lib/python3.12/site-packages/langchain_core/prompts/base.py:183\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    177\u001b[0m     example_key \u001b[38;5;241m=\u001b[39m missing\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    178\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m to be part of the string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    184\u001b[0m         create_message(message\u001b[38;5;241m=\u001b[39mmsg, error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_PROMPT_INPUT)\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Input to ChatPromptTemplate is missing variables {'text'}.  Expected: ['text'] Received: []\\nNote: if you intended {text} to be part of the string and not a variable, please escape it with double curly braces like: '{{text}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that generates company names\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat_prompt_template.invoke(\n",
    "    {\n",
    "        # \"text\": \"What would be a good company name for a company that makes colorful socks?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# model = Cha(model='gpt-4o-mini')\n",
    "\n",
    "ai_llm_result = chat.invoke(result)\n",
    "print(ai_llm_result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-for-genai",
   "language": "python",
   "name": "conda-for-genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
