{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f6f399-8c3b-4af3-ae8c-a1ac0a62a7df",
   "metadata": {},
   "source": [
    "* https://docs.google.com/document/d/1b7DlLtvrVxihcO65dsBaIjUU-a6aLjdQfNHSOM2PhUM/edit?tab=t.0#heading=h.v0phdvfalpr2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc866dac-b1b0-4ce4-9990-5dea8256244e",
   "metadata": {},
   "source": [
    "## setup and tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a239417-e284-42ed-92dc-ba8ead8b34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25481e72-a46a-47ef-a0be-a39f9fabf387",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install langchain-google-genai langchain langchain_openai beautifulsoup4 langchain-community lxml --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08a31554-bf98-4134-823c-1f7421bbf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from openai import OpenAI\n",
    "import jupyter_black\n",
    "import tqdm, tqdm.notebook\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "assert {\"OPENAI_API_KEY\", \"GOOGLE_API_KEY\"} <= set(os.environ)\n",
    "\n",
    "GEMINI_MODEL_NAME_CLEVER = \"gemini-2.0-flash\"\n",
    "GEMINI_MODEL_NAME_FAST = \"gemini-2.0-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96269410-51db-42e0-82ee-dd0dc0833e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing with LangChain effectively involves understanding its core components, best practices, and common pitfalls to avoid. Here's a breakdown of best practices, categorized for clarity:\n",
      "\n",
      "**I. Foundational Principles & Design:**\n",
      "\n",
      "*   **Understand LangChain's Core Components:**\n",
      "    *   **Models:** LLMs (e.g., OpenAI, Cohere), Chat Models (e.g., ChatOpenAI, ChatAnthropic), Embeddings models (e.g., OpenAIEmbeddings).\n",
      "    *   **Prompts:** Prompt templates, examples, variables, and how to structure them for optimal performance.\n",
      "    *   **Chains:** Sequences of calls to LLMs, other utilities, or even other chains.  Understand different chain types (e.g., Sequential, Router, MapReduce).\n",
      "    *   **Agents:**  Use LLMs as reasoning engines to select tools and decide actions.\n",
      "    *   **Memory:**  Manage conversations and context over time. Types include:  `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationSummaryMemory`.\n",
      "    *   **Indexes (Document Loaders, Retrievers):** Load, transform, and store documents for retrieval.\n",
      "    *   **Tools:** Integrate external functionalities (e.g., web search, calculation).\n",
      "    *   **Callbacks:**  Monitor and log events during chain execution.\n",
      "*   **Modular Design:** Break down complex tasks into smaller, manageable chains and agents. This improves readability, maintainability, and testability.\n",
      "*   **Abstraction and Reusability:**  Create reusable prompt templates, chains, and tools.  Avoid hardcoding whenever possible.  Use functions and classes to encapsulate logic.\n",
      "*   **Error Handling:** Implement robust error handling at every stage.  Catch exceptions related to API calls, data processing, and LLM responses. Provide informative error messages for debugging.\n",
      "*   **Configuration Management:** Use environment variables or configuration files to manage sensitive information (API keys), model parameters, and other settings.  Don't hardcode credentials.\n",
      "*   **Version Control:**  Use Git and a version control system to track changes, collaborate, and revert to previous states if needed.\n",
      "\n",
      "**II. Prompt Engineering:**\n",
      "\n",
      "*   **Clear and Concise Prompts:** Write instructions in a clear, unambiguous manner.  Be specific about the desired output format.\n",
      "*   **Few-Shot Learning (Examples):** Include a few examples (input/output pairs) within your prompt to guide the LLM towards the desired behavior.  This is often more effective than lengthy instructions.\n",
      "*   **Prompt Templates:** Utilize LangChain's `PromptTemplate` to create reusable and parameterized prompts.\n",
      "*   **Iterative Refinement:** Experiment with different prompts and evaluate their performance.  Use A/B testing or other evaluation techniques to identify the most effective prompts for your use case.\n",
      "*   **Output Formatting:**  Specify the desired output format (e.g., JSON, XML, specific markdown) to make parsing easier. Use `OutputParsers` to handle the output.\n",
      "*   **Temperature Tuning:** Adjust the `temperature` parameter of the LLM to control the randomness of the output.  Lower temperatures (e.g., 0.0) produce more deterministic results, while higher temperatures (e.g., 0.7-1.0) encourage more creative or varied responses.\n",
      "*   **Contextualization:**  Provide relevant context to the LLM to help it generate more accurate and relevant responses. This often involves using document retrieval or incorporating external knowledge.\n",
      "*   **Chain-of-Thought Prompting:** Encourage the LLM to explain its reasoning step-by-step. This can improve the accuracy and reliability of complex tasks.\n",
      "\n",
      "**III. Chain and Agent Design:**\n",
      "\n",
      "*   **Chain Selection:** Choose the appropriate chain type for your task.  Consider:\n",
      "    *   **Sequential Chains:** For executing a series of steps in order.\n",
      "    *   **Router Chains:** For branching execution based on user input or conditions.\n",
      "    *   **MapReduce Chains:** For processing large amounts of data.\n",
      "    *   **Conversational Chains:** Designed for handling conversational history.\n",
      "*   **Agent Selection:**  Use agents when you need the LLM to dynamically select tools or take actions based on its understanding of the situation.\n",
      "*   **Tool Selection:**  Choose the right tools for the agent.  Consider:\n",
      "    *   **Web Search:**  For accessing up-to-date information.\n",
      "    *   **Calculators:** For performing mathematical operations.\n",
      "    *   **Custom Tools:** For integrating with your specific systems or APIs.\n",
      "*   **Agent Configuration:** Carefully configure the agent's tools, memory, and prompt.  Choose the correct agent type (e.g., `AgentType.ZERO_SHOT_REACT_DESCRIPTION`, `AgentType.OPENAI_FUNCTIONS`).\n",
      "*   **Memory Management:**  Choose the appropriate memory type for your application. Consider:\n",
      "    *   **`ConversationBufferMemory`:** Stores the entire conversation history.\n",
      "    *   **`ConversationBufferWindowMemory`:** Stores a limited number of recent interactions.\n",
      "    *   **`ConversationSummaryMemory`:** Summarizes the conversation history to keep the context concise.\n",
      "    *   **`ConversationEntityMemory`:**  Stores and retrieves entities mentioned in the conversation.\n",
      "*   **Chain Callbacks:** Use callbacks to monitor chain execution, log events, and track performance metrics.  You can create custom callbacks to suit your needs.\n",
      "*   **Chain Parallelization:** Utilize async functions and threading (or multiprocessing) to speed up chain execution, especially when making multiple API calls.\n",
      "\n",
      "**IV. Document Loading and Indexing:**\n",
      "\n",
      "*   **Document Loaders:** Use appropriate document loaders to load documents from various sources (e.g., PDF, CSV, websites).\n",
      "*   **Text Splitting:** Split long documents into smaller chunks to improve retrieval performance and reduce token usage. Experiment with different chunking strategies (e.g., recursive character text splitter, token-based splitter).\n",
      "*   **Embeddings:** Generate embeddings for your document chunks to enable semantic search.  Choose an embedding model suitable for your task.\n",
      "*   **Vectorstores:**  Choose a vectorstore to store and index your embeddings (e.g., ChromaDB, FAISS, Pinecone, Weaviate).\n",
      "*   **Retrieval Strategies:**  Experiment with different retrieval strategies to optimize the retrieval of relevant documents. Consider:\n",
      "    *   **Similarity Search:**  Retrieve documents based on their semantic similarity to the query.\n",
      "    *   **MMR (Maximal Marginal Relevance):** Retrieve documents that are both relevant and diverse.\n",
      "    *   **HyDE (Hypothetical Document Embeddings):** Generate a hypothetical document based on the query and use its embedding to retrieve relevant documents.\n",
      "    *   **Contextual Compression:**  Compress retrieved documents to focus on the most relevant parts.\n",
      "\n",
      "**V. Evaluation and Testing:**\n",
      "\n",
      "*   **Unit Testing:**  Write unit tests to verify the functionality of individual components (e.g., prompt templates, chains, tools).\n",
      "*   **Integration Testing:**  Test the interaction between different components to ensure they work together correctly.\n",
      "*   **End-to-End Testing:**  Test the entire system from input to output to ensure it meets the desired requirements.\n",
      "*   **Evaluation Metrics:**  Define evaluation metrics to assess the performance of your LangChain applications. Consider:\n",
      "    *   **Accuracy:** How often the system provides the correct answer.\n",
      "    *   **Relevance:** How relevant the output is to the input.\n",
      "    *   **Coherence:** How logically consistent the output is.\n",
      "    *   **Fluency:** How natural and grammatically correct the output is.\n",
      "    *   **Latency:** The time it takes to generate a response.\n",
      "    *   **Cost:** The cost of running the application.\n",
      "*   **Prompt Evaluation Frameworks:** Use frameworks like `LangChainHub` or custom evaluation scripts to systematically evaluate the performance of different prompts and configurations.\n",
      "\n",
      "**VI.  Deployment and Monitoring:**\n",
      "\n",
      "*   **Scalability:** Design your application to scale to handle increased traffic.  Consider using asynchronous operations, caching, and load balancing.\n",
      "*   **Monitoring:**  Monitor the performance of your application in production.  Track metrics such as latency, error rates, and API usage.\n",
      "*   **Logging:** Implement comprehensive logging to capture events, errors, and debugging information.\n",
      "*   **Rate Limiting:** Implement rate limiting to protect your application from overuse and to manage API costs.\n",
      "*   **Security:** Secure your application by protecting sensitive information (API keys) and implementing appropriate authentication and authorization mechanisms.\n",
      "*   **Cost Optimization:**  Monitor and optimize the costs associated with using LLMs and other cloud services.  Consider:\n",
      "    *   **Prompt Optimization:**  Reduce the number of tokens used in prompts.\n",
      "    *   **Model Selection:**  Choose the most cost-effective LLM for your needs.\n",
      "    *   **Caching:**  Cache frequently used results to reduce API calls.\n",
      "    *   **Token Counting:**  Use tools to estimate and monitor token usage.\n",
      "\n",
      "**VII.  Common Pitfalls to Avoid:**\n",
      "\n",
      "*   **Over-reliance on Complex Chains:** Start with simpler chains and add complexity gradually.  Avoid creating overly complex chains that are difficult to understand and maintain.\n",
      "*   **Ignoring Token Limits:**  Be mindful of token limits, especially when using LLMs with limited context windows. Use text splitting and summarization techniques to manage context.\n",
      "*   **Lack of Prompt Engineering:**  Don't underestimate the importance of prompt engineering.  Spend time experimenting with different prompts to optimize performance.\n",
      "*   **Ignoring Error Handling:**  Don't assume that API calls will always succeed.  Implement robust error handling to gracefully handle unexpected situations.\n",
      "*   **Not Testing Thoroughly:**  Test your application thoroughly to ensure that it meets the desired requirements and performs reliably.\n",
      "*   **Security Vulnerabilities:**  Avoid hardcoding sensitive information.  Implement proper authentication and authorization.  Sanitize user input to prevent injection attacks.\n",
      "*   **Over-reliance on LLMs for Tasks LLMs aren't good at:** LLMs are good at text generation, summarization, and some reasoning tasks, but not good at things like precise calculations (use a calculator tool).\n",
      "\n",
      "By following these best practices, you can develop more robust, reliable, and effective LangChain applications.  Remember to iterate, experiment, and continuously evaluate your results to improve performance and achieve your desired outcomes. Remember to check the LangChain documentation and examples often, as the framework evolves rapidly.\n",
      "CPU times: user 1.49 s, sys: 352 ms, total: 1.85 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "response = llm.invoke(\"What are the best practices for developing with LangChain?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a078a1-9f34-4f21-a57b-3435bd23bf9f",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "# chat = ChatOpenAI(openai_api_key=\"...\")\n",
    "# If you have an envionrment variable set for OPENAI_API_KEY, you can just do:\n",
    "chat = ChatOpenAI()\n",
    "chat.invoke(\"Hello, how are you?\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654b4fcb-f826-4012-96b9-a35fa19c5c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well, thank you for asking! As a large language model, I don't experience emotions like humans do, but I am functioning properly and ready to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable.\n",
    "# You can get one from Google AI Studio: https://aistudio.google.com/app/apikey\n",
    "\n",
    "# Initialize the Gemini chat model\n",
    "# You can also specify other models like \"gemini-1.5-pro\"\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "# Invoke the model with a prompt\n",
    "response = chat.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0333dc9-a2a4-4b45-9bf1-02348ff467b3",
   "metadata": {},
   "source": [
    "## 85. Chat Models -- Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4b12e-cda6-4142-ac41-f61911adfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = chat.invoke(\"What is the capital of France?\")\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb264da-cacc-4fa9-87c0-b7a305281b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0f7d4-5d29-45c2-bcd3-385cfcfccc53",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "result = chat.invoke(messages)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5784b22-dce4-4992-9562-cd3af0fa1f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Ensure your GOOGLE_API_KEY environment variable is set\n",
    "# 1. Initialize the Gemini chat model\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "# 2. Define the message(s) for the model\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that generates company names.\"),\n",
    "    HumanMessage(content=text),\n",
    "]\n",
    "\n",
    "# 3. Invoke the model with the messages\n",
    "result = chat.invoke(messages)\n",
    "\n",
    "# 4. Print the AI's response content\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a4456-7f1f-4747-839e-943486e3425c",
   "metadata": {},
   "source": [
    "## 86. Chat Prompt Templates\n",
    "* https://drive.google.com/file/d/1JoyxZlYfngmXnvrRyo7qqvUoB7qz6il0/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6a8d9-78c2-4604-b3e0-cbb9a603efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that generates company names\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat_prompt_template.invoke(\n",
    "    {\n",
    "        \"text\": \"What would be a good company name for a company that makes colorful socks?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# model = Cha(model='gpt-4o-mini')\n",
    "\n",
    "ai_llm_result = chat.invoke(result)\n",
    "print(ai_llm_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561f15e-9717-4d09-9e29-8ba7d53fbc3a",
   "metadata": {},
   "source": [
    "## 87. Streaming\n",
    "* https://drive.google.com/file/d/18sGlOZ8AKwON1CXUMnqf9ONfj7bwjSiO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db2846-80d8-4ef7-a5ad-b225aba91066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm, tqdm.notebook\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    # streaming=True,\n",
    ")\n",
    "for chunk in tqdm.notebook.tqdm(chat.stream(\"What is the capital of the moon?\")):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb60bd3-6ed4-46de-b0bb-af66c3962836",
   "metadata": {},
   "source": [
    "## 88. Output Parsers\n",
    "* https://drive.google.com/file/d/1QWwi3AOCHEoMR83zR21sB7zzKdUxVdfO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7202cb-737a-4396-998f-36a914b55ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddb083-21d6-4463-baf5-9de828dad2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7732fa1-8542-4911-aa58-18613e3b8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6c1e9-e7b7-4a97-b4f5-94318c5358b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "\n",
    "messages = chat_prompt.invoke(\n",
    "    {\n",
    "        \"query\": \"What is a really funny joke about Python programming?\",\n",
    "        \"format_instructions\": parser.get_format_instructions(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8f8a7-7e75-4dab-974c-7845e12c1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()\n",
    "## does not work with Gemini\n",
    "result = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af04d1-8046-43d5-a1b7-6da3f7c30306",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    joke_object = parser.parse(result.content)\n",
    "    print(joke_object.setup)\n",
    "    print(joke_object.punchline)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c358e2-007f-429a-8297-c71275ed72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7afe0-a0b0-4d5b-9233-6556838c9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f91080-cfa2-49a3-b42c-40ce6b224b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    explanation: str = Field(\n",
    "        description=\"A detailed explanation of why this joke is funny.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3a9b0-6199-410e-9c69-9cc95f51c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275de99-b846-4cbd-88da-9f4bcfa2bf2e",
   "metadata": {},
   "source": [
    "## 89. Summarizing large amounts of text\n",
    "* https://colab.research.google.com/drive/11t0e03SThhKRPq9T1M7xg6BcooBFaTkA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775946d-4c56-414d-a533-914ba58de93a",
   "metadata": {},
   "source": [
    "### crisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d808c-3f91-4376-b8a9-4c0eb30c4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    temperature=0,\n",
    "    model=GEMINI_MODEL_NAME_CLEVER,\n",
    ")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "res = chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56313b30-4169-49ec-9d0c-e484fc5122bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"input_documents\"]\n",
    "Markdown(res[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224b2aa-5ecc-456a-9851-b141d0a820aa",
   "metadata": {},
   "source": [
    "### map reduce\n",
    "\n",
    "* problem if pages refer to each other (since summaries are done independently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f7fa2-83c6-4dcb-99c0-b9e6cab914cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import (\n",
    "    ReduceDocumentsChain,\n",
    "    MapReduceDocumentsChain,\n",
    "    StuffDocumentsChain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f491b-eed6-4373-ae2f-9a4447342be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(temperature=0, model=GEMINI_MODEL_NAME_CLEVER)\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# map_chain:\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes.\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26d252-e08a-4b56-879f-33d170308e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccadc0-b9f5-4623-ab75-d49b33a39531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ddcd9-71e2-4653-805e-6624093556e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print()\n",
    "res = map_reduce_chain.invoke(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1fa67-aced-4345-aedb-192336c7270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(res[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db3570-931e-47fc-ae4a-4258cc741b9f",
   "metadata": {},
   "source": [
    "### template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e109f63e-e3a5-435b-8484-8c14782636c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWrite a concise summary of the following:\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;132;01m{text}\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mCONCISE SUMMARY:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(prompt_template)\n\u001b[1;32m      6\u001b[0m refine_template \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour job is to produce a final summary\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe have provided an existing summary up to a certain point: \u001b[39m\u001b[38;5;132;01m{existing_answer}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the context isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt useful, return the original summary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m refine_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(refine_template)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)\n",
    "\n",
    "# Page 1 --> Page 2 (Refine) --> Page 3 (Refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b77696-979a-473c-89a2-7c66a06ea248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(result[\"intermediate_steps\"]):\n",
    "    display(Markdown(f\"## step {i+1}\\n{v}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ee122-daab-4695-9547-55f1b7ed317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65598b-c2ad-422c-9ebb-407e0e372a3e",
   "metadata": {},
   "source": [
    "## 91. Document Loaders, Text Splitting, Creating LangChain Documents\n",
    "\n",
    "https://colab.research.google.com/drive/1YdtBCggWStErmFeP5GBSmEaw04kXeKqD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f681afb-0def-485e-b64e-acba6c7c4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import requests\n",
    "\n",
    "# Get this file and save it locally:\n",
    "url = \"https://github.com/hammer-mt/thumb/blob/master/README.md\"\n",
    "\n",
    "# Save it locally:\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the text from the HTML:\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "loader = TextLoader(\"README.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaab45fc-b5c7-4fc8-821c-77db1e7079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac22e81-df9a-4f65-b986-2d604671b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'test': 'test'}, page_content='test')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "[Document(page_content=\"test\", metadata={\"test\": \"test\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa692e28-061f-4d92-8250-ca8abd7f0c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the text into sentences:\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "final_docs = text_splitter.split_documents(loader.load())\n",
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f6c560f-0533-43a2-aeb8-647960c0cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d20a453-590b-43a9-b71a-306a02cdee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.4 ms, sys: 50.5 ms, total: 120 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chain = load_summarize_chain(llm=chat, chain_type=\"map_reduce\")\n",
    "res = chain.invoke(\n",
    "    {\n",
    "        \"input_documents\": final_docs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3853c392-b6bd-4a21-ab70-7babac04a0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This describes a comprehensive GitHub interface, outlining features for code management, collaboration, security, AI-powered tools (Copilot), automation (Actions), and learning. It includes project-specific views, enterprise-grade security and support options, and links to resources, documentation, and community features. The interface encompasses navigation menus, error messages, and user settings, including a feedback form, saved search management, and appearance customization.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()\n",
    "res[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee58ead-f5ea-4b49-a05c-580744658705",
   "metadata": {},
   "source": [
    "## 92. Tagging Documents\n",
    "https://colab.research.google.com/drive/1Gn1IxMqz0RcOaDKVdY7cnzgik0JoQlqZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50718f47-4d27-4e63-8211-57501d8091e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixes a bug with asyncio and jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719cbb2c-9f7f-4efe-9db6-36f8a2cb5362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c493471d-d23f-4bb2-b8c7-7a30dbcf8572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|############################################################################################################################################################| 103/103 [00:03<00:00, 33.36it/s]\n"
     ]
    }
   ],
   "source": [
    "sitemap_loader = SitemapLoader(web_path=\"https://understandingdata.com/sitemap.xml\")\n",
    "sitemap_loader.requests_per_second = 5\n",
    "docs = sitemap_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aedb080f-8883-4469-bddc-e1981a440684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\"type\": \"string\"},\n",
    "        \"aggressiveness\": {\"type\": \"integer\"},\n",
    "        \"primary_topic\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The main topic of the document.\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"primary_topic\", \"sentiment\", \"aggressiveness\"],\n",
    "}\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "## does not work with Gemini\n",
    "# llm = ChatGoogleGenerativeAI(temperature=0, model=GEMINI_MODEL_NAME_CLEVER)\n",
    "chain = create_tagging_chain(schema, llm, output_key=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69ee82b9-9eb4-4a27-8f73-8fda88b2b04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing doc 1\n",
      "Processing doc 2\n",
      "Processing doc 3\n",
      "Processing doc 4\n",
      "Processing doc 5\n",
      "Processing doc 6\n",
      "Processing doc 7\n",
      "Processing doc 8\n",
      "Processing doc 9\n",
      "Processing doc 10\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Remove the 0:10 to run on all documents:\n",
    "for index, doc in enumerate(docs[0:10]):\n",
    "    print(f\"Processing doc {index +1}\")\n",
    "    chain_result = chain.invoke({\"input\": doc.page_content})\n",
    "    results.append(chain_result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75147b4b-ab4b-4298-85e9-548c495e045a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>aggressiveness</th>\n",
       "      <th>primary_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>AI products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>3</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Software &amp; Data Engineering Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Software &amp; Data Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Engineering Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>3</td>\n",
       "      <td>React Software Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Python software development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Python software development</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment  aggressiveness                         primary_topic\n",
       "0  positive               0                           AI products\n",
       "1  positive               3                            technology\n",
       "2  positive               0                               Contact\n",
       "3  positive               2  Software & Data Engineering Services\n",
       "4  positive               0           Software & Data Engineering\n",
       "5   neutral               0                      Data Engineering\n",
       "6  positive               0             Data Engineering Services\n",
       "7  positive               3            React Software Development\n",
       "8  positive               0           Python software development\n",
       "9  positive               0           Python software development"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0aff68-09f5-4472-bbdd-88a6466c29f5",
   "metadata": {},
   "source": [
    "### with Pyadntic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b4ba245-dd5a-429d-95e6-d8a4f6df239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|############################################################################################################################################################| 103/103 [00:02<00:00, 48.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# fixes a bug with asyncio and jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_tagging_chain_pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. Pydantic Schema Definition\n",
    "class DocumentTags(BaseModel):\n",
    "    \"\"\"Pydantic model for the tags to be extracted from the document.\"\"\"\n",
    "\n",
    "    sentiment: str = Field(\n",
    "        description=\"The overall sentiment of the document (e.g., positive, negative, neutral).\"\n",
    "    )\n",
    "    aggressiveness: int = Field(\n",
    "        description=\"A rating from 1 to 10 of how aggressive the text is.\"\n",
    "    )\n",
    "    primary_topic: str = Field(description=\"The main topic of the document.\")\n",
    "\n",
    "\n",
    "# 2. Load Documents\n",
    "# Note: This can take a moment to run\n",
    "sitemap_loader = SitemapLoader(web_path=\"https://understandingdata.com/sitemap.xml\")\n",
    "sitemap_loader.requests_per_second = 5\n",
    "docs = sitemap_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "688b45ff-3eb2-425c-93f1-49d878939e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://understandingdata.com/',\n",
       " 'loc': 'https://understandingdata.com/',\n",
       " 'lastmod': '2025-08-17T12:52:13.527Z',\n",
       " 'changefreq': 'monthly',\n",
       " 'priority': '1.0'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493d8a6-ef21-45ec-bd84-863c7db90122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize Gemini LLM\n",
    "# Make sure your GOOGLE_API_KEY environment variable is set\n",
    "# llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.0-flash-lite\")\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 4. Create the Pydantic Tagging Chain\n",
    "# This chain is specifically designed to work with Pydantic models\n",
    "chain = create_tagging_chain_pydantic(DocumentTags, llm)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 5. Process Documents\n",
    "# Using a smaller slice [0:3] for a quick demonstration\n",
    "for index, doc in tqdm.notebook.tqdm(list(enumerate(docs[:10]))):\n",
    "    print(f\"--- Processing doc {index + 1} ---\")\n",
    "\n",
    "    # The input to invoke is the document content\n",
    "    chain_result = chain.invoke({\"input\": doc.page_content})\n",
    "\n",
    "    # The result is a Pydantic object, which we convert to a dict\n",
    "    # Access the result via the \"function\" key\n",
    "    # tag_data = chain_result[\"function\"].dict()\n",
    "    tag_data = chain_result[\"text\"]\n",
    "    results.append(tag_data)\n",
    "\n",
    "    print(tag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e0ceccd-18a1-4119-84ea-c0c990206bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Results ---\n",
      "  sentiment  aggressiveness                         primary_topic\n",
      "0  positive               3           Software & Data Engineering\n",
      "1  positive               3                            technology\n",
      "2   neutral               2                   Contact Information\n",
      "3  positive               3  Software & Data Engineering Services\n",
      "4  positive               3                  Software Engineering\n",
      "5   neutral               5                      Data Engineering\n",
      "6  positive               3             Data Engineering Services\n",
      "7  positive               3            React Software Development\n",
      "8  positive               3           Python Software Development\n",
      "9  positive               3           Python Software Development\n"
     ]
    }
   ],
   "source": [
    "# Optional: Display results in a DataFrame\n",
    "df = pd.DataFrame(map(dict, results))\n",
    "print(\"\\n--- Final Results ---\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ce924-c2a7-4a50-83ef-7ea897c102b7",
   "metadata": {},
   "source": [
    "## 93. Tracing with LangSmith\n",
    "https://colab.research.google.com/drive/1Sf-_1QP92iuJmFkykCufOYRkOB7tkliU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126efb30-3271-48e2-be43-f36e7b718fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-for-genai",
   "language": "python",
   "name": "conda-for-genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
