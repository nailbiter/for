{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f6f399-8c3b-4af3-ae8c-a1ac0a62a7df",
   "metadata": {},
   "source": [
    "* https://docs.google.com/document/d/1b7DlLtvrVxihcO65dsBaIjUU-a6aLjdQfNHSOM2PhUM/edit?tab=t.0#heading=h.v0phdvfalpr2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc866dac-b1b0-4ce4-9990-5dea8256244e",
   "metadata": {},
   "source": [
    "## setup and tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a239417-e284-42ed-92dc-ba8ead8b34f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25481e72-a46a-47ef-a0be-a39f9fabf387",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install langchain-google-genai langchain langchain_openai beautifulsoup4 langchain-community lxml serpapi google-search-results faiss-cpu langchainhub --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08a31554-bf98-4134-823c-1f7421bbf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from openai import OpenAI\n",
    "import jupyter_black\n",
    "import tqdm, tqdm.notebook\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "assert {\"OPENAI_API_KEY\", \"GOOGLE_API_KEY\"} <= set(os.environ)\n",
    "\n",
    "GEMINI_MODEL_NAME_CLEVER = \"gemini-2.0-flash\"\n",
    "GEMINI_MODEL_NAME_FAST = \"gemini-2.0-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96269410-51db-42e0-82ee-dd0dc0833e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing with LangChain effectively requires understanding its core components and adhering to best practices for building robust, reliable, and efficient LLM-powered applications. Here's a breakdown of key areas and recommendations:\n",
      "\n",
      "**1. Understanding LangChain Core Components**\n",
      "\n",
      "*   **Models:**  Choose the right LLM (OpenAI, Cohere, Hugging Face, etc.) based on your needs (cost, performance, specific capabilities).  Consider parameters like context window size, token limits, and model pricing.\n",
      "*   **Prompts:**\n",
      "    *   **Prompt Templates:**  Use templates to structure prompts consistently, allowing for easy parameterization and reuse.\n",
      "    *   **Prompt Engineering:**  Experiment with different prompt formulations to optimize performance.  Consider using techniques like few-shot learning (providing example inputs and outputs) and chain-of-thought prompting.\n",
      "    *   **Prompt Optimization:**  Iteratively refine your prompts based on LLM responses.  Evaluate responses and identify areas for improvement.\n",
      "*   **Chains:**  Organize sequences of calls to LLMs and other utilities.\n",
      "    *   **Simple Chains:**  Use basic chains for straightforward tasks (e.g., summarizing text).\n",
      "    *   **Sequential Chains:**  Chain steps together in a defined order.\n",
      "    *   **Router Chains:**  Choose the next chain based on the input.\n",
      "    *   **MapReduce/Refine Chains:**  Handle large inputs by breaking them down, processing them, and combining the results.\n",
      "*   **Indexes:**  Access and structure data for retrieval.\n",
      "    *   **Document Loaders:**  Use loaders to ingest data from various sources (PDFs, websites, databases, etc.).\n",
      "    *   **Text Splitters:**  Divide text into manageable chunks for LLM processing.\n",
      "    *   **Vectorstores:**  Store and search embeddings of your documents.  (e.g., Chroma, FAISS, Pinecone).  Choose a vectorstore that fits your scale, performance, and cost requirements.\n",
      "*   **Agents:**  Use LLMs to dynamically decide which tools to use and in what order.\n",
      "    *   **Tool Selection:**  Provide agents with appropriate tools (e.g., search, calculators, code execution).\n",
      "    *   **Agent Types:**  Experiment with different agent types (e.g., OpenAI, Conversational, Zero-shot).\n",
      "*   **Memory:**  Persist information across chain executions.\n",
      "    *   **Memory Types:**  Choose the best memory type (e.g., ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryBufferMemory) based on your application's needs and context window limitations.\n",
      "    *   **Memory Management:**  Be mindful of token limits and manage memory effectively to prevent exceeding context windows.\n",
      "*   **Callbacks:**  Monitor and log chain executions for debugging and observability.\n",
      "    *   **Custom Callbacks:**  Implement custom callbacks to track metrics, log events, and integrate with external monitoring systems.\n",
      "\n",
      "**2. Best Practices for Development**\n",
      "\n",
      "*   **Modular Design:**\n",
      "    *   **Break Down Complex Tasks:**  Decompose your application into smaller, reusable components (chains, agents, tools).\n",
      "    *   **Abstraction:**  Create abstract layers to encapsulate logic and make it easier to modify and maintain your code.\n",
      "*   **Error Handling and Resilience:**\n",
      "    *   **Try-Except Blocks:**  Wrap LLM calls and other operations in try-except blocks to catch and handle exceptions gracefully.\n",
      "    *   **Retries:**  Implement retry logic to handle transient errors (e.g., rate limits, network issues).  Use libraries like `tenacity`.\n",
      "    *   **Timeouts:**  Set timeouts on API calls to prevent hanging operations.\n",
      "    *   **Circuit Breakers:**  Consider using circuit breakers to prevent cascading failures.\n",
      "*   **Data Management:**\n",
      "    *   **Data Cleaning and Preprocessing:**  Clean and preprocess your data thoroughly before indexing it.  This improves the quality of your embeddings and search results.\n",
      "    *   **Chunking Strategies:**  Choose appropriate chunking strategies for your data.  The optimal chunk size depends on the LLM's context window, the nature of your data, and the desired level of detail in retrieval.\n",
      "    *   **Metadata:**  Add metadata to your documents to enable filtering and more precise search.\n",
      "*   **Prompt Engineering and Evaluation:**\n",
      "    *   **Iterative Refinement:**  Continuously refine your prompts based on the LLM's responses.\n",
      "    *   **Evaluation Metrics:**  Define evaluation metrics to measure the performance of your application.  (e.g., accuracy, relevance, coherence, fluency).\n",
      "    *   **A/B Testing:**  Experiment with different prompts and configurations to determine the best approach.\n",
      "    *   **Prompt Versioning:**  Track different prompt versions and their performance for reproducibility and comparison.\n",
      "*   **Security:**\n",
      "    *   **Input Validation:**  Validate user inputs to prevent prompt injection attacks.\n",
      "    *   **API Key Management:**  Securely store and manage your API keys.  Use environment variables or a secrets management system.\n",
      "    *   **Rate Limiting:**  Implement rate limiting to prevent abuse of your API.\n",
      "    *   **Data Privacy:**  Consider data privacy regulations (e.g., GDPR, CCPA) and handle user data responsibly.\n",
      "*   **Scalability and Performance:**\n",
      "    *   **Caching:**  Cache LLM responses to reduce latency and cost.\n",
      "    *   **Asynchronous Operations:**  Use asynchronous operations (e.g., `asyncio` in Python) to improve performance, especially when making multiple LLM calls.\n",
      "    *   **Batching:**  Batch requests to the LLM when possible to improve efficiency.\n",
      "    *   **Vectorstore Optimization:**  Optimize your vectorstore for performance (e.g., indexing strategies, query parameters).\n",
      "*   **Testing and Debugging:**\n",
      "    *   **Unit Tests:**  Write unit tests for individual components (chains, prompts, tools).\n",
      "    *   **Integration Tests:**  Test the interactions between different components.\n",
      "    *   **Logging:**  Use logging to track events and debug issues.  Log input prompts, LLM responses, errors, and other relevant information.\n",
      "    *   **Debugging Tools:**  Use debugging tools (e.g., LangChain's debugging features, IDE debuggers) to identify and fix issues.  Utilize the LangSmith platform for tracing and debugging.\n",
      "*   **Cost Optimization:**\n",
      "    *   **Model Selection:**  Choose the most cost-effective LLM for your needs.\n",
      "    *   **Token Usage:**  Monitor token usage and optimize your prompts and chains to minimize costs.\n",
      "    *   **Caching:**  Use caching to reduce the number of LLM calls.\n",
      "    *   **Batching:**  Batch requests to the LLM when possible.\n",
      "*   **Documentation:**\n",
      "    *   **Clear Documentation:**  Document your code thoroughly.  Include comments, docstrings, and a README file.\n",
      "    *   **Examples and Tutorials:**  Provide examples and tutorials to help others understand and use your application.\n",
      "\n",
      "**3. Specific Recommendations for Common Use Cases**\n",
      "\n",
      "*   **Chatbots:**  Use ConversationBufferMemory or ConversationBufferWindowMemory to maintain conversational context.  Consider using agents for more complex interactions.  Implement input validation and security measures.\n",
      "*   **Question Answering:**  Use RetrievalQA chains to answer questions based on a document or knowledge base.  Optimize your indexing and retrieval strategies.  Use prompt engineering to improve the quality of answers.\n",
      "*   **Summarization:**  Use Summarization chains to summarize text.  Consider using map-reduce or refine chains for large documents.\n",
      "*   **Code Generation:**  Use chains with tools that can generate and execute code.  Implement security measures to prevent malicious code execution.\n",
      "\n",
      "**4. Tools and Resources**\n",
      "\n",
      "*   **LangChain Documentation:**  The official documentation is the primary resource for understanding LangChain.\n",
      "*   **LangSmith:**  A powerful platform for tracing, debugging, and evaluating LangChain applications.\n",
      "*   **LangChain Hub:**  A repository of pre-built prompts, chains, and agents.\n",
      "*   **LangChain Examples:**  Explore LangChain's official examples for inspiration and guidance.\n",
      "*   **Community Forums and Discord:**  Engage with the LangChain community to ask questions and share knowledge.\n",
      "\n",
      "By following these best practices, you can build more robust, reliable, and efficient LLM-powered applications with LangChain. Remember that LLM development is an iterative process. Continuously learn, experiment, and refine your approach to achieve the best results.\n",
      "CPU times: user 317 ms, sys: 67.1 ms, total: 384 ms\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "response = llm.invoke(\"What are the best practices for developing with LangChain?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a078a1-9f34-4f21-a57b-3435bd23bf9f",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "# chat = ChatOpenAI(openai_api_key=\"...\")\n",
    "# If you have an envionrment variable set for OPENAI_API_KEY, you can just do:\n",
    "chat = ChatOpenAI()\n",
    "chat.invoke(\"Hello, how are you?\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "654b4fcb-f826-4012-96b9-a35fa19c5c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well, thank you for asking! As a large language model, I don't experience feelings in the same way humans do, but I am functioning correctly and ready to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable.\n",
    "# You can get one from Google AI Studio: https://aistudio.google.com/app/apikey\n",
    "\n",
    "# Initialize the Gemini chat model\n",
    "# You can also specify other models like \"gemini-1.5-pro\"\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "# Invoke the model with a prompt\n",
    "response = chat.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0333dc9-a2a4-4b45-9bf1-02348ff467b3",
   "metadata": {},
   "source": [
    "## 85. Chat Models -- Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4b12e-cda6-4142-ac41-f61911adfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = chat.invoke(\"What is the capital of France?\")\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb264da-cacc-4fa9-87c0-b7a305281b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0f7d4-5d29-45c2-bcd3-385cfcfccc53",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "result = chat.invoke(messages)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5784b22-dce4-4992-9562-cd3af0fa1f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Ensure your GOOGLE_API_KEY environment variable is set\n",
    "# 1. Initialize the Gemini chat model\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "# 2. Define the message(s) for the model\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that generates company names.\"),\n",
    "    HumanMessage(content=text),\n",
    "]\n",
    "\n",
    "# 3. Invoke the model with the messages\n",
    "result = chat.invoke(messages)\n",
    "\n",
    "# 4. Print the AI's response content\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a4456-7f1f-4747-839e-943486e3425c",
   "metadata": {},
   "source": [
    "## 86. Chat Prompt Templates\n",
    "* https://drive.google.com/file/d/1JoyxZlYfngmXnvrRyo7qqvUoB7qz6il0/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6a8d9-78c2-4604-b3e0-cbb9a603efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that generates company names\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat_prompt_template.invoke(\n",
    "    {\n",
    "        \"text\": \"What would be a good company name for a company that makes colorful socks?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# model = Cha(model='gpt-4o-mini')\n",
    "\n",
    "ai_llm_result = chat.invoke(result)\n",
    "print(ai_llm_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561f15e-9717-4d09-9e29-8ba7d53fbc3a",
   "metadata": {},
   "source": [
    "## 87. Streaming\n",
    "* https://drive.google.com/file/d/18sGlOZ8AKwON1CXUMnqf9ONfj7bwjSiO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db2846-80d8-4ef7-a5ad-b225aba91066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm, tqdm.notebook\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    # streaming=True,\n",
    ")\n",
    "for chunk in tqdm.notebook.tqdm(chat.stream(\"What is the capital of the moon?\")):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb60bd3-6ed4-46de-b0bb-af66c3962836",
   "metadata": {},
   "source": [
    "## 88. Output Parsers\n",
    "* https://drive.google.com/file/d/1QWwi3AOCHEoMR83zR21sB7zzKdUxVdfO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7202cb-737a-4396-998f-36a914b55ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddb083-21d6-4463-baf5-9de828dad2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7732fa1-8542-4911-aa58-18613e3b8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6c1e9-e7b7-4a97-b4f5-94318c5358b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "\n",
    "messages = chat_prompt.invoke(\n",
    "    {\n",
    "        \"query\": \"What is a really funny joke about Python programming?\",\n",
    "        \"format_instructions\": parser.get_format_instructions(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8f8a7-7e75-4dab-974c-7845e12c1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()\n",
    "## does not work with Gemini\n",
    "result = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af04d1-8046-43d5-a1b7-6da3f7c30306",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    joke_object = parser.parse(result.content)\n",
    "    print(joke_object.setup)\n",
    "    print(joke_object.punchline)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c358e2-007f-429a-8297-c71275ed72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7afe0-a0b0-4d5b-9233-6556838c9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f91080-cfa2-49a3-b42c-40ce6b224b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    explanation: str = Field(\n",
    "        description=\"A detailed explanation of why this joke is funny.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3a9b0-6199-410e-9c69-9cc95f51c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275de99-b846-4cbd-88da-9f4bcfa2bf2e",
   "metadata": {},
   "source": [
    "## 89. Summarizing large amounts of text\n",
    "* https://colab.research.google.com/drive/11t0e03SThhKRPq9T1M7xg6BcooBFaTkA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775946d-4c56-414d-a533-914ba58de93a",
   "metadata": {},
   "source": [
    "### crisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d808c-3f91-4376-b8a9-4c0eb30c4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    temperature=0,\n",
    "    model=GEMINI_MODEL_NAME_CLEVER,\n",
    ")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "res = chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56313b30-4169-49ec-9d0c-e484fc5122bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"input_documents\"]\n",
    "Markdown(res[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224b2aa-5ecc-456a-9851-b141d0a820aa",
   "metadata": {},
   "source": [
    "### map reduce\n",
    "\n",
    "* problem if pages refer to each other (since summaries are done independently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f7fa2-83c6-4dcb-99c0-b9e6cab914cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import (\n",
    "    ReduceDocumentsChain,\n",
    "    MapReduceDocumentsChain,\n",
    "    StuffDocumentsChain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f491b-eed6-4373-ae2f-9a4447342be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(temperature=0, model=GEMINI_MODEL_NAME_CLEVER)\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# map_chain:\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes.\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26d252-e08a-4b56-879f-33d170308e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccadc0-b9f5-4623-ab75-d49b33a39531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ddcd9-71e2-4653-805e-6624093556e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print()\n",
    "res = map_reduce_chain.invoke(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1fa67-aced-4345-aedb-192336c7270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(res[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db3570-931e-47fc-ae4a-4258cc741b9f",
   "metadata": {},
   "source": [
    "### template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e109f63e-e3a5-435b-8484-8c14782636c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWrite a concise summary of the following:\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;132;01m{text}\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mCONCISE SUMMARY:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(prompt_template)\n\u001b[1;32m      6\u001b[0m refine_template \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour job is to produce a final summary\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe have provided an existing summary up to a certain point: \u001b[39m\u001b[38;5;132;01m{existing_answer}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the context isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt useful, return the original summary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m refine_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(refine_template)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)\n",
    "\n",
    "# Page 1 --> Page 2 (Refine) --> Page 3 (Refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b77696-979a-473c-89a2-7c66a06ea248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(result[\"intermediate_steps\"]):\n",
    "    display(Markdown(f\"## step {i+1}\\n{v}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ee122-daab-4695-9547-55f1b7ed317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65598b-c2ad-422c-9ebb-407e0e372a3e",
   "metadata": {},
   "source": [
    "## 91. Document Loaders, Text Splitting, Creating LangChain Documents\n",
    "\n",
    "https://colab.research.google.com/drive/1YdtBCggWStErmFeP5GBSmEaw04kXeKqD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f681afb-0def-485e-b64e-acba6c7c4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import requests\n",
    "\n",
    "# Get this file and save it locally:\n",
    "url = \"https://github.com/hammer-mt/thumb/blob/master/README.md\"\n",
    "\n",
    "# Save it locally:\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the text from the HTML:\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "text = soup.get_text()\n",
    "\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "loader = TextLoader(\"README.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaab45fc-b5c7-4fc8-821c-77db1e7079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac22e81-df9a-4f65-b986-2d604671b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'test': 'test'}, page_content='test')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "[Document(page_content=\"test\", metadata={\"test\": \"test\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa692e28-061f-4d92-8250-ca8abd7f0c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the text into sentences:\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "final_docs = text_splitter.split_documents(loader.load())\n",
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f6c560f-0533-43a2-aeb8-647960c0cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d20a453-590b-43a9-b71a-306a02cdee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.4 ms, sys: 50.5 ms, total: 120 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chain = load_summarize_chain(llm=chat, chain_type=\"map_reduce\")\n",
    "res = chain.invoke(\n",
    "    {\n",
    "        \"input_documents\": final_docs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3853c392-b6bd-4a21-ab70-7babac04a0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This describes a comprehensive GitHub interface, outlining features for code management, collaboration, security, AI-powered tools (Copilot), automation (Actions), and learning. It includes project-specific views, enterprise-grade security and support options, and links to resources, documentation, and community features. The interface encompasses navigation menus, error messages, and user settings, including a feedback form, saved search management, and appearance customization.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()\n",
    "res[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee58ead-f5ea-4b49-a05c-580744658705",
   "metadata": {},
   "source": [
    "## 92. Tagging Documents\n",
    "https://colab.research.google.com/drive/1Gn1IxMqz0RcOaDKVdY7cnzgik0JoQlqZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50718f47-4d27-4e63-8211-57501d8091e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixes a bug with asyncio and jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719cbb2c-9f7f-4efe-9db6-36f8a2cb5362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c493471d-d23f-4bb2-b8c7-7a30dbcf8572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|############################################################################################################################################################| 103/103 [00:03<00:00, 33.36it/s]\n"
     ]
    }
   ],
   "source": [
    "sitemap_loader = SitemapLoader(web_path=\"https://understandingdata.com/sitemap.xml\")\n",
    "sitemap_loader.requests_per_second = 5\n",
    "docs = sitemap_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aedb080f-8883-4469-bddc-e1981a440684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\"type\": \"string\"},\n",
    "        \"aggressiveness\": {\"type\": \"integer\"},\n",
    "        \"primary_topic\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The main topic of the document.\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"primary_topic\", \"sentiment\", \"aggressiveness\"],\n",
    "}\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "## does not work with Gemini\n",
    "# llm = ChatGoogleGenerativeAI(temperature=0, model=GEMINI_MODEL_NAME_CLEVER)\n",
    "chain = create_tagging_chain(schema, llm, output_key=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69ee82b9-9eb4-4a27-8f73-8fda88b2b04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing doc 1\n",
      "Processing doc 2\n",
      "Processing doc 3\n",
      "Processing doc 4\n",
      "Processing doc 5\n",
      "Processing doc 6\n",
      "Processing doc 7\n",
      "Processing doc 8\n",
      "Processing doc 9\n",
      "Processing doc 10\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Remove the 0:10 to run on all documents:\n",
    "for index, doc in enumerate(docs[0:10]):\n",
    "    print(f\"Processing doc {index +1}\")\n",
    "    chain_result = chain.invoke({\"input\": doc.page_content})\n",
    "    results.append(chain_result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75147b4b-ab4b-4298-85e9-548c495e045a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>aggressiveness</th>\n",
       "      <th>primary_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>AI products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>3</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Software &amp; Data Engineering Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Software &amp; Data Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Engineering Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>3</td>\n",
       "      <td>React Software Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Python software development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>Python software development</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment  aggressiveness                         primary_topic\n",
       "0  positive               0                           AI products\n",
       "1  positive               3                            technology\n",
       "2  positive               0                               Contact\n",
       "3  positive               2  Software & Data Engineering Services\n",
       "4  positive               0           Software & Data Engineering\n",
       "5   neutral               0                      Data Engineering\n",
       "6  positive               0             Data Engineering Services\n",
       "7  positive               3            React Software Development\n",
       "8  positive               0           Python software development\n",
       "9  positive               0           Python software development"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0aff68-09f5-4472-bbdd-88a6466c29f5",
   "metadata": {},
   "source": [
    "### with Pyadntic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b4ba245-dd5a-429d-95e6-d8a4f6df239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|############################################################################################################################################################| 103/103 [00:02<00:00, 48.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# fixes a bug with asyncio and jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_tagging_chain_pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. Pydantic Schema Definition\n",
    "class DocumentTags(BaseModel):\n",
    "    \"\"\"Pydantic model for the tags to be extracted from the document.\"\"\"\n",
    "\n",
    "    sentiment: str = Field(\n",
    "        description=\"The overall sentiment of the document (e.g., positive, negative, neutral).\"\n",
    "    )\n",
    "    aggressiveness: int = Field(\n",
    "        description=\"A rating from 1 to 10 of how aggressive the text is.\"\n",
    "    )\n",
    "    primary_topic: str = Field(description=\"The main topic of the document.\")\n",
    "\n",
    "\n",
    "# 2. Load Documents\n",
    "# Note: This can take a moment to run\n",
    "sitemap_loader = SitemapLoader(web_path=\"https://understandingdata.com/sitemap.xml\")\n",
    "sitemap_loader.requests_per_second = 5\n",
    "docs = sitemap_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "688b45ff-3eb2-425c-93f1-49d878939e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://understandingdata.com/',\n",
       " 'loc': 'https://understandingdata.com/',\n",
       " 'lastmod': '2025-08-17T12:52:13.527Z',\n",
       " 'changefreq': 'monthly',\n",
       " 'priority': '1.0'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493d8a6-ef21-45ec-bd84-863c7db90122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize Gemini LLM\n",
    "# Make sure your GOOGLE_API_KEY environment variable is set\n",
    "# llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.0-flash-lite\")\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 4. Create the Pydantic Tagging Chain\n",
    "# This chain is specifically designed to work with Pydantic models\n",
    "chain = create_tagging_chain_pydantic(DocumentTags, llm)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 5. Process Documents\n",
    "# Using a smaller slice [0:3] for a quick demonstration\n",
    "for index, doc in tqdm.notebook.tqdm(list(enumerate(docs[:10]))):\n",
    "    print(f\"--- Processing doc {index + 1} ---\")\n",
    "\n",
    "    # The input to invoke is the document content\n",
    "    chain_result = chain.invoke({\"input\": doc.page_content})\n",
    "\n",
    "    # The result is a Pydantic object, which we convert to a dict\n",
    "    # Access the result via the \"function\" key\n",
    "    # tag_data = chain_result[\"function\"].dict()\n",
    "    tag_data = chain_result[\"text\"]\n",
    "    results.append(tag_data)\n",
    "\n",
    "    print(tag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e0ceccd-18a1-4119-84ea-c0c990206bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Results ---\n",
      "  sentiment  aggressiveness                         primary_topic\n",
      "0  positive               3           Software & Data Engineering\n",
      "1  positive               3                            technology\n",
      "2   neutral               2                   Contact Information\n",
      "3  positive               3  Software & Data Engineering Services\n",
      "4  positive               3                  Software Engineering\n",
      "5   neutral               5                      Data Engineering\n",
      "6  positive               3             Data Engineering Services\n",
      "7  positive               3            React Software Development\n",
      "8  positive               3           Python Software Development\n",
      "9  positive               3           Python Software Development\n"
     ]
    }
   ],
   "source": [
    "# Optional: Display results in a DataFrame\n",
    "df = pd.DataFrame(map(dict, results))\n",
    "print(\"\\n--- Final Results ---\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ce924-c2a7-4a50-83ef-7ea897c102b7",
   "metadata": {},
   "source": [
    "## 93. Tracing with LangSmith\n",
    "* https://colab.research.google.com/drive/1Sf-_1QP92iuJmFkykCufOYRkOB7tkliU\n",
    "* https://smith.langchain.com\n",
    "* https://serpapi.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94539d54-f1f2-41aa-81f3-8754a2a17225",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert {\"LANGCHAIN_API_KEY\", \"SERPAPI_API_KEY\"} <= set(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "126efb30-3271-48e2-be43-f36e7b718fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "unique_id = uuid.uuid4().hex[0:8]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"LANGCHAIN_API_KEY\"\n",
    "\n",
    "# # Used by the agent in this tutorial\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"SERPAPI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54c0759b-b384-484e-a06d-bca273cfdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0f6d78-90df-45b0-a209-cf0e9642454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/m1j4q6jj79zbjw0qtxcldpnc0000gn/T/ipykernel_3407/2786892264.py:8: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_CLEVER, temperature=0)\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff6a6a55-4ad9-4939-9d25-ca4afe63e070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117913144a00484b80c3a9ccf778bf19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 7\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 52\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "].\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import tenacity\n",
    "\n",
    "inputs = [\n",
    "    \"How many people live in canada as of 2023?\",\n",
    "    \"who is dua lipa's boyfriend? what is his age raised to the .43 power?\",\n",
    "    \"what is dua lipa's boyfriend age raised to the .43 power?\",\n",
    "    \"how far is it from paris to boston in miles\",\n",
    "    \"what was the total number of points scored in the 2023 super bowl? what is that number raised to the .23 power?\",\n",
    "    \"what was the total number of points scored in the 2023 super bowl raised to the .23 power?\",\n",
    "    \"how many more points were scored in the 2023 super bowl than in the 2022 super bowl?\",\n",
    "    \"what is 153 raised to .1312 power?\",\n",
    "    \"who is kendall jenner's boyfriend? what is his height (in inches) raised to .13 power?\",\n",
    "    \"what is 1213 divided by 4345?\",\n",
    "]\n",
    "results = []\n",
    "\n",
    "\n",
    "async def arun(agent, input_example):\n",
    "    try:\n",
    "        return await agent.arun(input_example)\n",
    "    except Exception as e:\n",
    "        # The agent sometimes makes mistakes! These will be captured by the tracing.\n",
    "        return e\n",
    "\n",
    "\n",
    "@tenacity.retry(stop=tenacity.stop_after_attempt(4), wait=tenacity.wait_fixed(30))\n",
    "def run(agent, input_example: str) -> str:\n",
    "    return agent.invoke(input_example)\n",
    "\n",
    "\n",
    "# for input_example in inputs:\n",
    "#     results.append(arun(agent, input_example))\n",
    "# results = await asyncio.gather(*results)\n",
    "\n",
    "for input_example in tqdm.notebook.tqdm(inputs):\n",
    "    time.sleep(2)\n",
    "    results.append(run(agent, input_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fe29bb1-ae28-48aa-8fe4-16165ef147e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n",
    "\n",
    "# Logs are submitted in a background thread to avoid blocking execution.\n",
    "# For the sake of this tutorial, we want to make sure\n",
    "# they've been submitted before moving on. This is also\n",
    "# useful for serverless deployments.\n",
    "wait_for_all_tracers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff2dcaa0-deef-4606-9d2e-88a6ee4ca897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many people live in canada as of 2023?</td>\n",
       "      <td>40.08 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who is dua lipa's boyfriend? what is his age raised to the .43 power?</td>\n",
       "      <td>Dua Lipa's boyfriend is Callum Turner. His age, 35, raised to the power of 0.43 is approximately 4.61.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is dua lipa's boyfriend age raised to the .43 power?</td>\n",
       "      <td>4.612636795281377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how far is it from paris to boston in miles</td>\n",
       "      <td>The distance from Paris to Boston is approximately 3440 miles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what was the total number of points scored in the 2023 super bowl? what is that number raised to the .23 power?</td>\n",
       "      <td>The total number of points scored in the 2023 Super Bowl was 73. 73 raised to the power of 0.23 is approximately 2.682651500990882.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what was the total number of points scored in the 2023 super bowl raised to the .23 power?</td>\n",
       "      <td>2.682651500990882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>how many more points were scored in the 2023 super bowl than in the 2022 super bowl?</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what is 153 raised to .1312 power?</td>\n",
       "      <td>1.9347796717823205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>who is kendall jenner's boyfriend? what is his height (in inches) raised to .13 power?</td>\n",
       "      <td>Kendall Jenner's boyfriend is Devin Booker. His height (in inches) raised to the power of 0.13 is approximately 1.76.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what is 1213 divided by 4345?</td>\n",
       "      <td>0.2791714614499425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## gemini\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(pd.DataFrame(results).to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c0a58-6657-4bbc-9e90-5a66f43223c6",
   "metadata": {},
   "source": [
    "tracing result: https://drive.google.com/drive/folders/1_4hnRpTYZxO_JLBDDE2HCESQW0eHE6Bj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6799dd-e707-4734-8b96-bde3df4f2060",
   "metadata": {},
   "source": [
    "## 94. LangChain Hub\n",
    "* https://colab.research.google.com/drive/1lxCk4cnk60rzmu0Wz6pzK-sUmWGca5b0\n",
    "* https://docs.smith.langchain.com/prompt_engineering/how_to_guides#prompt-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec3e290-23fb-4b7d-95fc-d433dd89533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cbf1a02-f62d-43ac-94b0-b62ea5d35ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"homanp/question-answer-pair\")\n",
    "prompt_two = hub.pull(\"gitmaxd/synthetic-training-data\")\n",
    "prompt_three = hub.pull(\"rlm/text-to-sql\")\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79610019-1f43-4ef0-a239-4b8caecaa272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'data_format', 'number_of_pairs'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'homanp', 'lc_hub_repo': 'question-answer-pair', 'lc_hub_commit_hash': 'ee1cda5f28d2e6992e2b5a782edaa5711092246ea914671133bd24296a2f63de'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'data_format', 'number_of_pairs'], input_types={}, partial_variables={}, template='You are an AI assistant tasked with generating question and answer pairs for the given context using the given format. Only answer in the format with no other text. You should create the following number of question/answer pairs: {number_of_pairs}. Return the question/answer pairs as a Python List. Each dict in the list should have the full context provided, a relevant question to the context and an answer to the question.\\n\\nFormat:\\n{data_format}\\n\\nContext:\\n{context}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc55b0bb-d856-48bc-8fa9-9318d7c33e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['EXAMPLE', 'NUMBER', 'PERSPECTIVE', 'SEED_CONTENT'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'gitmaxd', 'lc_hub_repo': 'synthetic-training-data', 'lc_hub_commit_hash': '61aef88bc285cb07783a5575f2f8d504a7cc7d1b2e71fdb63d91dcedfefceb5c'}, template='Utilize Natural Language Processing techniques and Generative AI to create new Question/Answer pair textual training data for OpenAI LLMs by drawing inspiration from the given seed content: {SEED_CONTENT} \\n\\nHere are the steps to follow:\\n\\n1. Examine the provided seed content to identify significant and important topics, entities, relationships, and themes. You should use each important topic, entity, relationship, and theme you recognize. You can employ methods such as named entity recognition, content summarization, keyword/keyphrase extraction, and semantic analysis to comprehend the content deeply.\\n\\n2. Based on the analysis conducted in the first step, employ a generative language model to generate fresh, new synthetic text samples. These samples should cover the same topic, entities, relationships, and themes present in the seed data. Aim to generate {NUMBER} high-quality variations that accurately explore different Question and Answer possibilities within the data space.\\n\\n3. Ensure that the generated synthetic samples exhibit language diversity. Vary elements like wording, sentence structure, tone, and complexity while retaining the core concepts. The objective is to produce diverse, representative data rather than repetitive instances.\\n\\n4. Format and deliver the generated synthetic samples in a structured Pandas Dataframe suitable for training and machine learning purposes.\\n\\n5. The desired output length is roughly equivalent to the length of the seed content.\\n\\nCreate these generated synthetic samples as if you are writing from the {PERSPECTIVE} perspective.\\n\\nOnly output the resulting dataframe in the format of this example:  {EXAMPLE}\\n\\nDo not include any commentary or extraneous casualties.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fae5ab0d-6dc8-4928-be6e-63dc71c02d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5882f65-a8ae-4184-90dc-290ab131c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(rag_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6360c1fd-95c4-4299-8fff-4d4d75ccb9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/var/folders/02/m1j4q6jj79zbjw0qtxcldpnc0000gn/T/ipykernel_3407/2784775807.py:14: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The approaches to Task Decomposition include using LLM with simple prompting, task-specific instructions, and human inputs. Task decomposition involves breaking down large tasks into smaller subgoals for efficient handling of complex tasks and reflecting on past actions for improvement. Challenges in long-term planning and task decomposition include adjusting plans over a lengthy history and effectively exploring the solution space.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load docs\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Store splits\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# RAG prompt\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=vectorstore.as_retriever(), chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19932a95-31d1-42bc-af29-303e25049ac2",
   "metadata": {},
   "source": [
    "## 95. LCEL (=LangChain Expression Language) - The Runnable Protocol\n",
    "* https://colab.research.google.com/drive/1iHmhKEhntUy71C_gO4kNqylZQYshAgD1\n",
    "* https://python.langchain.com/docs/concepts/lcel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c2b8a9-7fe3-4293-86f0-adc962e6dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "print(\n",
    "    type(RunnableLambda(lambda x: x + 1))\n",
    ")  # <class 'langchain.schema.runnable.RunnableLambda'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc1603e-8e72-4a4a-8f85-0818eda80d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableLambda(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1703f88-bdfd-458c-9950-96053d3cb919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 43)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(1), chain.invoke(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9a5a1ac-7814-4c58-be22-e535fd101f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "\n",
      "\n",
      "---\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 6, 8]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A RunnableSequence constructed using the `|` operator\n",
    "sequence = RunnableLambda(lambda x: x + 1) | (lambda x: x * 2)\n",
    "\n",
    "print(type(sequence))  # <class 'langchain.schema.runnable.RunnableSequence'>\n",
    "print(\"\\n\\n---\")\n",
    "print(sequence.invoke(1))  # 4\n",
    "sequence.batch([1, 2, 3])  # [4, 6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc536f93-1f86-49eb-95df-f7b61e47da15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mul_2': 4, 'mul_5': 10}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A sequence that contains a RunnableParallel constructed using a dict literal\n",
    "sequence = RunnableLambda(lambda x: x + 1) | {\n",
    "    \"mul_2\": RunnableLambda(lambda x: x * 2),\n",
    "    \"mul_5\": RunnableLambda(lambda x: x * 5),\n",
    "}\n",
    "sequence.invoke(1)  # {'mul_2': 4, 'mul_5': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca29822-115d-4abd-b47f-ee317bec9517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = (\n",
    "    RunnableLambda(lambda x: x + 1)\n",
    "    | {\n",
    "        \"mul_2\": RunnableLambda(lambda x: x * 2),\n",
    "        \"mul_5\": RunnableLambda(lambda x: x * 5),\n",
    "    }\n",
    "    | RunnableLambda(lambda x: x[\"mul_2\"] + x[\"mul_5\"])\n",
    ")\n",
    "sequence.invoke(1)  # {'mul_2': 4, 'mul_5': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "977e06f2-971f-4be3-b7d4-bdb4672cc3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableParallel'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    {\"mul_2\": RunnableLambda(lambda x: x * 2), \"mul_5\": RunnableLambda(lambda x: x * 5)}\n",
    ")\n",
    "\n",
    "# This is a dictionary, however it will be composed with other runnables when used in a sequence:\n",
    "parallel_two = {\n",
    "    \"mul_2\": RunnableLambda(lambda x: x[\"input_one\"] * 2),\n",
    "    \"mul_5\": RunnableLambda(lambda x: x[\"input_two\"] * 5),\n",
    "}\n",
    "\n",
    "print(type(parallel))  # <class 'langchain.schema.runnable.RunnableParallel'>\n",
    "print(type(parallel_two))  # <class 'dict'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df2d138d-0b57-4e54-88f5-bc7c4682c4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = parallel | RunnableLambda(lambda x: x[\"mul_2\"] + x[\"mul_5\"])\n",
    "chain.invoke(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b03119-3cf8-4dad-bd18-7d5d8bef8556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_chain = parallel_two | RunnableLambda(lambda x: x[\"mul_2\"] + x[\"mul_5\"])\n",
    "second_chain.invoke({\"input_one\": 5, \"input_two\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a444c0-59a3-49cb-a38f-b9924bd6d233",
   "metadata": {},
   "source": [
    "## 96. ChatModels, itemgetter and RAG\n",
    "* https://colab.research.google.com/drive/1PwupkZARnPadd4i1700WY6Y0u8eiiHxz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5c95bec-5554-482b-988a-a064b73c1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    "    RunnableLambda,\n",
    ")\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bafe03aa-3b67-408d-8da1-4860fcb660f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'origin': 1, 'modified': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original': 'hello world', 'parsed': 'dlrow olleh'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnableParallel(origin=RunnablePassthrough(), modified=lambda x: x + 1)\n",
    "\n",
    "print(runnable.invoke(1))  # {'origin': 1, 'modified': 2}\n",
    "\n",
    "\n",
    "def fake_llm(prompt: str) -> str:  # Fake LLM for the example\n",
    "    return prompt + \" world\"\n",
    "\n",
    "\n",
    "chain = RunnableLambda(fake_llm) | {\n",
    "    \"original\": RunnablePassthrough(),  # Original LLM output\n",
    "    \"parsed\": lambda text: text[::-1],  # Parsing logic\n",
    "}\n",
    "\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e8d15bf-6cf2-4e7d-b0a9-392b83685592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}'), additional_kwargs={})]) middle=[] last=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash-lite', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x1157a3fe0>, default_metadata=(), model_kwargs={})\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# chat = ChatOpenAI()\n",
    "chat = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_FAST)\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "chain = prompt | chat\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e7339de-cec7-427f-88bd-7a9b8a5b3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first input_variables=['topic'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}'), additional_kwargs={})]\n",
      "last model='models/gemini-2.0-flash-lite' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x1157a3fe0> default_metadata=() model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "print(\"first\", chain.first)\n",
    "print(\"last\", chain.last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44276d97-879b-4af8-9d73-6fb09a5ffcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stream:\n",
      "\n",
      "Why don't bears wear shoes?\n",
      "\n",
      "Because they have bear feet!\n",
      "\n",
      "\n",
      "Invoke:\n",
      "\n",
      "Why don't bears wear shoes?\n",
      "\n",
      "Because they have bear feet!\n",
      "\n",
      "\n",
      "Batch:\n",
      "\n",
      "[AIMessage(content=\"Why don't bears like fast food?\\n\\nBecause they can't *bear* the wait!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--99ca5db1-9e50-44f9-b091-9f13d926caee-0', usage_metadata={'input_tokens': 6, 'output_tokens': 22, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}}), AIMessage(content='Why did the Red Hat employee bring a ladder to the Linux conference?\\n\\nBecause they heard there was a lot of **upgrading** to do!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--f6aa99a9-bfe5-42fd-af8c-8a6b93c98269-0', usage_metadata={'input_tokens': 7, 'output_tokens': 31, 'total_tokens': 38, 'input_token_details': {'cache_read': 0}}), AIMessage(content='Why did the monk refuse to eat the apple?\\n\\nBecause he was a *nun*!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--87518887-6451-402f-9e7e-93e78335e336-0', usage_metadata={'input_tokens': 6, 'output_tokens': 20, 'total_tokens': 26, 'input_token_details': {'cache_read': 0}})]\n",
      "Why don't bears like fast food?\n",
      "\n",
      "Because they can't *bear* the wait!\n",
      "Why did the Red Hat employee bring a ladder to the Linux conference?\n",
      "\n",
      "Because they heard there was a lot of **upgrading** to do!\n",
      "Why did the monk refuse to eat the apple?\n",
      "\n",
      "Because he was a *nun*!\n"
     ]
    }
   ],
   "source": [
    "# Stream:\n",
    "print(\"\\n\\nStream:\\n\")\n",
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(s.content, end=\"\", flush=True)\n",
    "\n",
    "# Invoke:\n",
    "print(\"\\n\\nInvoke:\\n\")\n",
    "print(chain.invoke({\"topic\": \"bears\"}).content)\n",
    "\n",
    "# Batch:\n",
    "print(\"\\n\\nBatch:\\n\")\n",
    "res = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"redhats\"}, {\"topic\": \"monks\"}])\n",
    "print(res)\n",
    "print(\"\\n\".join((map(operator.attrgetter(\"content\"), res))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee95f25-f9ad-4767-8ac2-4effd15fa117",
   "metadata": {},
   "source": [
    "### RAG in LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "697a3aaa-92fa-45e0-adf7-64636e5dd9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d770e92d-29bf-40c5-b92c-4f7154d3a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\n",
    "        \"James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData\",\n",
    "        \"James Phoenix has an age of 31 years old.\",\n",
    "    ],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# model = ChatOpenAI()\n",
    "model = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_CLEVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b09cf62-4bd9-4149-95ee-d028f041baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# It's the same as this, but the tuple allows for line breaks:\n",
    "# {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3afbd029-77aa-4535-8904-081ea14985e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'James Phoenix works at JustUnderstandingData.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What company does James phoenix work at?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e50ca61d-8b29-4317-a37e-1d47a12def3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'James Phoenix has an age of 31 years old.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is James Phoenix's age?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f930a-687a-4191-9093-803467eff922",
   "metadata": {},
   "source": [
    "### itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e93d2fb-8f07-4d8a-a4b4-7dd343caa75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operator.itemgetter({'data': ['This is a test', 'Another entry...']})\n",
      "['This is a test', 'Another entry...']\n"
     ]
    }
   ],
   "source": [
    "test = {\"data\": [\"This is a test\", \"Another entry...\"]}\n",
    "\n",
    "print(itemgetter(test))\n",
    "print(itemgetter(\"data\")(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cceb7dd3-c7fc-4904-8bdf-25db861996ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"James Phoenix's profession is **Data Engineer**.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"What is the profession of James Phoenix? His profession is {profession}.\"\"\"\n",
    ")\n",
    "\n",
    "first_chain = RunnableParallel(name=lambda x: \"James Phoenix\", age=lambda x: 31)\n",
    "\n",
    "second_chain = {\n",
    "    # itemgetter is used to get the value from the dictionary from the previous step: (note this is only the previous step, not the whole chain)\n",
    "    \"name\": itemgetter(\"name\"),\n",
    "    \"age\": itemgetter(\"age\"),\n",
    "    # You can not use string values, either use itemgetter or a lambda, or RunnablePassthrough\n",
    "    \"profession\": lambda x: \"Data Engineer\",\n",
    "}\n",
    "\n",
    "chain = (\n",
    "    first_chain\n",
    "    | second_chain\n",
    "    | prompt\n",
    "    | ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_FAST)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69e5f3-e9be-4221-9a72-aa135c0d96c5",
   "metadata": {},
   "source": [
    "## 97. LCEL - Chat Message History and Memory\n",
    "* https://colab.research.google.com/drive/1dmDHw39-5NNiQtz3s_b8470lXTJYNx8I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a0c7f40-3c81-4ffe-916f-4d4e1c797a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066032b-bb1f-4068-92fc-b069249efd18",
   "metadata": {},
   "source": [
    "### conversational history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45915c3c-485d-4195-a423-514addf2dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\n",
    "        \"James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData\",\n",
    "        \"James is 31 years old.\",\n",
    "    ],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "242968dc-f556-4c6f-985a-36d36d7c0e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6561aeb6-01fc-42f0-aece-69e83c1973d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "ANSWER_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "171df715-eaed-4210-989e-c407d9d5e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs: list,\n",
    "    document_prompt: PromptTemplate = DEFAULT_DOCUMENT_PROMPT,\n",
    "    document_separator: str = \"\\n\\n\",\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79bf861e-ced7-4f21-9924-51342152855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "def _format_chat_history(\n",
    "    chat_history: List[Union[HumanMessage, SystemMessage, AIMessage]]\n",
    ") -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        if isinstance(dialogue_turn, HumanMessage):\n",
    "            buffer += \"\\nHuman: \" + dialogue_turn.content\n",
    "        elif isinstance(dialogue_turn, AIMessage):\n",
    "            buffer += \"\\nAssistant: \" + dialogue_turn.content\n",
    "        elif isinstance(dialogue_turn, SystemMessage):\n",
    "            buffer += \"\\nSystem: \" + dialogue_turn.content\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "692659bb-8c75-4c5a-a8b1-1374f6acc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_CLEVER, temperature=0)\n",
    "_inputs = RunnableMap(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    # | ChatOpenAI(temperature=0)\n",
    "    | chat\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1520b95f-548f-4f57-accd-44b19a66b8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'James worked at JustUnderstandingData.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"where did James work?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd0582-c94f-437c-bde3-c4c2667bcc76",
   "metadata": {},
   "source": [
    "### memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5d8a734-cc8d-45fa-bb33-2b882564f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/m1j4q6jj79zbjw0qtxcldpnc0000gn/T/ipykernel_6680/3982471492.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90140c74-aba5-4f40-95d0-69cd9688d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add a step to load memory\n",
    "# This adds a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    # | ChatOpenAI(temperature=0)\n",
    "    | chat\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "\n",
    "# This is REALLY IMPORTANT as the chain above becomes StrOutputParser() so it will only have one key, which gets passed to the retriever!\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": (\n",
    "        final_inputs\n",
    "        | ANSWER_PROMPT\n",
    "        # | ChatOpenAI()\n",
    "        | chat\n",
    "    ),\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# And now we put it all together!\n",
    "final_chain = (\n",
    "    loaded_memory\n",
    "    | standalone_question\n",
    "    | retrieved_documents\n",
    "    | answer\n",
    "    # | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5417400-38fa-4e90-b1b3-44280b97cad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': AIMessage(content='James Phoenix worked at JustUnderstandingData.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--452e3306-6fc4-47e5-8a5b-a3593bc25256-0', usage_metadata={'input_tokens': 45, 'output_tokens': 9, 'total_tokens': 54, 'input_token_details': {'cache_read': 0}}), 'docs': [Document(id='fb603293-81a3-4749-ba96-d0acde1c2e77', metadata={}, page_content='James Phoenix works as a data engineering and LLM consultant at JustUnderstandingData'), Document(id='e743fca8-1eb6-4d42-b861-c8bd21bb34bb', metadata={}, page_content='James is 31 years old.')]}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"where did James Phoenix work?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "05c8048c-fdf5-4f72-a912-95b430b6444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "98699c61-cd94-4da7-a0df-5f9f350a570d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='where did James Phoenix work?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='James Phoenix worked at JustUnderstandingData.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aea857-a9f3-43e5-b5de-cacd06abaa6f",
   "metadata": {},
   "source": [
    "## 98. LCEL - Multiple Chains\n",
    "* https://colab.research.google.com/drive/1W6T-iQ835IEV4fPPiYykY29yCYL8ZIwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "04a378c0-4e4e-4081-aff9-af1900209ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"What city was {person} born in?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"What country is the city {city} in? Respond in {language}\"\n",
    ")\n",
    "\n",
    "# model = ChatOpenAI()\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c6cd07f2-e207-4cf1-b015-a75e5246c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Obama is from Honolulu, Hawaii.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    prompt1 | ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME_FAST) | StrOutputParser()\n",
    ").invoke({\"person\": \"Barack Obama\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce55eae3-f666-4b65-a253-e9323c5fe61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great question with a few parts to it!\\n\\nWhile he was born in **Honolulu, Hawaii**, the city most closely associated with Barack Obama's adult life and political career is **Chicago, Illinois**.\\n\\nHere's a quick breakdown:\\n\\n*   **Birthplace:** Honolulu, Hawaii\\n*   **Hometown & Political Base:** Chicago, Illinois. This is where he worked as a community organizer, taught law at the University of Chicago, served as an Illinois State Senator, and launched his campaign for U.S. Senate and President. It's the city he and Michelle Obama call their hometown.\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prompt1 | model | StrOutputParser()).invoke({\"person\": \"Barack Obama\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "12f4b03b-d727-4695-ace2-67262ec8144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ambas ciudades, tanto Chicago como Honolulu, estn en los **Estados Unidos**.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke({\"person\": \"Barack Obama\", \"language\": \"Spanish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d8956-c435-4f56-aed5-cbe1594f7ab4",
   "metadata": {},
   "source": [
    "## 99. LCEL - Conditional Logic, Branching and Merging\n",
    "* https://colab.research.google.com/drive/1f4rSRMDzzAGlSh2zR0oJGIqW2s8NfJeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2bf76cb2-fc25-45db-a454-db89f77e6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "af66079a-5323-4520-b5a2-7a58a4db2a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "HELL\n",
      "This is the default case, in case no above lambda functions match.\n"
     ]
    }
   ],
   "source": [
    "branch = RunnableBranch(\n",
    "    (lambda x: x == \"hello\", lambda x: x),\n",
    "    (lambda x: isinstance(x, str), lambda x: x.upper()),\n",
    "    (lambda x: \"This is the default case, in case no above lambda functions match.\"),\n",
    ")\n",
    "\n",
    "print(branch.invoke(\"hello\"))  # \"hello\"\n",
    "print(branch.invoke(\"hell\"))\n",
    "print(branch.invoke(None))  # \"This is the default case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "73296bf4-01f0-4875-af46-ec61daea07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = (\n",
    "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the pros or positive aspects of {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the cons or negative aspects of {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
    "            (\"system\", \"Generate a final response given the critique\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "981c3dee-c61d-43f2-a9ab-c3167854a013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In conclusion, while Scrum offers numerous advantages such as promoting collaboration, enhancing adaptability, and improving efficiency, there are also potential drawbacks that should be considered. These include issues like lack of predictability, high overhead, dependency on team collaboration, limited scalability, focus on short-term goals, and potential for burnout.\\n\\nTo address these challenges, it's important for teams adopting Scrum to establish clear communication channels, prioritize team well-being, and ensure alignment with stakeholders on expectations. By proactively managing these potential drawbacks, teams can maximize the benefits of the Scrum framework and achieve successful project outcomes.\\n\\nUltimately, like any project management approach, the effectiveness of Scrum depends on proper implementation, team dynamics, and adaptation to the specific needs of the project. With careful planning and ongoing refinement, teams can leverage the strengths of Scrum while mitigating its potential limitations to drive project success.\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"scrum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "94218513-d0c8-4020-b493-970544de4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running chain with a custom callback handler ---\n",
      "\n",
      "--- Final Response ---\n",
      "Why don't cats play poker in the jungle? \n",
      "\n",
      "Too many cheetahs!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import langchain\n",
    "from contextlib import contextmanager\n",
    "from operator import itemgetter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from typing import Any, Dict\n",
    "\n",
    "# 1. Configure logging to save output to a file\n",
    "# logging.basicConfig(\n",
    "#     filename=\"langchain_debug.log\",\n",
    "#     filemode=\"w\",\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "# )\n",
    "logger = logging.getLogger(\"langchain\")\n",
    "logger.setLevel(logging.INFO)  # Set the desired logging level (e.g., INFO, DEBUG)\n",
    "\n",
    "# 2. Create a handler to write to a file\n",
    "#    'w' mode overwrites the file each time, use 'a' to append\n",
    "handler = logging.FileHandler(\"langchain.log\", mode=\"w\")\n",
    "\n",
    "# 3. Create a formatter to define the log message's structure\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# 4. Add the handler to the logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# 5. Prevent logs from propagating to the root logger to avoid duplicates\n",
    "logger.propagate = False\n",
    "\n",
    "\n",
    "class MyLoggingCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"A callback handler that logs events to a given logger.\"\"\"\n",
    "\n",
    "    def __init__(self, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Log the start of a chain run.\"\"\"\n",
    "        self.logger.info(f\"Chain started with inputs: {inputs}\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs: Any) -> None:\n",
    "        \"\"\"Log the end of an LLM call.\"\"\"\n",
    "        # The actual response object structure may vary by model provider\n",
    "        self.logger.info(\n",
    "            f\"LLM generated response: {response.generations[0][0].text[:80]}...\"\n",
    "        )\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Instantiate your handler with your logger\n",
    "my_handler = MyLoggingCallbackHandler(logger=logger)\n",
    "\n",
    "print(\"--- Running chain with a custom callback handler ---\")\n",
    "response = chain.invoke(\n",
    "    {\"topic\": \"cats\"}, config={\"callbacks\": [my_handler]}  # Pass the handler here\n",
    ")\n",
    "print(\"\\n--- Final Response ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "99a99360-a32a-4479-b006-91a1e6bef34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fba8ea53-65b9-4d2a-af0e-1475ef29ab35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analysis: This joke hinges entirely on a pun, a form of wordplay that exploits the multiple meanings of a word or the similarity in sound between different words. The effectiveness of the joke lies in the unexpected twist of substituting \"bear\" (referring to the animal) for \"bare\" (meaning uncovered). This creates a humorous situation because it takes a literal interpretation of \"bear feet\" while simultaneously providing a nonsensical, yet understandable, \"reason\" for the absence of shoes. The humor arises from the incongruity between the expected answer (a logical reason) and the pun-based answer. It\\'s a simple, clever joke that leverages the listener\\'s knowledge of language and common understanding of bears to achieve its comedic effect.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = False\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.stdout import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "joke_chain = (\n",
    "    ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | {\"joke\": RunnablePassthrough(), \"topic\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "explain_joke = (\n",
    "    ChatPromptTemplate.from_template(\"Explain the joke: {joke}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "benefits_of_joke = (\n",
    "    ChatPromptTemplate.from_template(\"List the benefits of this joke: {joke}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are responsible for generating a small analysis of a joke. The topic will be: {topic}\",\n",
    "            ),\n",
    "            (\"ai\", \"{joke}. The benefits of this joke are: {benefits}\"),\n",
    "            (\"human\", \"The explanation of the joke is: {explanation}\"),\n",
    "            (\"human\", \"Generate a small analysis of the joke. Analysis: \"),\n",
    "        ]\n",
    "    )\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | joke_chain\n",
    "    | {\n",
    "        \"explanation\": explain_joke,\n",
    "        \"benefits\": benefits_of_joke,\n",
    "        \"joke\": itemgetter(\"joke\"),\n",
    "        \"topic\": itemgetter(\"topic\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")\n",
    "\n",
    "final_chain.invoke(\n",
    "    {\"topic\": \"bears\"},\n",
    "    # config={\"callbacks\": [StdOutCallbackHandler()]},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-for-genai",
   "language": "python",
   "name": "conda-for-genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
