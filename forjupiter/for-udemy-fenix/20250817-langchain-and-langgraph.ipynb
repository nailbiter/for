{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f6f399-8c3b-4af3-ae8c-a1ac0a62a7df",
   "metadata": {},
   "source": [
    "* https://docs.google.com/document/d/1b7DlLtvrVxihcO65dsBaIjUU-a6aLjdQfNHSOM2PhUM/edit?tab=t.0#heading=h.v0phdvfalpr2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc866dac-b1b0-4ce4-9990-5dea8256244e",
   "metadata": {},
   "source": [
    "## setup and tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a239417-e284-42ed-92dc-ba8ead8b34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25481e72-a46a-47ef-a0be-a39f9fabf387",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install langchain-google-genai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb011f3-d7ea-48c1-b96b-36e0c7a51a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from openai import OpenAI\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "assert {\"OPENAI_API_KEY\", \"GOOGLE_API_KEY\"} <= set(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96269410-51db-42e0-82ee-dd0dc0833e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing with LangChain effectively requires understanding its core concepts, best practices, and common pitfalls. Here's a breakdown of best practices categorized for clarity:\n",
      "\n",
      "**I. Core Principles and Conceptual Understanding:**\n",
      "\n",
      "*   **Understand the LangChain Abstractions:**\n",
      "    *   **Models:**  LLMs (e.g., OpenAI, Cohere), Chat Models, Embeddings. Know their strengths, weaknesses, and cost implications.\n",
      "    *   **Chains:**  Sequences of calls, often combining models with other components.  Understand different chain types (e.g., Sequential, Router, MapReduce, Stuffing).\n",
      "    *   **Agents:**  Use LLMs to decide which actions to take. Understand the interplay between agents, tools, and toolkits.\n",
      "    *   **Memory:**  How to store and manage the history of interactions.  Choose the right memory type (e.g., ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryMemory) based on your needs.\n",
      "    *   **Indexes:**  Methods for structuring data for LLMs to access.  Understand different index types (e.g., DocumentLoaders, TextSplitters, Vectorstores).\n",
      "    *   **Callbacks:**  Used for monitoring, logging, and custom behavior within the LangChain framework.  Essential for debugging and performance analysis.\n",
      "    *   **Document Loaders and Text Splitters:** Choose the appropriate document loader (e.g., PDF, HTML, CSV) and text splitter (e.g., RecursiveCharacterTextSplitter, TokenTextSplitter) to handle your data effectively.  Consider chunk size and overlap for optimal performance.\n",
      "\n",
      "*   **Start Simple and Iterate:**  Begin with smaller, more manageable chains and agents.  Test and refine each step before building complex systems.\n",
      "\n",
      "*   **Think About the User Experience:**  Consider how your application will interact with users.  Design prompts and outputs that are clear, concise, and helpful.\n",
      "\n",
      "*   **Cost Optimization:**  LLM calls can be expensive.  Be mindful of:\n",
      "    *   **Token Usage:**  Keep prompts as concise as possible.  Use techniques like summarization and retrieval to reduce context window size.\n",
      "    *   **Model Selection:**  Choose the right model for the task.  More powerful models are often more expensive.\n",
      "    *   **Caching:**  Cache the results of expensive operations (e.g., embeddings) to avoid repeated calls.\n",
      "    *   **Rate Limiting:** Implement rate limiting to prevent exceeding API usage limits.\n",
      "\n",
      "**II.  Prompt Engineering and Model Interaction:**\n",
      "\n",
      "*   **Prompt Design is Crucial:**  The quality of your prompts directly impacts the performance of your chains and agents.\n",
      "    *   **Clear Instructions:**  Provide explicit instructions to the LLM about what you want it to do.\n",
      "    *   **Context:**  Provide the LLM with the necessary context to answer the question or complete the task.\n",
      "    *   **Format:**  Specify the desired output format (e.g., JSON, bullet points).\n",
      "    *   **Examples (Few-Shot Learning):**  Include examples of the desired input/output pairs to guide the LLM.\n",
      "    *   **Iterate and Refine:**  Experiment with different prompts and evaluate their effectiveness.  Use A/B testing to compare different prompt variations.\n",
      "\n",
      "*   **Handle Model Responses Gracefully:**  LLMs can sometimes generate unexpected or incorrect responses.\n",
      "    *   **Error Handling:**  Implement error handling to catch exceptions and prevent your application from crashing.\n",
      "    *   **Output Validation:**  Validate the LLM's output to ensure it meets your requirements.\n",
      "    *   **Prompt Optimization for Reliability:**  Refine prompts to improve the reliability and accuracy of responses.\n",
      "    *   **Temperature and Top_p:** Experiment with temperature (randomness) and top_p (nucleus sampling) to control the creativity and determinism of the model. Lower temperatures are generally better for factual tasks, while higher temperatures can be useful for creative tasks.\n",
      "\n",
      "*   **Use Templates for Reusability and Maintainability:**  LangChain provides template classes for prompts, allowing you to create reusable and maintainable prompts.\n",
      "    *   `PromptTemplate`:  For simple prompts.\n",
      "    *   `FewShotPromptTemplate`:  For prompts with few-shot examples.\n",
      "    *   Parameterize your prompts to make them dynamic and flexible.\n",
      "\n",
      "**III.  Data Handling and Retrieval:**\n",
      "\n",
      "*   **Data Preprocessing:**\n",
      "    *   **Cleaning:** Clean your data by removing noise, inconsistencies, and irrelevant information.\n",
      "    *   **Chunking:** Split your data into smaller, manageable chunks for processing by the LLM.  Choose an appropriate chunk size and overlap based on your use case.\n",
      "    *   **Metadata:**  Add metadata to your chunks to provide context and improve retrieval.\n",
      "\n",
      "*   **Vectorstores and Embeddings:**\n",
      "    *   **Choose the Right Vectorstore:**  Select a vectorstore (e.g., ChromaDB, Pinecone, Weaviate, FAISS) that meets your performance, scalability, and cost requirements.\n",
      "    *   **Embedding Models:**  Use a suitable embedding model (e.g., OpenAI embeddings, Sentence Transformers) to convert your text data into vector representations.\n",
      "    *   **Semantic Search:**  Use semantic search to retrieve relevant information from your vectorstore based on the meaning of the query.\n",
      "\n",
      "*   **Retrieval Augmented Generation (RAG):**  Use RAG to improve the accuracy and relevance of your LLM responses by retrieving contextually relevant information from your data.\n",
      "    *   **Retrieval Strategies:** Experiment with different retrieval strategies (e.g., similarity search, hybrid search) to optimize retrieval performance.\n",
      "    *   **Context Window Management:**  Carefully manage the context window of the LLM by only providing the most relevant information retrieved from your data.\n",
      "\n",
      "**IV.  Agent Design and Tool Usage:**\n",
      "\n",
      "*   **Choose the Right Agent Type:**  LangChain offers different agent types (e.g., Zero-Shot Agent, Conversational Agent, Open AI Functions Agent).  Select the agent type that best suits your needs.\n",
      "\n",
      "*   **Tool Selection and Design:**\n",
      "    *   **Relevant Tools:**  Provide the agent with a relevant set of tools.\n",
      "    *   **Clear Tool Descriptions:**  Write clear and concise descriptions for each tool.\n",
      "    *   **Tool Input and Output:**  Define the input and output formats for each tool.\n",
      "    *   **Tool Functionality:**  Ensure that your tools perform their intended functions correctly.\n",
      "\n",
      "*   **Agent Training and Evaluation:**\n",
      "    *   **Prompt Optimization:**  Fine-tune the agent's prompts to improve its performance.\n",
      "    *   **Evaluation Metrics:**  Define evaluation metrics to assess the agent's accuracy and effectiveness.\n",
      "    *   **Iterative Improvement:**  Continuously evaluate and improve the agent's performance based on feedback and results.\n",
      "\n",
      "**V.  Testing, Debugging, and Monitoring:**\n",
      "\n",
      "*   **Unit Testing:** Write unit tests to ensure that individual components of your LangChain application are working correctly.\n",
      "    *   Test Chains, Agents, and Tools.\n",
      "    *   Mock external dependencies (e.g., LLM calls, API calls) to isolate your tests.\n",
      "\n",
      "*   **Integration Testing:**  Test the interactions between different components of your application.\n",
      "\n",
      "*   **Debugging:**\n",
      "    *   **Logging:**  Use logging to track the execution of your chains and agents.\n",
      "    *   **Callbacks:**  Use callbacks to monitor the performance of your application and identify bottlenecks.\n",
      "    *   **LangChainHub:**  Utilize LangChainHub for pre-built components and examples to understand and debug your code.\n",
      "\n",
      "*   **Monitoring:**\n",
      "    *   **Performance Metrics:**  Monitor key performance metrics (e.g., latency, token usage, error rates) to identify and address performance issues.\n",
      "    *   **LLM Provider Monitoring:** Leverage any monitoring tools provided by your LLM provider (e.g., OpenAI's usage dashboard).\n",
      "    *   **Error Tracking:**  Implement error tracking to identify and resolve errors in your application.\n",
      "\n",
      "**VI.  Scalability and Productionization:**\n",
      "\n",
      "*   **Asynchronous Operations:** Use asynchronous operations to improve the performance and scalability of your application, especially when making API calls.\n",
      "\n",
      "*   **Caching:** Implement caching to reduce the load on your LLM and improve response times.\n",
      "\n",
      "*   **Rate Limiting:** Implement rate limiting to protect your application from exceeding API usage limits.\n",
      "\n",
      "*   **Load Balancing:** Use load balancing to distribute traffic across multiple instances of your application.\n",
      "\n",
      "*   **Deployment:**  Deploy your application to a production environment (e.g., cloud platform, serverless function).\n",
      "\n",
      "*   **Security:**\n",
      "    *   **Input Sanitization:**  Sanitize user inputs to prevent security vulnerabilities (e.g., prompt injection).\n",
      "    *   **Access Control:**  Implement access control to protect your application from unauthorized access.\n",
      "    *   **Secret Management:**  Securely store and manage your API keys and other sensitive information.\n",
      "\n",
      "**VII.  Code Structure and Maintainability:**\n",
      "\n",
      "*   **Modular Design:**  Break down your application into modular components to improve code reusability and maintainability.\n",
      "\n",
      "*   **Code Organization:**  Organize your code into well-defined modules and packages.\n",
      "\n",
      "*   **Documentation:**  Document your code thoroughly, including comments, docstrings, and user guides.\n",
      "\n",
      "*   **Version Control:**  Use version control (e.g., Git) to track changes to your code and collaborate with others.\n",
      "\n",
      "*   **Code Style:**  Follow a consistent code style (e.g., PEP 8 for Python) to improve code readability.\n",
      "\n",
      "**VIII.  Community and Resources:**\n",
      "\n",
      "*   **LangChain Documentation:**  Consult the official LangChain documentation for detailed information and examples.\n",
      "*   **LangChain Hub:** Explore the LangChain Hub for pre-built components, templates, and examples.\n",
      "*   **LangChain Community:**  Engage with the LangChain community through forums, discussions, and social media.\n",
      "*   **Open Source Examples:**  Study open-source LangChain projects to learn from experienced developers.\n",
      "*   **Tutorials and Courses:**  Follow tutorials and courses to learn the basics and advanced techniques.\n",
      "\n",
      "By following these best practices, you can develop robust, efficient, and scalable applications with LangChain.  Remember to continuously learn, experiment, and iterate to improve your skills and build innovative solutions.\n",
      "CPU times: user 1.14 s, sys: 302 ms, total: 1.44 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "response = llm.invoke(\"What are the best practices for developing with LangChain?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a078a1-9f34-4f21-a57b-3435bd23bf9f",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "# chat = ChatOpenAI(openai_api_key=\"...\")\n",
    "# If you have an envionrment variable set for OPENAI_API_KEY, you can just do:\n",
    "chat = ChatOpenAI()\n",
    "chat.invoke(\"Hello, how are you?\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b4fcb-f826-4012-96b9-a35fa19c5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable.\n",
    "# You can get one from Google AI Studio: https://aistudio.google.com/app/apikey\n",
    "\n",
    "# Initialize the Gemini chat model\n",
    "# You can also specify other models like \"gemini-1.5-pro\"\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "# Invoke the model with a prompt\n",
    "response = chat.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0333dc9-a2a4-4b45-9bf1-02348ff467b3",
   "metadata": {},
   "source": [
    "## 85. Chat Models -- Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4b12e-cda6-4142-ac41-f61911adfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = chat.invoke(\"What is the capital of France?\")\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb264da-cacc-4fa9-87c0-b7a305281b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0f7d4-5d29-45c2-bcd3-385cfcfccc53",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "result = chat.invoke(messages)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5784b22-dce4-4992-9562-cd3af0fa1f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Ensure your GOOGLE_API_KEY environment variable is set\n",
    "# 1. Initialize the Gemini chat model\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "# 2. Define the message(s) for the model\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that generates company names.\"),\n",
    "    HumanMessage(content=text),\n",
    "]\n",
    "\n",
    "# 3. Invoke the model with the messages\n",
    "result = chat.invoke(messages)\n",
    "\n",
    "# 4. Print the AI's response content\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a4456-7f1f-4747-839e-943486e3425c",
   "metadata": {},
   "source": [
    "## 86. Chat Prompt Templates\n",
    "* https://drive.google.com/file/d/1JoyxZlYfngmXnvrRyo7qqvUoB7qz6il0/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6a8d9-78c2-4604-b3e0-cbb9a603efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that generates company names\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat_prompt_template.invoke(\n",
    "    {\n",
    "        \"text\": \"What would be a good company name for a company that makes colorful socks?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# model = Cha(model='gpt-4o-mini')\n",
    "\n",
    "ai_llm_result = chat.invoke(result)\n",
    "print(ai_llm_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561f15e-9717-4d09-9e29-8ba7d53fbc3a",
   "metadata": {},
   "source": [
    "## 87. Streaming\n",
    "* https://drive.google.com/file/d/18sGlOZ8AKwON1CXUMnqf9ONfj7bwjSiO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db2846-80d8-4ef7-a5ad-b225aba91066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm, tqdm.notebook\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    # streaming=True,\n",
    ")\n",
    "for chunk in tqdm.notebook.tqdm(chat.stream(\"What is the capital of the moon?\")):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb60bd3-6ed4-46de-b0bb-af66c3962836",
   "metadata": {},
   "source": [
    "## 88. Output Parsers\n",
    "* https://drive.google.com/file/d/1QWwi3AOCHEoMR83zR21sB7zzKdUxVdfO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7202cb-737a-4396-998f-36a914b55ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddb083-21d6-4463-baf5-9de828dad2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7732fa1-8542-4911-aa58-18613e3b8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6c1e9-e7b7-4a97-b4f5-94318c5358b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "\n",
    "messages = chat_prompt.invoke(\n",
    "    {\n",
    "        \"query\": \"What is a really funny joke about Python programming?\",\n",
    "        \"format_instructions\": parser.get_format_instructions(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8f8a7-7e75-4dab-974c-7845e12c1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()\n",
    "## does not work with Gemini\n",
    "result = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af04d1-8046-43d5-a1b7-6da3f7c30306",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    joke_object = parser.parse(result.content)\n",
    "    print(joke_object.setup)\n",
    "    print(joke_object.punchline)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c358e2-007f-429a-8297-c71275ed72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7afe0-a0b0-4d5b-9233-6556838c9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f91080-cfa2-49a3-b42c-40ce6b224b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    explanation: str = Field(\n",
    "        description=\"A detailed explanation of why this joke is funny.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3a9b0-6199-410e-9c69-9cc95f51c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275de99-b846-4cbd-88da-9f4bcfa2bf2e",
   "metadata": {},
   "source": [
    "## 89. Summarizing large amounts of text\n",
    "* https://colab.research.google.com/drive/11t0e03SThhKRPq9T1M7xg6BcooBFaTkA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0a797-a198-40b5-92ec-18d41fe0c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_MODEL_NAME_CLEVER = \"gemini-2.0-flash\"\n",
    "GEMINI_MODEL_NAME_FAST = \"gemini-2.0-flash-lite\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775946d-4c56-414d-a533-914ba58de93a",
   "metadata": {},
   "source": [
    "### crisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d808c-3f91-4376-b8a9-4c0eb30c4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    temperature=0,\n",
    "    model=GEMINI_MODEL_NAME_CLEVER,\n",
    ")\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "res = chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56313b30-4169-49ec-9d0c-e484fc5122bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"input_documents\"]\n",
    "Markdown(res[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224b2aa-5ecc-456a-9851-b141d0a820aa",
   "metadata": {},
   "source": [
    "### map reduce\n",
    "\n",
    "* problem if pages refer to each other (since summaries are done independently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f7fa2-83c6-4dcb-99c0-b9e6cab914cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import (\n",
    "    ReduceDocumentsChain,\n",
    "    MapReduceDocumentsChain,\n",
    "    StuffDocumentsChain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f491b-eed6-4373-ae2f-9a4447342be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(temperature=0, model=GEMINI_MODEL_NAME_CLEVER)\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# map_chain:\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes.\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26d252-e08a-4b56-879f-33d170308e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccadc0-b9f5-4623-ab75-d49b33a39531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ddcd9-71e2-4653-805e-6624093556e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print()\n",
    "res = map_reduce_chain.invoke(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1fa67-aced-4345-aedb-192336c7270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(res[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db3570-931e-47fc-ae4a-4258cc741b9f",
   "metadata": {},
   "source": [
    "### template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109f63e-e3a5-435b-8484-8c14782636c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)\n",
    "\n",
    "# Page 1 --> Page 2 (Refine) --> Page 3 (Refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b77696-979a-473c-89a2-7c66a06ea248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(result[\"intermediate_steps\"]):\n",
    "    display(Markdown(f\"## step {i+1}\\n{v}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ee122-daab-4695-9547-55f1b7ed317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65598b-c2ad-422c-9ebb-407e0e372a3e",
   "metadata": {},
   "source": [
    "## 91. Document Loaders, Text Splitting, Creating LangChain Documents\n",
    "\n",
    "https://colab.research.google.com/drive/1YdtBCggWStErmFeP5GBSmEaw04kXeKqD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f681afb-0def-485e-b64e-acba6c7c4e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-for-genai",
   "language": "python",
   "name": "conda-for-genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
