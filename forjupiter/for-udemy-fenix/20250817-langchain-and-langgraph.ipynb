{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f6f399-8c3b-4af3-ae8c-a1ac0a62a7df",
   "metadata": {},
   "source": [
    "* https://docs.google.com/document/d/1b7DlLtvrVxihcO65dsBaIjUU-a6aLjdQfNHSOM2PhUM/edit?tab=t.0#heading=h.v0phdvfalpr2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc866dac-b1b0-4ce4-9990-5dea8256244e",
   "metadata": {},
   "source": [
    "## setup and tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a239417-e284-42ed-92dc-ba8ead8b34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25481e72-a46a-47ef-a0be-a39f9fabf387",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install langchain-google-genai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f99fa0-3230-4882-8265-952753150d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from openai import OpenAI\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96269410-51db-42e0-82ee-dd0dc0833e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing with LangChain effectively requires understanding its core components and applying best practices for efficiency, maintainability, and responsible AI usage. Here's a breakdown of key areas and recommended practices:\n",
      "\n",
      "**1. Understanding LangChain Components & Architecture:**\n",
      "\n",
      "*   **LLMs (Large Language Models):**  Familiarize yourself with different LLMs (OpenAI, Cohere, Hugging Face models, etc.) and their strengths and weaknesses. Choose the right model for your task (e.g., GPT-3.5 for cost-effectiveness, GPT-4 for advanced reasoning).\n",
      "*   **Prompts & Prompt Templates:**  Master prompt engineering. Understand how to craft effective prompts that guide the LLM's behavior. Use prompt templates for reusability and parameterization.\n",
      "*   **Chains:**  Learn how to chain LLMs and other components together. Chains enable complex workflows, from simple summarization to sophisticated question-answering systems.\n",
      "*   **Indexes:**  Utilize indexes (e.g., document loaders, retrievers, vectorstores) to efficiently manage and search through large datasets.  Understand the trade-offs of different indexing techniques.\n",
      "*   **Agents:**  Explore agents, which use LLMs to decide which actions to take (e.g., calling APIs, searching the web).  Understand their capabilities and limitations.\n",
      "*   **Memory:**  Implement memory mechanisms to maintain context across interactions.  Choose the appropriate memory type (e.g., conversation buffer, summary memory) for your application.\n",
      "*   **Callbacks:**  Use callbacks to monitor and debug your LangChain applications. They provide insights into the inner workings of chains, agents, and LLMs.\n",
      "*   **Document Loaders & Splitters:**  Learn how to load different document types and split them into manageable chunks for processing.\n",
      "*   **Vectorstores:** Understand how to store and query embeddings generated from your documents. Choose the appropriate vectorstore (e.g., ChromaDB, FAISS, Pinecone) based on your needs.\n",
      "\n",
      "**2. Prompt Engineering Best Practices:**\n",
      "\n",
      "*   **Clarity & Specificity:**  Be clear and specific in your instructions. Avoid ambiguity.\n",
      "*   **Role-Playing:**  Assign roles to the LLM to influence its behavior (e.g., \"You are a helpful assistant...\" or \"You are a legal expert...\").\n",
      "*   **Examples (Few-Shot Learning):**  Provide examples of input-output pairs to guide the LLM's response. This is especially useful when dealing with specific formats or tasks.\n",
      "*   **Step-by-Step Instructions:** Break down complex tasks into smaller, manageable steps.\n",
      "*   **Constraints & Formatting:**  Set constraints on the output (e.g., \"Summarize in 3 sentences,\" \"Return the answer in JSON format\").  Specify desired output formats.\n",
      "*   **Iterative Refinement:**  Prompt engineering is often an iterative process. Experiment with different prompts and refine them based on the LLM's responses.\n",
      "*   **Test Prompts Regularly:**  Use a set of test cases to evaluate your prompts and ensure consistency.\n",
      "\n",
      "**3. Chain & Agent Design Best Practices:**\n",
      "\n",
      "*   **Modularity:**  Design chains and agents with modularity in mind. Break down complex tasks into smaller, reusable components.\n",
      "*   **Error Handling:**  Implement robust error handling to gracefully handle unexpected situations (e.g., API failures, invalid inputs).\n",
      "*   **Input Validation:**  Validate user inputs to prevent issues and ensure data integrity.\n",
      "*   **Contextualization:**  Provide sufficient context to LLMs and agents to improve their accuracy and relevance.\n",
      "*   **Agent Selection:**  Carefully choose the right agent type for your task. Consider the agent's reasoning capabilities, tools, and limitations.\n",
      "*   **Tool Selection (Agents):**  Select and configure tools carefully for agents.  Ensure the tools are relevant and reliable.  Consider the potential for misuse and security implications.\n",
      "*   **Chain of Thought (CoT):**  Encourage the LLM to explain its reasoning process (e.g., \"Let's think step by step...\"). This can improve accuracy and transparency.\n",
      "\n",
      "**4. Indexing & Retrieval Best Practices:**\n",
      "\n",
      "*   **Chunking Strategy:** Choose an appropriate chunking strategy based on the document type and the desired retrieval granularity.  Consider semantic meaning when chunking.\n",
      "*   **Embedding Models:** Select an embedding model that is suitable for your data and the task (e.g., OpenAI's embeddings, Sentence Transformers).\n",
      "*   **Vectorstore Selection:** Choose a vectorstore that meets your scalability, performance, and cost requirements.\n",
      "*   **Metadata:**  Add relevant metadata to your documents to enable filtering and more precise retrieval.\n",
      "*   **Hybrid Search:**  Consider using a hybrid search approach that combines keyword search with semantic search for improved accuracy.\n",
      "*   **Query Optimization:**  Optimize your queries to the vectorstore to retrieve the most relevant results.\n",
      "*   **Evaluation:** Evaluate the performance of your retrieval system using metrics like precision, recall, and F1-score.\n",
      "\n",
      "**5. Memory Management Best Practices:**\n",
      "\n",
      "*   **Choose the Right Memory Type:**  Select the memory type that aligns with your application's requirements.  Consider the context window limits of your LLM.\n",
      "*   **Context Window Awareness:**  Be mindful of the LLM's context window size.  Summarize or truncate the conversation history if needed.\n",
      "*   **Memory Overflows:** Implement mechanisms to prevent memory overflows.\n",
      "*   **Serialization & Persistence:**  Consider serializing and persisting memory to disk or a database for long-running applications.\n",
      "\n",
      "**6. Code Quality & Maintainability:**\n",
      "\n",
      "*   **Use a Version Control System (Git):**  Track your code changes and collaborate effectively.\n",
      "*   **Write Clear & Concise Code:**  Follow coding style guidelines and use meaningful variable names.\n",
      "*   **Document Your Code:**  Add comments to explain the purpose of your code and the logic behind it.\n",
      "*   **Write Unit Tests:**  Create unit tests to verify the correctness of your functions and components.\n",
      "*   **Use a Linter & Formatter:**  Use a linter (e.g., `flake8`, `pylint`) and a formatter (e.g., `black`, `autopep8`) to maintain consistent code style.\n",
      "*   **Modularize Your Code:** Break down your code into reusable functions and classes.\n",
      "*   **Refactor Regularly:**  Refactor your code to improve its readability, maintainability, and performance.\n",
      "*   **Follow SOLID principles:** Apply object-oriented design principles for well-structured code.\n",
      "\n",
      "**7. Responsible AI Practices:**\n",
      "\n",
      "*   **Bias Detection & Mitigation:**  Be aware of potential biases in LLMs and your data.  Implement strategies to mitigate bias (e.g., data augmentation, prompt engineering).\n",
      "*   **Transparency & Explainability:**  Strive for transparency in your LLM-based systems.  Provide explanations for the LLM's decisions.\n",
      "*   **Human Oversight:**  Implement human oversight where appropriate, especially in high-stakes applications.\n",
      "*   **Data Privacy & Security:**  Protect user data and comply with privacy regulations.  Implement security measures to prevent unauthorized access.\n",
      "*   **Avoid Hallucinations:**  Be aware of the risk of LLMs generating false or misleading information (hallucinations).  Implement techniques to reduce hallucinations (e.g., grounding in reliable sources, confidence scores).\n",
      "*   **Content Moderation:**  Implement content moderation mechanisms to prevent the generation of harmful or offensive content.\n",
      "*   **Consider the Impact:**  Think about the potential societal impact of your application and address any ethical concerns.\n",
      "*   **Documentation:** Document your system's capabilities, limitations, and intended use cases.\n",
      "\n",
      "**8. Debugging & Monitoring:**\n",
      "\n",
      "*   **Use Callbacks:**  Leverage LangChain's callback system to monitor the execution of chains and agents.\n",
      "*   **Logging:**  Implement comprehensive logging to track events, errors, and performance metrics.\n",
      "*   **Error Reporting:**  Implement error reporting mechanisms to capture and analyze errors.\n",
      "*   **Performance Monitoring:**  Monitor the performance of your LLM-based applications (e.g., latency, throughput, cost).\n",
      "*   **Experimentation:**  Experiment with different configurations and evaluate their impact on performance and accuracy.\n",
      "\n",
      "**9. Deployment & Scaling:**\n",
      "\n",
      "*   **Infrastructure:** Choose the appropriate infrastructure for deploying your application (e.g., cloud platforms, containerization).\n",
      "*   **Scalability:** Design your application to scale to handle increasing workloads.  Consider techniques like load balancing and caching.\n",
      "*   **Cost Optimization:**  Optimize your application for cost-effectiveness.  Monitor your usage of LLMs and other resources.\n",
      "*   **API Management:**  Use an API gateway to manage access to your application and provide security.\n",
      "*   **CI/CD:**  Implement continuous integration and continuous deployment (CI/CD) to automate the deployment process.\n",
      "\n",
      "**10. Staying Up-to-Date:**\n",
      "\n",
      "*   **Follow LangChain Updates:**  Keep up-to-date with the latest LangChain releases, features, and bug fixes.\n",
      "*   **Read Documentation:**  Consult the official LangChain documentation regularly.\n",
      "*   **Explore Examples & Tutorials:**  Learn from LangChain's examples and tutorials.\n",
      "*   **Join the Community:**  Participate in the LangChain community (e.g., forums, Discord) to learn from others and share your experiences.\n",
      "*   **Experiment and Learn:**  Continually experiment with new techniques and technologies to improve your skills.\n",
      "\n",
      "By following these best practices, you can develop robust, maintainable, and responsible LangChain applications that effectively leverage the power of LLMs. Remember that the field of LLMs is rapidly evolving, so continuous learning and adaptation are crucial.\n",
      "CPU times: user 28.8 ms, sys: 17.9 ms, total: 46.7 ms\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "response = llm.invoke(\"What are the best practices for developing with LangChain?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a078a1-9f34-4f21-a57b-3435bd23bf9f",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "# chat = ChatOpenAI(openai_api_key=\"...\")\n",
    "# If you have an envionrment variable set for OPENAI_API_KEY, you can just do:\n",
    "chat = ChatOpenAI()\n",
    "chat.invoke(\"Hello, how are you?\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654b4fcb-f826-4012-96b9-a35fa19c5c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well, thank you for asking! As a large language model, I don't experience emotions like humans do, but I am functioning properly and ready to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Make sure to set your GOOGLE_API_KEY environment variable.\n",
    "# You can get one from Google AI Studio: https://aistudio.google.com/app/apikey\n",
    "\n",
    "# Initialize the Gemini chat model\n",
    "# You can also specify other models like \"gemini-1.5-pro\"\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "# Invoke the model with a prompt\n",
    "response = chat.invoke(\"Hello, how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0333dc9-a2a4-4b45-9bf1-02348ff467b3",
   "metadata": {},
   "source": [
    "## 85. Chat Models -- Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95c4b12e-cda6-4142-ac41-f61911adfd81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of France is **Paris**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = chat.invoke(\"What is the capital of France?\")\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abb264da-cacc-4fa9-87c0-b7a305281b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
       " 'finish_reason': 'STOP',\n",
       " 'model_name': 'gemini-2.0-flash-lite',\n",
       " 'safety_ratings': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0f7d4-5d29-45c2-bcd3-385cfcfccc53",
   "metadata": {},
   "source": [
    "original:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "result = chat.invoke(messages)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5784b22-dce4-4992-9562-cd3af0fa1f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some company name ideas for a colorful sock company, playing on different angles:\n",
      "\n",
      "**Emphasizing Color & Fun:**\n",
      "\n",
      "*   Chromatic Threads\n",
      "*   Rainbow Footwear\n",
      "*   Socktacular Colors\n",
      "*   Kaleidoscope Socks\n",
      "*   Vivid Soles\n",
      "*   The Color Sock Shop\n",
      "*   Happy Feet Hues\n",
      "*   Sock Pop\n",
      "*   Technicolor Toes\n",
      "\n",
      "**Highlighting Quality & Comfort:**\n",
      "\n",
      "*   Cozy Canvas Socks\n",
      "*   Comfort & Hue\n",
      "*   The Soft Sole\n",
      "*   Thread & Thrive\n",
      "*   Artisan Socks\n",
      "*   Stride in Style\n",
      "*   Foot Haven\n",
      "\n",
      "**Playful & Catchy:**\n",
      "\n",
      "*   Sock It To 'Em\n",
      "*   Sole Mates\n",
      "*   Toe-riffic Socks\n",
      "*   Sock Monster\n",
      "*   The Sock Drawer\n",
      "*   Get Your Sock On\n",
      "*   Soleful Creations\n",
      "\n",
      "**Modern & Minimalist:**\n",
      "\n",
      "*   Hue & Heel\n",
      "*   The Sock Lab\n",
      "*   Bold Threads\n",
      "*   Stride Studio\n",
      "\n",
      "**Tips for Choosing:**\n",
      "\n",
      "*   **Check availability:** Make sure the name isn't already in use and that the domain name is available.\n",
      "*   **Consider your target audience:** Who are you trying to sell to? (e.g., kids, adults, fashion-conscious individuals)\n",
      "*   **Keep it memorable:** Easy to say and spell.\n",
      "*   **Reflect your brand:** Does the name convey the feeling you want to evoke?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Ensure your GOOGLE_API_KEY environment variable is set\n",
    "# 1. Initialize the Gemini chat model\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
    "\n",
    "# 2. Define the message(s) for the model\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that generates company names.\"),\n",
    "    HumanMessage(content=text),\n",
    "]\n",
    "\n",
    "# 3. Invoke the model with the messages\n",
    "result = chat.invoke(messages)\n",
    "\n",
    "# 4. Print the AI's response content\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a4456-7f1f-4747-839e-943486e3425c",
   "metadata": {},
   "source": [
    "## 86. Chat Prompt Templates\n",
    "* https://drive.google.com/file/d/1JoyxZlYfngmXnvrRyo7qqvUoB7qz6il0/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1f6a8d9-78c2-4604-b3e0-cbb9a603efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some company name ideas for a colorful sock company, playing on different angles:\n",
      "\n",
      "**Emphasizing Color & Fun:**\n",
      "\n",
      "*   Chromatic Threads\n",
      "*   Kaleidoscope Kicks\n",
      "*   Rainbow Soles\n",
      "*   Color Burst Socks\n",
      "*   Happy Feet Hues\n",
      "*   The Sock Spectrum\n",
      "*   Vivid Verse\n",
      "*   Pop of Pattern\n",
      "*   Socktastic Colors\n",
      "*   Funky Footwear\n",
      "\n",
      "**Highlighting Quality & Comfort:**\n",
      "\n",
      "*   Cozy Canvas\n",
      "*   Soft Strides\n",
      "*   The Comfort Collective\n",
      "*   Woven Wonders\n",
      "*   The Sock Studio\n",
      "*   Thread & Thrive\n",
      "*   Stride & Style\n",
      "\n",
      "**Playful & Catchy:**\n",
      "\n",
      "*   Sock It To 'Em\n",
      "*   Sock It Up\n",
      "*   Sole Mates\n",
      "*   Footloose & Fancy\n",
      "*   Toe-tally Awesome Socks\n",
      "*   Sockalicious\n",
      "*   The Happy Heel\n",
      "*   Sock Therapy\n",
      "\n",
      "**Modern & Minimalist:**\n",
      "\n",
      "*   Hue & Heel\n",
      "*   The Sock Lab\n",
      "*   Stride Studio\n",
      "*   Pattern Play\n",
      "*   Canvas & Co.\n",
      "\n",
      "**Location-Based (If Applicable):**\n",
      "\n",
      "*   (Your City/Region) Socks\n",
      "*   [Landmark] Socks\n",
      "\n",
      "**Tips for Choosing:**\n",
      "\n",
      "*   **Check for availability:** Make sure the name and domain name are available.\n",
      "*   **Consider your target audience:** Who are you selling to? The name should resonate with them.\n",
      "*   **Keep it memorable:** Easy to say and remember is key.\n",
      "*   **Say it out loud:** Does it sound good?\n",
      "*   **Get feedback:** Ask friends and family for their opinions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that generates company names\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat_prompt_template.invoke(\n",
    "    {\n",
    "        \"text\": \"What would be a good company name for a company that makes colorful socks?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# model = Cha(model='gpt-4o-mini')\n",
    "\n",
    "ai_llm_result = chat.invoke(result)\n",
    "print(ai_llm_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561f15e-9717-4d09-9e29-8ba7d53fbc3a",
   "metadata": {},
   "source": [
    "## 87. Streaming\n",
    "* https://drive.google.com/file/d/18sGlOZ8AKwON1CXUMnqf9ONfj7bwjSiO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2db2846-80d8-4ef7-a5ad-b225aba91066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0342beb091de416f90ab79eaf3d085a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Moon doesn't have a capital city. It's a natural satellite, and no country or entity has established a permanent capital there.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tqdm, tqdm.notebook\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    # streaming=True,\n",
    ")\n",
    "for chunk in tqdm.notebook.tqdm(chat.stream(\"What is the capital of the moon?\")):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb60bd3-6ed4-46de-b0bb-af66c3962836",
   "metadata": {},
   "source": [
    "## 88. Output Parsers\n",
    "* https://drive.google.com/file/d/1QWwi3AOCHEoMR83zR21sB7zzKdUxVdfO/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c7202cb-737a-4396-998f-36a914b55ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73ddb083-21d6-4463-baf5-9de828dad2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7732fa1-8542-4911-aa58-18613e3b8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"description\": \"The setup to the joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"The punchline to the joke\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abc6c1e9-e7b7-4a97-b4f5-94318c5358b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "\n",
    "messages = chat_prompt.invoke(\n",
    "    {\n",
    "        \"query\": \"What is a really funny joke about Python programming?\",\n",
    "        \"format_instructions\": parser.get_format_instructions(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0b8f8a7-7e75-4dab-974c-7845e12c1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()\n",
    "## does not work with Gemini\n",
    "result = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53af04d1-8046-43d5-a1b7-6da3f7c30306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do Python programmers prefer snakes over dogs?\n",
      "Because snakes come with built-in __ssscale__ method!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    joke_object = parser.parse(result.content)\n",
    "    print(joke_object.setup)\n",
    "    print(joke_object.punchline)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9c358e2-007f-429a-8297-c71275ed72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfc7afe0-a0b0-4d5b-9233-6556838c9c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why do Python programmers prefer using snakes?', punchline=\"Because they can't resist a good Python!\")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10f91080-cfa2-49a3-b42c-40ce6b224b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup to the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    explanation: str = Field(\n",
    "        description=\"A detailed explanation of why this joke is funny.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Jokes(BaseModel):\n",
    "    jokes: List[Joke] = Field(description=\"A list of jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ce3a9b0-6199-410e-9c69-9cc95f51c6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='I know a guy who can get you any Python library you want.', punchline='I get all my Python libraries from him.', explanation=\"This is a classic programming joke format, but tailored to Python. The humor comes from Python's 'import' statement being used in a literal, real-world context.\")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "structured_llm = chat.with_structured_output(Joke)\n",
    "result = structured_llm.invoke(\"What is a really funny joke about Python programming?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275de99-b846-4cbd-88da-9f4bcfa2bf2e",
   "metadata": {},
   "source": [
    "## 89. Summarizing large amounts of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d808c-3f91-4376-b8a9-4c0eb30c4c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-for-genai",
   "language": "python",
   "name": "conda-for-genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
